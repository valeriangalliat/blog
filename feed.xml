<?xml version="1.0" encoding="utf-8" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CodeJam</title>
  <subtitle>Hey, I‚Äôm Val, welcome to my blog!</subtitle>
  <link href="https://www.codejam.info/feed.xml" rel="self" />
  <link href="https://www.codejam.info/" />
  <id>https://www.codejam.info/</id>
  <updated>2023-04-13T18:05:26.427Z</updated>
  <author>
    <name>Val</name>
  </author>
  <entry>
    <title>Changing default mail client on macOS without signing in to the Mail app</title>
    <link href="https://www.codejam.info/2023/04/macos-default-mail-client.html" />
    <id>https://www.codejam.info/2023/04/macos-default-mail-client.html</id>
    <updated>2023-04-13T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>The things we have to do sometimes‚Ä¶ üôà</p>
<p>Maybe you use another mail client than the Mail app on macOS, and you
want to make it the default, so that when you click <code>mailto:</code> links, it
actually opens the app you want.</p>
<p>Apple documents how to do that in <a href="https://support.apple.com/en-ca/HT201607">change the default email app</a>:</p>
<blockquote>
<ol>
<li>Open the Mail app.</li>
<li>From the menu bar, choose <strong>Mail &gt; Settings</strong>.</li>
<li>Click <strong>General</strong>.</li>
<li>Choose an email app from the <strong>Default email reader</strong> menu.</li>
</ol>
</blockquote>
<p>That‚Äôs great, except it doesn‚Äôt work. If you never used the Mail app,
which you probably didn‚Äôt if you use another mail client, you can‚Äôt
access the settings! You‚Äôre greeted with this screen:</p>
<figure class="center">
  <img alt="A dialog prompting you to set up a mail account" srcset="../../img/2023/04/mail/01-blocking-dialog.png 2x">
</figure>
<p>And the settings are greyed out!</p>
<figure class="center">
  <img alt="Greyed out settings menu" srcset="../../img/2023/04/mail/02-disabled-settings.png 2x">
</figure>
<p>What to do then? There‚Äôs <a href="https://apple.stackexchange.com/q/261881/452681">a number of solutions</a>:</p>
<ul>
<li>Connect your mail account to the Mail app to go through this dialog
and finally access the settings.</li>
<li>Use a number of different third-party apps that can change default
associations.</li>
<li>Write a script to mess with the <code>LaunchServices</code> API.</li>
</ul>
<p>But <a href="https://apple.stackexchange.com/a/422772/452681">my</a>
<a href="https://osxdaily.com/2014/05/06/change-default-mail-app-mac/#comment-745047">favorite</a>,
that doesn‚Äôt require any third-party app, consists in selecting <strong>Other
Mail Account</strong>, putting garbage in the fields, and let it fail a few
times until it works!</p>
<figure class="center">
  <img alt="Mail account settings" srcset="../../img/2023/04/mail/03-add-account.png 2x">
</figure>
<p>This will obviously fail, and prompt you for more information:</p>
<figure class="center">
  <img alt="Advanced account settings" srcset="../../img/2023/04/mail/04-add-account-error.png 2x">
</figure>
<p>Just keep hitting the <strong>Sign In</strong> button until it gives up and lets you
through! You now have access to the settings menu.</p>
<figure class="center">
  <img alt="Active settings menu" srcset="../../img/2023/04/mail/05-settings-menu.png 2x">
</figure>
<p>From there, you can set your <strong>Default email reader</strong> to your favorite
app.</p>
<figure class="center">
  <img alt="Default email reader settings" srcset="../../img/2023/04/mail/06-settings.png 2x">
</figure>
<h2 id="using-gmail-inside-firefox-as-default-email-reader-on-macos" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/macos-default-mail-client.html#using-gmail-inside-firefox-as-default-email-reader-on-macos"><span>Using Gmail inside Firefox as default email reader on macOS</span></a></h2>
<p>In my case, I selected the Firefox app in the previous step, because I
want to use Gmail inside Firefox as my default email reader.</p>
<p>With that, the next time you open a <code>mailto:</code> link from anywhere on your
system, it‚Äôs going to open Firefox, and Firefox will then need to know
you want to use Gmail for this. Normally it‚Äôll prompt you the first
time, but you can also configure it in the <strong>Applications</strong> settings:</p>
<figure class="center">
  <img alt="Firefox settings" srcset="../../img/2023/04/mail/07-firefox-settings.png 2x">
</figure>
<p>I hope this helps!</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Configuring a GCP Pub/Sub dead letter queue with Pulumi</title>
    <link href="https://www.codejam.info/2023/04/gcp-pubsub-dlq-pulumi.html" />
    <id>https://www.codejam.info/2023/04/gcp-pubsub-dlq-pulumi.html</id>
    <updated>2023-04-09T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>I‚Äôve been playing a bit with <a href="https://www.pulumi.com/">Pulumi</a> lately,
and it quickly became one of my favorite infrastructure as code tools.
It feels like the power of <a href="https://aws.amazon.com/cdk/">AWS CDK</a> which
lets you code your infrastructure in a full-fledged scripting language,
but without being limited to AWS!</p>
<p>I like coding my infrastructure in TypeScript because the typing,
autocomplete and IDE integrations makes it particularly nice to discover
the SDK on the fly as you‚Äôre creating your infrastructure, so that‚Äôs
what I‚Äôll use in the examples.</p>
<p>Today, we‚Äôre gonna see how to programmatically create a Pub/Sub topic
and subscription on GCP, with a matching dead letter queue. Finally,
we‚Äôll add a monitoring alert policy to warn us when our
<abbr title="Dead letter queue">DLQ</abbr> is not empty.</p>
<h2 id="getting-started" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/gcp-pubsub-dlq-pulumi.html#getting-started"><span>Getting started</span></a></h2>
<p>If you already have Pulumi installed, and an existing project, you can
skip this. In order to install Pulumi on macOS, run:</p>
<pre><code class="hljs language-sh">brew install pulumi
</code></pre>
<p>Create an account on Pulumi if you don‚Äôt have one already, then create a
new directory for your project, and inside it, run:</p>
<pre><code class="hljs language-sh">pulumi new gcp-typescript
</code></pre>
<p>Follow the instructions to initialize your project and connect it to
your GCP account.</p>
<p>Finally, you can remove the default code from <code>index.ts</code> that creates a
test bucket.</p>
<h2 id="creating-a-topic-and-a-subscription" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/gcp-pubsub-dlq-pulumi.html#creating-a-topic-and-a-subscription"><span>Creating a topic and a subscription</span></a></h2>
<pre><code class="hljs language-ts"><span class="hljs-keyword">import</span> * <span class="hljs-keyword">as</span> gcp <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;@pulumi/gcp&#x27;</span>

<span class="hljs-keyword">const</span> topic = <span class="hljs-keyword">new</span> gcp.<span class="hljs-property">pubsub</span>.<span class="hljs-title class_">Topic</span>(<span class="hljs-string">&#x27;hello-world-topic&#x27;</span>, { <span class="hljs-attr">name</span>: <span class="hljs-string">&#x27;hello-world&#x27;</span> })

<span class="hljs-keyword">const</span> subscription = <span class="hljs-keyword">new</span> gcp.<span class="hljs-property">pubsub</span>.<span class="hljs-title class_">Subscription</span>(<span class="hljs-string">&#x27;hello-world-subscription&#x27;</span>, {
  <span class="hljs-attr">name</span>: <span class="hljs-string">&#x27;hello-world&#x27;</span>,
  <span class="hljs-attr">topic</span>: topic.<span class="hljs-property">id</span>
})
</code></pre>
<p>This will create a topic and a basic pull subscription, that you can‚Ä¶
subscribe to using the Google Cloud SDK in your favorite language.</p>
<h2 id="adding-the-dead-letter-queue" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/gcp-pubsub-dlq-pulumi.html#adding-the-dead-letter-queue"><span>Adding the dead letter queue</span></a></h2>
<p>On GCP, a dead letter queue consists in configuring an existing
subscription to send messages that failed a number of times to another
topic. Having a subscription on that dead letter topic, even if it has
no consumer, lets us store those messages for a period of time, so we
can eventually do something with them.</p>
<p>Here‚Äôs our DLQ:</p>
<pre><code class="hljs language-ts"><span class="hljs-keyword">const</span> dlqTopic = <span class="hljs-keyword">new</span> gcp.<span class="hljs-property">pubsub</span>.<span class="hljs-title class_">Topic</span>(<span class="hljs-string">&#x27;hello-world-dl-topic&#x27;</span>, { <span class="hljs-attr">name</span>: <span class="hljs-string">&#x27;helo-world-dl&#x27;</span> })

<span class="hljs-keyword">new</span> gcp.<span class="hljs-property">pubsub</span>.<span class="hljs-title class_">Subscription</span>(<span class="hljs-string">&#x27;hello-world-dl-subscription&#x27;</span>, {
  <span class="hljs-attr">name</span>: <span class="hljs-string">&#x27;hello-world-dl&#x27;</span>,
  <span class="hljs-attr">topic</span>: dlqTopic.<span class="hljs-property">id</span>
})
</code></pre>
<p>Then we can add the dead letter policy to our existing subscription:</p>
<pre><code class="hljs language-diff:ts"> <span class="hljs-keyword">const</span> subscription = <span class="hljs-keyword">new</span> gcp.<span class="hljs-property">pubsub</span>.<span class="hljs-title class_">Subscription</span>(<span class="hljs-string">&#x27;hello-world-subscription&#x27;</span>, {
   <span class="hljs-attr">name</span>: <span class="hljs-string">&#x27;hello-world&#x27;</span>,
   <span class="hljs-attr">topic</span>: topic.<span class="hljs-property">name</span>,
<span class="hljs-addition">+  <span class="hljs-attr">deadLetterPolicy</span>: {</span>
<span class="hljs-addition">+    <span class="hljs-attr">deadLetterTopic</span>: dlqTopic.<span class="hljs-property">id</span>,</span>
<span class="hljs-addition">+    <span class="hljs-attr">maxDeliveryAttempts</span>: <span class="hljs-number">5</span></span>
<span class="hljs-addition">+  }</span>
 })
</code></pre>
<p><code>maxDepliveryAttempts</code> is optional and defaults to 5. When a messaged
failed to be delivered that many times, it‚Äôll be sent to the DLQ.</p>
<p>You may also like to tweak your subscription‚Äôs retry policy at that
point. By default, it retries a failed message immediately, but you can
configure an exponential backoff instead:</p>
<pre><code class="hljs language-diff:ts"> <span class="hljs-keyword">const</span> subscription = <span class="hljs-keyword">new</span> gcp.<span class="hljs-property">pubsub</span>.<span class="hljs-title class_">Subscription</span>(<span class="hljs-string">&#x27;hello-world-subscription&#x27;</span>, {
   <span class="hljs-attr">name</span>: <span class="hljs-string">&#x27;hello-world&#x27;</span>,
   <span class="hljs-attr">topic</span>: topic.<span class="hljs-property">id</span>,
<span class="hljs-addition">+  <span class="hljs-attr">retryPolicy</span>: {</span>
<span class="hljs-addition">+    <span class="hljs-attr">minimumBackoff</span>: <span class="hljs-string">&#x27;10s&#x27;</span>,</span>
<span class="hljs-addition">+    <span class="hljs-attr">maximumBackoff</span>: <span class="hljs-string">&#x27;600s&#x27;</span></span>
<span class="hljs-addition">+  },</span>
   <span class="hljs-attr">deadLetterPolicy</span>: {
     <span class="hljs-attr">deadLetterTopic</span>: dlqTopic.<span class="hljs-property">id</span>,
     <span class="hljs-attr">maxDeliveryAttempts</span>: <span class="hljs-number">5</span>
   }
 })
</code></pre>
<p>While you don‚Äôt have precise control over the exponential backoff
behavior, you can tweak the minimum and maximum duration that Pub/Sub
will wait before retrying a message. Anything in between is out of your
control.</p>
<h2 id="handling-permissions" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/gcp-pubsub-dlq-pulumi.html#handling-permissions"><span>Handling permissions</span></a></h2>
<p>But we‚Äôre not done yet! If you go to your subscription page, you‚Äôll
notice the following issues warnings:</p>
<figure class="center">
  <img alt="Permission issues with dead letter queue" srcset="../../img/2023/04/pubsub-dlq-warning.png 2x">
</figure>
<blockquote>
<p>‚ùóÔ∏è <strong>Assign Publisher role</strong></p>
<p>The Cloud Pub/Sub service account for this project needs the publisher
role to publish dead-lettered messages to the dead letter topic.</p>
<p>‚ùóÔ∏è <strong>Assign Subscriber role</strong></p>
<p>The Cloud Pub/Sub service account for this project needs the subscriber
role to forward messages from this subscription to the dead letter topic.</p>
</blockquote>
<p>You can identify the Pub/Sub service account in your IAM principals
list, by ticking ‚Äúinclude Google-provided role grants‚Äù. It‚Äôs always
under the form <code>service-{projectId}@gcp-sa-pubsub.iam.gserviceaccount.com</code>.</p>
<p>We can fix that in our Pulumi code by adding the following:</p>
<pre><code class="hljs language-ts"><span class="hljs-keyword">import</span> * <span class="hljs-keyword">as</span> pulumi <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;@pulumi/pulumi&#x27;</span>

<span class="hljs-keyword">const</span> project = gcp.<span class="hljs-property">organizations</span>.<span class="hljs-title function_">getProjectOutput</span>()

<span class="hljs-keyword">const</span> pubSubServiceAccountPublisherPolicy =
  gcp.<span class="hljs-property">organizations</span>.<span class="hljs-title function_">getIAMPolicyOutput</span>({
    <span class="hljs-attr">bindings</span>: [
      {
        <span class="hljs-attr">role</span>: <span class="hljs-string">&#x27;roles/pubsub.publisher&#x27;</span>,
        <span class="hljs-attr">members</span>: [
          pulumi.<span class="hljs-property">interpolate</span><span class="hljs-string">`serviceAccount:service-<span class="hljs-subst">${project.<span class="hljs-built_in">number</span>}</span>@gcp-sa-pubsub.iam.gserviceaccount.com`</span>
        ]
      }
    ]
  })

<span class="hljs-keyword">const</span> pubSubServiceAccountSubscriberPolicy =
  gcp.<span class="hljs-property">organizations</span>.<span class="hljs-title function_">getIAMPolicyOutput</span>({
    <span class="hljs-attr">bindings</span>: [
      {
        <span class="hljs-attr">role</span>: <span class="hljs-string">&#x27;roles/pubsub.subscriber&#x27;</span>,
        <span class="hljs-attr">members</span>: [
          pulumi.<span class="hljs-property">interpolate</span><span class="hljs-string">`serviceAccount:service-<span class="hljs-subst">${project.<span class="hljs-built_in">number</span>}</span>@gcp-sa-pubsub.iam.gserviceaccount.com`</span>
        ]
      }
    ]
  })

<span class="hljs-keyword">new</span> gcp.<span class="hljs-property">pubsub</span>.<span class="hljs-title class_">TopicIAMPolicy</span>(<span class="hljs-string">&#x27;hello-world-dl-topic-policy&#x27;</span>, {
  <span class="hljs-attr">topic</span>: dlqTopic.<span class="hljs-property">name</span>,
  <span class="hljs-attr">policyData</span>: pubSubServiceAccountPublisherPolicy.<span class="hljs-property">policyData</span>
})

<span class="hljs-keyword">new</span> gcp.<span class="hljs-property">pubsub</span>.<span class="hljs-title class_">SubscriptionIAMPolicy</span>(<span class="hljs-string">&#x27;hello-world-subscription-policy&#x27;</span>, {
  <span class="hljs-attr">subscription</span>: subscription.<span class="hljs-property">name</span>,
  <span class="hljs-attr">policyData</span>: pubSubServiceAccountSubscriberPolicy.<span class="hljs-property">policyData</span>
})
</code></pre>
<p>Now our Pub/Sub DLQ page should be all green!</p>
<h2 id="getting-alerted-for-new-messages-in-the-dlq" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/gcp-pubsub-dlq-pulumi.html#getting-alerted-for-new-messages-in-the-dlq"><span>Getting alerted for new messages in the DLQ</span></a></h2>
<p>The first thing you usually do when you create a DLQ is add a mechanism
to <em>know</em> when messages hit the DLQ, so that you can act on them.</p>
<p>When it comes to alert policies, I typically create them in the GCP
console, then I use the ‚Äúdownload as JSON‚Äù button in the policy details.
I can use this verbatim inside Pulumi‚Äôs <code>gcp.monitoring.AlertPolicy</code>
constructor!</p>
<p>Here‚Äôs what I‚Äôve got when I made an alert policy to get notified when
any of my subscriptions whose name ends with <code>-dl</code> has undelivered
messages.</p>
<pre><code class="hljs language-ts"><span class="hljs-keyword">const</span> notificationChannels = [
  <span class="hljs-string">&#x27;projects/{projectId}/notificationChannels/{channelId}&#x27;</span>
]

<span class="hljs-keyword">new</span> gcp.<span class="hljs-property">monitoring</span>.<span class="hljs-title class_">AlertPolicy</span>(<span class="hljs-string">&#x27;alert-policy-pubsub-dl&#x27;</span>, {
  <span class="hljs-attr">alertStrategy</span>: {
    <span class="hljs-attr">autoClose</span>: <span class="hljs-string">&#x27;604800s&#x27;</span>
  },
  <span class="hljs-attr">combiner</span>: <span class="hljs-string">&#x27;OR&#x27;</span>,
  <span class="hljs-attr">conditions</span>: [
    {
      <span class="hljs-attr">conditionThreshold</span>: {
        <span class="hljs-attr">aggregations</span>: [
          {
            <span class="hljs-attr">alignmentPeriod</span>: <span class="hljs-string">&#x27;300s&#x27;</span>,
            <span class="hljs-attr">perSeriesAligner</span>: <span class="hljs-string">&#x27;ALIGN_MEAN&#x27;</span>
          }
        ],
        <span class="hljs-attr">comparison</span>: <span class="hljs-string">&#x27;COMPARISON_GT&#x27;</span>,
        <span class="hljs-attr">duration</span>: <span class="hljs-string">&#x27;0s&#x27;</span>,
        <span class="hljs-attr">filter</span>: <span class="hljs-string">`
              resource.type = &quot;pubsub_subscription&quot;
          AND metric.type = &quot;pubsub.googleapis.com/subscription/num_undelivered_messages&quot;
          AND resource.labels.subscription_id = ends_with(&quot;-dl&quot;)
        `</span>,
        <span class="hljs-attr">thresholdValue</span>: <span class="hljs-number">0</span>,
        <span class="hljs-attr">trigger</span>: {
          <span class="hljs-attr">count</span>: <span class="hljs-number">1</span>
        }
      },
      <span class="hljs-attr">displayName</span>: <span class="hljs-string">&#x27;Cloud Pub/Sub Subscription - Unacked messages&#x27;</span>
    }
  ],
  notificationChannels,
  <span class="hljs-attr">displayName</span>: <span class="hljs-string">&#x27;Pub/Sub messages in dead letter&#x27;</span>
})
</code></pre>
<p>Just put the ID of your notification channel in the array on top. To
find it, you can use the following command that will list all your
notification channels including their full ID:</p>
<pre><code class="hljs language-sh">gcloud alpha monitoring channels list
</code></pre>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Firebase functions in a monorepo? A challenging pile of hacks</title>
    <link href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html" />
    <id>https://www.codejam.info/2023/04/firebase-functions-monorepo.html</id>
    <updated>2023-04-07T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>I recently went through the trouble of migrating a Firebase app to a
monorepo, in particular the Cloud Functions part. While doing so, I went
through a total of 3 different ‚Äúmethods‚Äù, all of which were full of
surprises that I discovered along the way.</p>
<p>In this blog post I‚Äôll go through those 3 options, and highlight their
tradeoffs, in order to help you pick the one that‚Äôs the most appropriate
to your workflow. It‚Äôs a long post, so feel free to jump to the
<a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#comparison">comparison</a> directly, and then cherry pick what to read
from there. üòÑ</p>
<p>Here, I assume that your monorepo uses something like npm or Yarn
workspaces. It may be applicable to pnpm but I didn‚Äôt try it.</p>
<h2 id="the-common-ground" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-common-ground"><span>The common ground</span></a></h2>
<p>Before we get started with the 3 options, they all share a common
ground. And for the sake of this blog post, I‚Äôll start with an
hypothetical base monorepo structure which I‚Äôll lay down below.</p>
<h3 id="the-base-monorepo" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-base-monorepo"><span>The base monorepo</span></a></h3>
<p>This is a basic monorepo with two websites and a shared package, e.g.
for helper functions, types or any other common code.</p>
<pre><code class="hljs">monorepo
‚îú‚îÄ‚îÄ apps
‚îÇ   ‚îú‚îÄ‚îÄ website1
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ website2
‚îÇ       ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ packages
‚îÇ   ‚îî‚îÄ‚îÄ shared
‚îÇ       ‚îî‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ package-lock.json
‚îî‚îÄ‚îÄ package.json
</code></pre>
<p>The top-level <code>package.json</code> contains:</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;workspaces&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
    <span class="hljs-string">&quot;apps/*&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-string">&quot;packages/*&quot;</span>
  <span class="hljs-punctuation">]</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<h3 id="the-firebase-functions-in-its-own-repo" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-firebase-functions-in-its-own-repo"><span>The Firebase functions in its own repo</span></a></h3>
<p>In another repo, you have a Firebase app with functions:</p>
<pre><code class="hljs">firebase
‚îú‚îÄ‚îÄ functions
‚îÇ   ‚îú‚îÄ‚îÄ src
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.js
‚îÇ   ‚îú‚îÄ‚îÄ package-lock.json
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ firebase.json
</code></pre>
<p>Where your <code>firebase.json</code> contains:</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;functions&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">&quot;source&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;functions&quot;</span>
  <span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<h3 id="merging-them-together" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#merging-them-together"><span>Merging them together</span></a></h3>
<p>Since in a Firebase repo, <code>functions</code> is already its own subdirectory
with its own <code>package.json</code>, it feels pretty natural to just ‚Äúmerge‚Äù
both repos together, maybe  renaming <code>functions</code> into <code>apps/functions</code>
to match our initial structure better, but no more than that:</p>
<pre><code class="hljs language-diff"> monorepo
 ‚îú‚îÄ‚îÄ apps
 ‚îÇ   ‚îú‚îÄ‚îÄ website1
 ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ package.json
 ‚îÇ   ‚îú‚îÄ‚îÄ website2
 ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ package.json
<span class="hljs-addition">+‚îÇ   ‚îî‚îÄ‚îÄ functions</span>
<span class="hljs-addition">+‚îÇ       ‚îú‚îÄ‚îÄ src</span>
<span class="hljs-addition">+‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ index.js</span>
<span class="hljs-addition">+‚îÇ       ‚îî‚îÄ‚îÄ package.json</span>
 ‚îú‚îÄ‚îÄ packages
 ‚îÇ   ‚îî‚îÄ‚îÄ shared
 ‚îÇ       ‚îî‚îÄ‚îÄ package.json
<span class="hljs-addition">+‚îú‚îÄ‚îÄ firebase.json</span>
 ‚îú‚îÄ‚îÄ package-lock.json
 ‚îî‚îÄ‚îÄ package.json
</code></pre>
<p>In <code>firebase.json</code>, we just update the <code>source</code> to be <code>apps/functions</code>,
and we remove the <code>functions/package-lock.json</code> to let npm merge the
functions dependencies in the top-level <code>package-lock.json</code>. This way,
we only need to run <code>npm install</code> at the root of the monorepo, instead
of having to go inside <code>apps/functions</code> and run <code>npm install</code> there
again. After all, that‚Äôs part of the point of a monorepo.</p>
<p>Great, so we‚Äôre done? That was easy.</p>
<h2 id="why-this-works-but-not-really" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#why-this-works-but-not-really"><span>Why this works, but not really</span></a></h2>
<p>Not so fast. This will seemingly work, but it will do so kind of by
chance, as a somewhat lucky accident.</p>
<h3 id="how-firebase-deploy-works" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#how-firebase-deploy-works"><span>How <code>firebase deploy</code> works</span></a></h3>
<p>See, when <code>firebase deploy</code> deploys the functions, it will make a ZIP
archive of the functions source directory (as defined in <code>firebase.json</code>).</p>
<p>Then, it will deploy the function from that ZIP. The Cloud Functions
deploy process will send that ZIP to Cloud Build, which will:</p>
<ol>
<li>Run some variant of <code>npm install</code> or <code>yarn install</code>.</li>
<li>Run the <code>gcp-build</code> script if defined in <code>package.json</code>.</li>
<li>Prune development dependencies from <code>node_modules</code> if needed.</li>
<li>Use the output of that process as the source for the function
runtime.</li>
</ol>
<p>This is defined in GCP buildpacks, e.g. <a href="https://github.com/GoogleCloudPlatform/buildpacks/blob/99553d0a2051834324d621f20ad5355453f675a1/cmd/nodejs/npm/main.go">for npm</a>
and <a href="https://github.com/GoogleCloudPlatform/buildpacks/blob/99553d0a2051834324d621f20ad5355453f675a1/cmd/nodejs/yarn/main.go">for Yarn</a>.</p>
<p>We can already see a bit of a problem. Because we‚Äôre sending only the
<code>apps/functions</code> context to Cloud Build, it doesn‚Äôt have access to the
top-level <code>package-lock.json</code>, which means the install output will be
nondeterministic, and each deploy is subject to using different versions
of different packages and potentially break your code without you
knowing.</p>
<p><strong>This can introduce a whole range of sneaky errors that will be a pain to
debug!</strong></p>
<h3 id="using-shared-packages" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#using-shared-packages"><span>Using shared packages</span></a></h3>
<p>Moreover, we now understand that this will not allow using <em>shared
packages</em> inside the monorepo!</p>
<p>If we wanted to use <code>packages/shared</code> inside <code>apps/functions</code>, by adding
<code>&quot;shared&quot;: &quot;*&quot;</code> in our <code>dependencies</code>, letting npm or Yarn resolve it to
the local workspace version, it wouldn‚Äôt actually work.</p>
<p>Or actually, it will work in development, because we have the whole
monorepo there. And in our particular example, even the Firebase
deployment will surprisingly succeed, <strong>but only as an accident because
<a href="https://www.npmjs.com/package/shared"><code>shared</code></a> is a valid npm
package</strong>! It will break at runtime when you try to use a package that
doesn‚Äôt contain the code you expect at all.</p>
<p>Other names for common monorepo shared packages that are also valid npm
packages would be <a href="https://www.npmjs.com/package/eslint-config"><code>eslint-config</code></a>
and <a href="https://www.npmjs.com/package/tsconfig"><code>tsconfig</code></a>, so they would
also result in this kind of collision.</p>
<div class="note">
<p><strong>Note:</strong> if you use Yarn, you can prevent those collisions by prefixing
your version specifier for your shared dependencies with <code>workspace:</code>,
e.g. <code>&quot;shared&quot;: &quot;workspace:*&quot;</code> to use any version. This will ensure the
dependency is <em>always</em> installed from the local workspace and not from
the registry.</p>
<p>npm doesn‚Äôt support that, but you can still add a layer of safety by
making sure all your shared package names don‚Äôt conflict with anything
on npm, for example by prefixing them with <code>@myorg</code> such as
<code>@myorg/shared</code>, <code>@myorg/eslint-config</code>, <code>@myorg/tsconfig</code> and so on.</p>
<p>Or as an abundance of caution if you use Yarn, maybe do both. üò¨</p>
</div>
<h2 id="the-good-enough-for-me-approach" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-good-enough-for-me-approach"><span>The ‚Äúgood enough for me‚Äù approach</span></a></h2>
<p>We‚Äôre now in a situation where 1. the top-level <code>package-lock.json</code> is
not respected when deploying Cloud Functions, and 2. we cannot use any
workspace shared package in our functions.</p>
<p>You may actually be fine with that. Maybe you don‚Äôt care that your
production functions have an unpredictable dependency tree every time
you deploy, and maybe you don‚Äôt want to use shared packages in your
functions anyway!</p>
<div class="note">
<p><strong>Note:</strong> you can even use shared packages in your <code>devDependencies</code>
with that setup, as long as you don‚Äôt have a <code>gcp-build</code> script that
depends on them!</p>
<p>At least if you use npm. Because there‚Äôs currently a bug with the Yarn
Cloud Build buildpack that makes it install <code>devDependencies</code> before
pruning them right after, even when no build script is present. üòÖ</p>
<p>This would fail your build if the shared package from your
<code>devDependencies</code> don‚Äôt exist on npm. It‚Äôs one of those cases where
having a shared package name that collisions with a npm package would
help, although I wouldn‚Äôt really recommend this as a fix.</p>
</div>
<p>If that works for you, congratulations, your job here is done.
Otherwise, let‚Äôs dig in the two other options. üëá</p>
<h2 id="the-full-context-approach" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-full-context-approach"><span>The full context approach</span></a></h2>
<p>There‚Äôs a <a href="https://github.com/firebase/firebase-tools/issues/653">long thread</a>
in the <code>firebase-tools</code> repo about monorepo support. The majority of the
solutions described there are some variation of a deploy script that
packs your shared dependencies into <code>.tgz</code> files, and patch the
<code>functions/package.json</code> file to reference them with <code>file:</code> for the
time of the deployment. We‚Äôll explore this in details in the last solution: <a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-hybrid-approach">the hybrid approach</a>.</p>
<p>However, there‚Äôs <a href="https://github.com/firebase/firebase-tools/issues/653#issuecomment-1371306331">a particular comment</a>
in that thread that describes something very different, and caught my
attention despite not being given very much interest there.</p>
<figure class="center">
  <img alt="A comment suggesting to put the monorepo root as the functions source" srcset="../../img/2023/04/firebase-monorepo-comment.png 2x">
</figure>
<p>This comment suggests that we put the monorepo root as the functions
source in <code>firebase.json</code> (ignoring unnecessary files as needed), to
ensure we send the whole relevant monorepo context to Cloud Build!</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;functions&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">&quot;source&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;.&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;ignore&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
      <span class="hljs-string">&quot;firebase.json&quot;</span><span class="hljs-punctuation">,</span>
      <span class="hljs-string">&quot;**/.*&quot;</span><span class="hljs-punctuation">,</span>
      <span class="hljs-string">&quot;**/node_modules/**&quot;</span><span class="hljs-punctuation">,</span>
      <span class="hljs-string">&quot;**/packages/@(web|mobile)/**&quot;</span>
    <span class="hljs-punctuation">]</span>
  <span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<p>Then, adding the functions entrypoint in the top-level <code>package.json</code>,
because Cloud Functions still don‚Äôt know about monorepos, and expects the
functions <code>package.json</code> to be at the root.</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;main&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;./packages/functions/dist/index.js&quot;</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<div class="note">
<p><strong>Note:</strong> if you use <code>.env</code> files in your functions, e.g. <code>.env</code>,
<code>.env.production</code>, <code>.env.staging</code>, and any other project aliases you may
have, which is becoming more and more common <a href="https://firebase.google.com/docs/functions/config-env#environment_configuration">now Firebase deprecated
<code>functions.config()</code></a>,
you also need to put them at the root of your monorepo with this
solution, otherwise they will be ignored during deploy!</p>
</div>
<p>To me, this sounds <em>much more elegant</em> than the hacks with deploy
scripts and <code>file:</code> references! But after using this approach in
production for a few weeks, I decided to rollback, because there was too
many downsides for my use case.</p>
<h3 id="the-ignore-list-is-quirky" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-ignore-list-is-quirky"><span>The ignore list is quirky</span></a></h3>
<p>The ignore list is not exactly intuitive to work with. And if you forget
to ignore anything somewhat large, <a href="https://www.codejam.info/2023/04/firebase-functions-entity-too-large.html">your functions will fail to deploy</a>.
It struggled so much to figure out the precise rules of this ignore list
that I had to go in the <code>firebase-tools</code> source code in order to
understand it, and I wrote <a href="https://www.codejam.info/2023/04/firebase-functions-ignore.html">another blog post</a>
to explain how it really works, and how to test your ignore patterns!</p>
<p>The main caveat is that you <a href="https://github.com/firebase/firebase-tools/issues/2677">can‚Äôt use negative ignore rules</a>
like you could in <code>.gitignore</code> and most ignore systems, e.g.:</p>
<pre><code class="hljs language-gitignore">/apps/website1/*
!/apps/website1/package.json
/apps/website2/*
!/apps/website2/package.json
</code></pre>
<p>In a <code>.gitignore</code>, this would ignore everything in <code>apps/website1</code> and
<code>apps/website2</code> except for their <code>package.json</code>. If you use ‚Äúa modern
version of Yarn‚Äù (not 1.x), this is something you would need to do, because
<code>yarn install --immutable</code> will fail if the workspaces identified in
your <code>yarn.lock</code> don‚Äôt actually point to directories with a
<code>package.json</code> in them!</p>
<p>If you use npm or Yarn 1.x though, <code>npm ci</code> and <code>yarn install --frozen-lockfile</code> won‚Äôt care, so you‚Äôre good to go.</p>
<div class="note">
<p><strong>Note:</strong> just keep in mind that Yarn 1.x doesn‚Äôt let you install
dependencies for a single workspace, you systematically have to install
all dependencies for the whole monorepo, which can be a pretty bad hit
for any pipeline that works only on a small subset of the monorepo.</p>
<p>While you can <a href="https://classic.yarnpkg.com/en/docs/cli/install#toc-yarn-install-focus"><code>yarn install --focus</code></a>
with 1.x, which kind of sounds like this, it doesn‚Äôt work with
dependencies that are local to the monorepo, they <em>need</em> to be fetched
from a registry.</p>
</div>
<p>But on new Yarn versions, this is a pretty big deal because you can‚Äôt
ignore a whole workspace from your functions deploy, and because there‚Äôs
no negative patterns to ignore everything but the <code>package.json</code> in a
given workspace, you‚Äôre stuck with having to <em>explicitly</em> ignore
everything but the <code>package.json</code> in each of the workspaces you want to
exclude. And it‚Äôs a list you‚Äôll now have to maintain forever every time
you add new things to your monorepo.</p>
<p>This is even more of a problem because if you have any kind of secret in
your repo, and you fail to add them to your <code>functions.ignore</code> list,
they‚Äôll be packaged in your functions source and you won‚Äôt notice. Your
functions source is private to your Google Cloud account by default, but
this is silently waiting to make a future security issue much worse.</p>
<h3 id="all-the-other-workspace-dependencies-are-installed" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#all-the-other-workspace-dependencies-are-installed"><span>All the other workspace dependencies are installed</span></a></h3>
<p>This is the one that made me give up this solution. I could deal with
the ignore list issues, but this was another level.</p>
<p>As we saw earlier, Cloud Functions use Cloud Build to install your
dependencies. The whole thing is not designed for monorepos, which is
why we had to put our <code>main</code> entrypoint in the root <code>package.json</code>. A
more concerning effect of that though, is that Cloud Build will run <code>npm install</code>
at the top level of the monorepo.</p>
<p>This means installing all the dependencies of all your apps and
packages. This is big problem if you have a lot of unrelated
dependencies across your different workspaces.</p>
<p>Firebase doesn‚Äôt let you configure the install command either, to run
e.g. <code>npm install --workspace functions</code> or <code>yarn workspace function workspaces focus</code>
(I know, awkward command), which would install only the functions
dependencies. <em>This can speed up your install times drastically</em> in
remote build environments, but here it‚Äôs not an option.</p>
<p>For us, the difference was 10 minutes to deploy Firebase functions vs. 2
minutes, if we could install the dependencies of the functions only.</p>
<p>This was to much, which is why I ended up with the last approach.</p>
<div class="note">
<p><strong>Note:</strong> the build time issue was heavily magnified in my case by the
fact Cloud Build <a href="https://github.com/GoogleCloudPlatform/buildpacks/issues/203">doesn‚Äôt do any caching for Yarn 2.x and greater</a>
if it‚Äôs not used in <a href="https://yarnpkg.com/features/pnp">PnP mode</a>.
Proper caching may help a bit with npm and Yarn 1.x, even though it‚Äôs
still not ideal.</p>
<p>There may be a way though, for example by replacing the top-level
<code>package.json</code> and <code>package-lock.json</code> by dummy ones during <code>firebase deploy</code> so that from Cloud Build‚Äôs perspective it looks like you have no
dependencies, and then hijacking the <code>gcp-build</code> script to <em>actually</em>
install your dependencies yourself using the appropriate command that
doesn‚Äôt install the whole world at the same time. ü•π</p>
<p>I haven‚Äôt tested this but it may work. However, if you‚Äôre gonna get that
hacky, you might as well embrace the third solution.</p>
</div>
<h2 id="the-hybrid-approach" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-hybrid-approach"><span>The hybrid approach</span></a></h2>
<p>This is an improved version of <a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-good-enough-for-me-approach">the first ‚Äúgood enough for me‚Äù solution</a>,
where in our development environment, we work with a full-fledged
monorepo, with shared packages and everything, but when we deploy the
Firebase functions, we narrow it down to its own independent-repo-like
entity, but in a way that will actually work with our
<code>package-lock.json</code> and shared packages!</p>
<p>This will take a bit of code though, in the form of a <code>predeploy</code> and
<code>postdeploy</code> script for our functions. The <code>predeploy</code> script needs to:</p>
<ol>
<li>Do anything you were already doing in a <code>predeploy</code> script like
linting and building your app.</li>
<li>Copy all the shared packages you depend on in your functions
directory, either through <code>.tgz</code> files from using <code>npm pack</code> or <code>yarn pack</code>, or the directories themselves (see below for the difference).</li>
<li>Patch your functions <code>package.json</code> to reference the internal
dependencies using <code>file:</code> references to the <code>.tgz</code> files or
directories you just created.</li>
<li><strong>Do so recursively for your whole graph of internal dependencies.</strong>
Hopefully it‚Äôs small enough to be manageable, but I can see this
turning into a living hell in complex monorepos.</li>
<li>Copy the top-level lock file in the functions directory. If you use
Yarn 2.x and greater, you‚Äôll need to do a bit more than that, see
below.</li>
</ol>
<p>As for the <code>postdeploy</code> script, it needs to undo everything that
<code>predeploy</code> did.</p>
<p>Of course, your repo will be in an inconsistent state for the duration
of <code>firebase deploy</code>, so maybe run that from another copy of your
monorepo that you don‚Äôt work from, or make sure to not mess with your
dependencies during the deploy, or things will fall apart!</p>
<p>You‚Äôll find a number of examples of those <code>predeploy</code> and <code>postdeploy</code>
scripts in the issue thread I linked earlier. Here‚Äôs
<a href="https://github.com/firebase/firebase-tools/issues/653#issuecomment-1464911379">one of the most recent ones</a>
that you can take inspiration from.</p>
<p>For the part where you replace the versions of your internal packages in
your <code>package.json</code>, you can use <a href="https://docs.npmjs.com/cli/v7/commands/npm-pkg"><code>npm pkg set</code></a></p>
<pre><code class="hljs language-sh">npm pkg <span class="hljs-built_in">set</span> <span class="hljs-string">&#x27;dependencies.@myorg/shared=file:shared.tgz&#x27;</span> <span class="hljs-string">&#x27;dependencies.@myorg/tsconfig=file:tsconfig.tgz&#x27;</span>
</code></pre>
<p>Just make a backup of your original <code>package.json</code> so you can restore it
in the <code>postdeploy</code> script. Feel free to use it with Yarn as well since
this really just edits your <code>package.json</code> from the command line.</p>
<p>Now, about the downsides.</p>
<h3 id="you-have-to-recursively-package-your-internal-dependencies" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#you-have-to-recursively-package-your-internal-dependencies"><span>You have to recursively package your internal dependencies</span></a></h3>
<p>And to do so, you have to patch your <code>package.json</code> files all the way
down the internal dependency graph for your functions. Nasty.</p>
<p>As for using <code>.tgz</code> files from <code>npm pack</code> or <code>yarn pack</code> vs. copying the
directories directly, it comes down to personal preference with npm, but
if you use Yarn and you have nested internal dependencies, you‚Äôre much
better off going with the directory approach.</p>
<p>That‚Äôs because npm can resolve <code>file:</code> references to <code>.tgz</code> files
relative to <em>where <code>npm install</code> is ran from</em>, but Yarn only looks for
the <code>.tgz</code> files relative to the <code>package.json</code> referencing it.</p>
<p>You can see how this becomes a problem with more than one level of
dependency, because you would have to embed the archive of the same
packages in all the packages that reference it, and do so recursively,
which can get exponentially heavy and inefficient! Not to mention that
you‚Äôd end up with a lot of duplicated dependencies, which can cause a
whole lot of other problems on its own.</p>
<p>It will work with the directory approach though:</p>
<ol>
<li>You make your functions depend on <code>&quot;@myorg/shared&quot;: &quot;file:shared&quot;</code>.</li>
<li>You make <code>shared/package.json</code> depends on <code>&quot;@myorg/tsconfig&quot;: &quot;file:../tsconfig&quot;</code>.</li>
<li>You copy both <code>shared</code> and <code>tsconfig</code> under your functions directory
and you‚Äôre god to go.</li>
</ol>
<h3 id="you-need-to-mirror-some-top-level-logic" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#you-need-to-mirror-some-top-level-logic"><span>You need to mirror some top-level logic</span></a></h3>
<p>In the previous solution, we saw how we had to copy some functions logic
at the top level (<code>main</code> inside <code>package.json</code> as well as <code>.env</code> files).
Here, we have the opposite problem.</p>
<p>Because we‚Äôre shipping only the functions directory to Cloud Functions,
it‚Äôs missing your <code>package-lock.json</code> or <code>yarn.lock</code> from the top
level (and maybe a number of other files you may need without knowing it).</p>
<p>For example, if you use ‚Äúa modern version of Yarn‚Äù aka not Yarn 1.x, it
also needs its <code>.yarnrc.yml</code> as well as <code>.yarn/releases</code> and
<code>.yarn/plugins</code> directories in order to function!</p>
<p>If you forget to copy any of those inside your functions directory,
Cloud Build will either use the wrong package manager or the wrong
version of your package manager, which may result in the best case in a
broken deploy, or worst, resolving and linking dependencies differently
than in your local environment, which can lead to a number of sneaky
issues.</p>
<p>This is not something that‚Äôs accounted for in any of the solutions from
<a href="https://github.com/firebase/firebase-tools/issues/653">the thread</a>
I linked earlier. <strong>They all ship a lonely <code>functions/package.json</code> that
will end up installing unpredictable dependency versions in their
production environment.</strong></p>
<p>Luckily, this is easy to fix! Just copy your top-level
<code>package-lock.json</code> or <code>yarn.lock</code> in the functions directory as part of
your <code>predeploy</code> script.</p>
<p>npm and Yarn 1.x are resilient enough to do the right thing from a
<em>superset</em> of the lock file. More recent versions of Yarn though, are
pretty strict and will refuse to install if it finds anything
<em>superfluous</em> in <code>yarn.lock</code> (from its partial perspective).</p>
<p>There‚Äôs a whole bunch of ways to addresses this, tracked in
<a href="https://github.com/yarnpkg/yarn/issues/5428">those</a> <a href="https://github.com/yarnpkg/berry/issues/1223">issues</a>,
with the emerging of various experimental Yarn plugins to fix it like
<a href="https://github.com/andreialecu/yarn-plugin-workspace-lockfile">yarn-plugin-workspace-lockfile</a>
(<a href="https://github.com/bertho-zero/yarn-plugin-workspace-lockfile">and</a>
<a href="https://github.com/milesforks/yarn-plugin-workspace-lockfile">its</a>
<a href="https://github.com/jakebailey/yarn-plugin-workspace-lockfile">forks</a>)
or <a href="https://github.com/JanVoracek/yarn-plugin-entrypoint-lockfiles">yarn-plugin-entrypoint-lockfiles</a>
that maintains individual lock files for each workspaces (or
‚Äúentrypoint‚Äù) at the cost of slightly slower installs when you add or
remove dependencies.</p>
<p>I initially used some version of this, but while writing this blog post,
I stumbled upon <a href="https://stackoverflow.com/a/73118909/4324668">this StackOverflow comment</a>
that mentions <code>yarn install --mode update-lockfile</code>. This is <em>exactly
what we want</em>! So as of Yarn 3.x, we can just do the following:</p>
<pre><code class="hljs language-sh"><span class="hljs-built_in">cp</span> yarn.lock apps/functions
<span class="hljs-built_in">cd</span> apps/functions
yarn install --mode update-lockfile
</code></pre>
<p>This will updates <code>apps/functions/yarn.lock</code> to contain <em>only</em> the
entries <em>necessary</em> for your functions, while keeping the versions that
were pinned in the original lock file. This will happily work when Cloud
Build runs <code>yarn install --immutable</code> later on. üòç</p>
<p>Again, this is something you need to do in your <code>predeploy</code> script, and
undo in your <code>postdeploy</code>.</p>
<h2 id="comparison" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#comparison"><span>Comparison</span></a></h2>
<p>Let‚Äôs compare the pros and cons of those 3 options.</p>
<p><strong><a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-good-enough-for-me-approach">Good enough</a></strong></p>
<ul>
<li>üü¢ Easy AF.</li>
<li>üü°  Doesn‚Äôt use your lock file, you‚Äôre installing nondeterministic
versions of your dependencies in production (easily fixable by taking
that specific part of the hybrid approach though).</li>
<li>üî¥ Can‚Äôt use workspace shared packages.</li>
</ul>
<p><strong><a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-full-context-approach">Full context</a></strong></p>
<ul>
<li>üü¢ Supports your lock file and any other monorepo-wide config
(Yarn version, etc.) by design and out of the box.</li>
<li>üü¢ Supports shared workspaces packages by design and out of the box.</li>
<li>üü° Need to proxy the functions <code>main</code> entrypoint in the top-level <code>package.json</code>,
as well as other things like functions <code>.env</code> files.</li>
<li>üü† Need to maintain the <code>functions.ignore</code> list which is clunky,
and gets significantly worst when using modern Yarn versions.</li>
<li>üî¥ It installs your whole monorepo dependencies instead of just your
functions dependencies.</li>
</ul>
<p><strong><a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-hybrid-approach">Hybrid</a></strong></p>
<ul>
<li>üü¢ None of the downsides of the previous approach.</li>
<li>üü° You have to copy your lock file and maybe other global requirements
like your <code>.yarnrc.yml</code>, <code>.yarn</code> folder and alike inside your
functions directory.</li>
<li>üî¥ Needs a <code>predeploy</code> and <code>postdeploy</code> script to package workspace
dependencies inside the functions directories, and recursively patch
their <code>package.json</code> to reference them with <code>file:</code>.</li>
</ul>
<h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#conclusion"><span>Conclusion</span></a></h2>
<p>Today, we went through 3 methods to make Firebase functions <em>somewhat</em>
work with a monorepo: <a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-good-enough-for-me-approach">the ‚Äúgood enough for me‚Äù approach</a>,
<a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-full-context-approach">the full context approach</a>
and <a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#the-hybrid-approach">the hybrid approach</a>. Finally, we
<a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html#comparison">compared their pros and cons</a>.</p>
<p>By now, you should have everything you need in order to make an educated
decision about which method to pick.</p>
<p>And if you find any other cool trick to make working with Firebase
functions in a monorepo easier, don‚Äôt hesitate to <a href="https://www.codejam.info/val.html#contact">let me know</a>!</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Join the discussion on <a href="https://twitter.com/valeriangalliat/status/1644495294803898369">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>How Firebase functions.ignore really works</title>
    <link href="https://www.codejam.info/2023/04/firebase-functions-ignore.html" />
    <id>https://www.codejam.info/2023/04/firebase-functions-ignore.html</id>
    <updated>2023-04-07T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>Maybe you ran into <a href="https://www.codejam.info/2023/04/firebase-functions-entity-too-large.html">Firebase functions space issues</a>,
which is not uncommon if you‚Äôre <a href="https://www.codejam.info/2023/04/firebase-functions-monorepo.html">moving Firebase functions inside a monorepo</a>.
Anyhow, you‚Äôre now playing with the <code>functions.ignore</code> list in your
<code>firebase.json</code>.</p>
<p>The ignore list is not well documented, nor is the whole
<code>firebase.json</code> really, but there is <a href="https://firebase.google.com/docs/cli/#the_firebasejson_file">a section on it</a>
in the Firebase CLI reference. I always struggle to find this page, and
I systematically find it through <a href="https://github.com/firebase/firebase-tools/issues/1409">this GitHub issue</a>
about documenting it.</p>
<p>It says the following <a href="https://firebase.google.com/docs/cli/#functions-ignored-files">about the ignore list</a>:</p>
<blockquote>
<p>The list of files ignored by default, shown in JSON format, is:</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;ignore&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
    <span class="hljs-string">&quot;.git&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-string">&quot;.runtimeconfig.json&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-string">&quot;firebase-debug.log&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-string">&quot;firebase-debug.*.log&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-string">&quot;node_modules&quot;</span>
  <span class="hljs-punctuation">]</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<p>If you add your own custom values for ignore in <code>firebase.json</code>, make
sure that you keep (or add, if it is missing) the list of files shown
above.</p>
</blockquote>
<p>Let‚Äôs dig into it in a bit more details.</p>
<h2 id="if-you-set-an-ignore-list-it-overrides-the-defaults" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-ignore.html#if-you-set-an-ignore-list-it-overrides-the-defaults"><span>If you set an ignore list, it overrides the defaults!</span></a></h2>
<p>That‚Äôs the only thing the docs say about the ignore list. It‚Äôs important
to note because otherwise, you may notice that setting <code>&quot;ignore&quot;: []</code>
ends up including <em>much more stuff</em> than not setting it, and
<a href="https://github.com/firebase/firebase-tools/issues/1602">this can be surprising</a>.</p>
<p>This is actually partially true (but mostly true to be fair).</p>
<p>We can see <a href="https://github.com/firebase/firebase-tools/blob/8976456eebf75ab9ab2a1299c0d6561f324db7f8/src/deploy/functions/prepareFunctionsUpload.ts#L75-L80">in the source code</a>
the following:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">const</span> ignore = config.<span class="hljs-property">ignore</span> || [<span class="hljs-string">&#x27;node_modules&#x27;</span>, <span class="hljs-string">&#x27;.git&#x27;</span>]

ignore.<span class="hljs-title function_">push</span>(
  <span class="hljs-string">&#x27;firebase-debug.log&#x27;</span>,
  <span class="hljs-string">&#x27;firebase-debug.*.log&#x27;</span>,
  <span class="hljs-string">&#x27;.runtimeconfig.json&#x27;</span>
)
</code></pre>
<p>This means that regardless if you customize or not <code>functions.ignore</code>,
the debug logs and runtime config will always be ignored. But it‚Äôs also
true that if you explicitly set <code>functions.ignore</code> and forget to add
<code>node_modules</code> and <code>.git</code>, those will indeed be included. Now you know.</p>
<h2 id="you-can-t-use-to-refer-to-the-functions-root" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-ignore.html#you-can-t-use-to-refer-to-the-functions-root"><span>You can‚Äôt use <code>/</code> to refer to the functions root</span></a></h2>
<p>In <code>.gitignore</code> and any sane ignore format, you can use <code>/</code> to refer to
the project root. E.g. ignoring <code>/bar</code> will ignore <code>bar</code> at the top
level, but will still include <code>foo/bar</code>.</p>
<p>This is pretty handy in a number of situations, and just a good practice
in general to be more intentional about what you <em>mean</em> to exclude. If
you have a <code>data</code> directory that you want to ignore, but you just put
<code>data</code> in your ignore file, and later on you add <code>src/api/data/load.js</code>,
guess what, <code>src/api/data</code> will be ignored and you‚Äôll be confused until
you figure out the sneaky ignore pattern. You really should be ignoring
<code>/data</code> in that case.</p>
<p>So again, we can‚Äôt do that in <code>functions.ignore</code>. Why? Ultimately, this
is because Firebase uses <a href="https://github.com/isaacs/minimatch">minimatch</a>
for this <a href="https://github.com/firebase/firebase-tools/blob/8976456eebf75ab9ab2a1299c0d6561f324db7f8/src/fsAsync.ts#L53">here</a>,
but they pass the system-wide absolute path as the first argument! So
that‚Äôs what ends up happening when using a <code>/</code> pattern (using
<code>matchBase</code> and <code>dot</code> to mimic how Firebase uses it):</p>
<pre><code class="hljs language-js">&gt; <span class="hljs-title function_">minimatch</span>(<span class="hljs-string">&#x27;/path/to/functions/foo&#x27;</span>, <span class="hljs-string">&#x27;/foo&#x27;</span>, { <span class="hljs-attr">matchBase</span>: <span class="hljs-literal">true</span>, <span class="hljs-attr">dot</span>: <span class="hljs-literal">true</span> })
<span class="hljs-literal">false</span>
</code></pre>
<p>What would be great is if the first argument was ‚Äúscoped‚Äù to the
functions root. Then we would have nice things:</p>
<pre><code class="hljs language-js">&gt; <span class="hljs-title function_">minimatch</span>(<span class="hljs-string">&#x27;/foo&#x27;</span>, <span class="hljs-string">&#x27;/foo&#x27;</span>, { <span class="hljs-attr">matchBase</span>: <span class="hljs-literal">true</span>, <span class="hljs-attr">dot</span>: <span class="hljs-literal">true</span> })
<span class="hljs-literal">true</span>
</code></pre>
<p>But because we can‚Äôt have nice things, we have to resort to using a
wider pattern (without the <code>/</code>):</p>
<pre><code class="hljs language-js">&gt; <span class="hljs-title function_">minimatch</span>(<span class="hljs-string">&#x27;/path/to/functions/foo&#x27;</span>, <span class="hljs-string">&#x27;foo&#x27;</span>, { <span class="hljs-attr">matchBase</span>: <span class="hljs-literal">true</span>, <span class="hljs-attr">dot</span>: <span class="hljs-literal">true</span> })
<span class="hljs-literal">true</span>
</code></pre>
<p>This is a problem though because as we saw earlier, it‚Äôs <em>too</em> wide:</p>
<pre><code class="hljs language-js">&gt; <span class="hljs-title function_">minimatch</span>(<span class="hljs-string">&#x27;/path/to/functions/src/foo&#x27;</span>, <span class="hljs-string">&#x27;foo&#x27;</span>, { <span class="hljs-attr">matchBase</span>: <span class="hljs-literal">true</span>, <span class="hljs-attr">dot</span>: <span class="hljs-literal">true</span> })
<span class="hljs-literal">true</span>
</code></pre>
<h2 id="can-t-use-in-base-patterns" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-ignore.html#can-t-use-in-base-patterns"><span>Can‚Äôt use <code>/</code> in base patterns</span></a></h2>
<p>The last example works only because <code>matchBase</code> is enabled, but if you
look at the <a href="https://github.com/isaacs/minimatch#matchbase"><code>matchBase</code> documentation</a>,
you see that it breaks down as soon as our pattern includes a <code>/</code>:</p>
<pre><code class="hljs language-js">&gt; <span class="hljs-title function_">minimatch</span>(<span class="hljs-string">&#x27;/path/to/functions/foo/bar&#x27;</span>, <span class="hljs-string">&#x27;foo/bar&#x27;</span>, { <span class="hljs-attr">matchBase</span>: <span class="hljs-literal">true</span>, <span class="hljs-attr">dot</span>: <span class="hljs-literal">true</span> })
<span class="hljs-literal">false</span>
</code></pre>
<p>But since root patterns don‚Äôt work <em>anyway</em> as we just saw, we can get
around this by using wildcards:</p>
<pre><code class="hljs language-js">&gt; <span class="hljs-title function_">minimatch</span>(<span class="hljs-string">&#x27;/path/to/functions/foo/bar&#x27;</span>, <span class="hljs-string">&#x27;**/foo/bar&#x27;</span>, { <span class="hljs-attr">matchBase</span>: <span class="hljs-literal">true</span>, <span class="hljs-attr">dot</span>: <span class="hljs-literal">true</span> })
<span class="hljs-literal">true</span>
</code></pre>
<h2 id="there-s-no-pattern-negation" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-ignore.html#there-s-no-pattern-negation"><span>There‚Äôs no pattern negation</span></a></h2>
<p>Pattern negation is what allows you do do something like this in a
<code>.gitignore</code>:</p>
<pre><code class="hljs language-gitignore">/dist/*
!/dist/package.json
</code></pre>
<p>This would ignore everything in the <code>dist</code> directory except for
<code>dist/package.json</code>.</p>
<p>This is particularly handy in a number of situations, especially when
you consider the alternative which is to <em>explicitly ignore every single
file or directory you have in <code>dist</code></em>. And obviously, remembering to add
any <em>new</em> file to your ignore list when you create them, or when tools
you use create other random files you don‚Äôt even know exist.</p>
<p>All that to say, you guessed it, that Firebase <code>functions.ignore</code>
<a href="https://github.com/firebase/firebase-tools/issues/2677">doesn‚Äôt support this</a>.</p>
<h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-ignore.html#conclusion"><span>Conclusion</span></a></h2>
<p>That‚Äôs all I have for today. If you‚Äôre working with Firebase
<code>functions.ignore</code> right now and noticed a few quirks, I hope this made
it easier for you to understand what‚Äôs going on.</p>
<p>And if you‚Äôre trying to fix a Firebase functions source being too large
to be deployed, I also <a href="https://www.codejam.info/2023/04/firebase-functions-entity-too-large.html">wrote a post</a>
with tips to troubleshoot it.</p>
<p>Peace. ‚úåÔ∏è</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Firebase functions: debugging upload error EntityTooLarge</title>
    <link href="https://www.codejam.info/2023/04/firebase-functions-entity-too-large.html" />
    <id>https://www.codejam.info/2023/04/firebase-functions-entity-too-large.html</id>
    <updated>2023-04-07T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>So during a <code>firebase deploy</code>, you ran into the following error:</p>
<pre><code class="hljs">Upload Error: HTTP Error: 400
</code></pre>
<pre><code class="hljs language-xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">Error</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">Code</span>&gt;</span>EntityTooLarge<span class="hljs-tag">&lt;/<span class="hljs-name">Code</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">Message</span>&gt;</span>Your proposed upload is larger than the maximum object size specified in your Policy Document.<span class="hljs-tag">&lt;/<span class="hljs-name">Message</span>&gt;</span>
  <span class="hljs-tag">&lt;<span class="hljs-name">Details</span>&gt;</span>Content-length exceeds upper bound on range<span class="hljs-tag">&lt;/<span class="hljs-name">Details</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">Error</span>&gt;</span>
</code></pre>
<p>What to do from there?</p>
<p>This happens because your functions source was too large: over 100 MB
for the compressed source, or 500 MB for the uncompressed source and its
dependencies, as documented in <a href="https://cloud.google.com/functions/quotas#resource_limits">resource limits</a>.</p>
<p>Now, there may be ways you can reduce that, particularly using the
<a href="https://firebase.google.com/docs/cli/#functions-ignored-files"><code>functions.ignore</code></a>
list in <code>firebase.json</code> to ignore unnecessary (and possibly heavy)
files.</p>
<p>But it‚Äôs not necessarily easy to write this list. The ignore patterns
are not well documented and can be quirky, enough that I <a href="https://www.codejam.info/2023/04/firebase-functions-ignore.html">wrote another
blog post</a> to demistify them. You can
easily end up in a loop of trial and error until you get the patterns
right, and some guesswork to find what files and directories can be
exceeding the size limit.</p>
<p>Firebase doesn‚Äôt give us any way to inspect the functions packed source
to diagnose what failed to be ignored and is taking all that space.
Luckily, it‚Äôs pretty easy to hack that around.</p>
<h2 id="catching-the-temporary-zip-as-it-s-generated" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-entity-too-large.html#catching-the-temporary-zip-as-it-s-generated"><span>Catching the temporary ZIP as it‚Äôs generated</span></a></h2>
<p>By looking at the source code of Firebase, we can see they
<a href="https://github.com/firebase/firebase-tools/blob/b0798fb1fe96499e1404d6fea6c181735e3a8f11/src/deploy/functions/prepareFunctionsUpload.ts#L63">use the <code>tmp</code> module</a>
in order to generate the ZIP archive for Cloud Functions.</p>
<p>Let‚Äôs see where <code>tmp</code> creates the files. On macOS, that‚Äôs what I got:</p>
<pre><code class="hljs language-console"><span class="hljs-meta prompt_">$ </span><span class="language-bash">node -p <span class="hljs-string">&quot;require(&#x27;tmp&#x27;).fileSync({ prefix: &#x27;firebase-functions-&#x27;, postfix: &#x27;.zip&#x27; }).name&quot;</span></span>
/var/folders/8g/6ch743rn6p990xxbsd757yfm0000gn/T/firebase-functions--35549-LDS1bLg78ajZ.zip
</code></pre>
<p>Sweet. So we can look for <code>firebase-functions-*.zip</code> inside
<code>/var/folders</code> to find the archive that‚Äôs being uploaded!</p>
<p>Now, we just have to watch for those logs during the deploy:</p>
<pre><code class="hljs">i  functions: preparing . directory for uploading...
i  functions: packaged /path/to/repo (123.45 MB) for uploading
</code></pre>
<p>This tells us that the archive is ready. Be quick (or cancel the
deploy), because Firebase will clean it up pretty fast!</p>
<p>You can use a command like this to show the creation time of the files
that matched. Then just pick the most recent one.</p>
<pre><code class="hljs language-sh">find /var/folders -name <span class="hljs-string">&#x27;firebase-functions-*.zip&#x27;</span> -<span class="hljs-built_in">ls</span> 2&gt; /dev/null
</code></pre>
<p>The <code>2&gt; /dev/null</code> part is to ignore the error stream since a lot of
stuff in <code>/var/folders</code> will get permission denied errors.</p>
<p>Now you have the source ZIP file, you can uncompress it and see what
failed to be ignored, or what‚Äôs left in there that is too heavy and
needs to be added to the ignore list!</p>
<div class="note">
<p><strong>Note:</strong> while trying to find leftover files that are too large,
<a href="https://dev.yorhel.nl/ncdu">ncdu</a> is really useful. It‚Äôs a small CLI
tool that allows to browse a directory, showing the largest files and
folders on top, with their size. I can only highly recommend it when you
need to identify large files.</p>
<p>You may install it with one of the following commands, depending on your
system:</p>
<pre><code class="hljs language-sh">apt install ncdu
pacman -S ncdu
brew install ncdu
</code></pre>
</div>
<h2 id="an-even-better-solution" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/firebase-functions-entity-too-large.html#an-even-better-solution"><span>An even better solution</span></a></h2>
<p>While this first solution worked, I didn‚Äôt like having to catch the ZIP
files fast before Firebase removes it. I kept digging through the code,
and I found a way to call the Firebase archiving code directly,
instead of running <code>firebasde deploy</code>!</p>
<pre><code class="hljs language-js"><span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;firebase-tools/lib/deploy/functions/prepareFunctionsUpload&#x27;</span>).<span class="hljs-title function_">prepareFunctionsUpload</span>(
  process.<span class="hljs-title function_">cwd</span>(),
  <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;./firebase.json&#x27;</span>).<span class="hljs-property">functions</span>
).<span class="hljs-title function_">then</span>(<span class="hljs-function"><span class="hljs-params">x</span> =&gt;</span> <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(x.<span class="hljs-property">pathToSource</span>))
</code></pre>
<p>Running this from our Firebase root directory (the one where
<code>firebase.json</code> is in), it will generate the ZIP archive and output its
temporary path!</p>
<p>You can then decompress it and analyze it as we just saw.</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Vercel: custom preview domain for free?</title>
    <link href="https://www.codejam.info/2023/04/vercel-custom-preview-domain.html" />
    <id>https://www.codejam.info/2023/04/vercel-custom-preview-domain.html</id>
    <updated>2023-04-06T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>If you host your app on Vercel, you must be familiar with how preview
deployments are a first-class citizen, and each pull request you make gets
a preview deployment.</p>
<p>Those preview deployments default to be hosted on <code>vercel.app</code>, and
Vercel provisions two preview domains for it, with the following
patterns:</p>
<ul>
<li><code>{app}-{id}-{org}.vercel.app</code></li>
<li><code>{app}-git-{branch}-{org}.vercel.app</code></li>
</ul>
<p>So if you make a PR on an app called <code>my-app</code> in an organization
<code>my-org</code>, on a branch called <code>hello-world</code>, and the deployment ID for
this commit was <code>lkj8trp27</code>, your URLs would be:</p>
<ul>
<li><code>https://my-app-lkj8trp27-my-org.vercel.app/</code></li>
<li><code>https://my-app-git-hello-world-my-org.vercel.app/</code></li>
</ul>
<p>But what if you want, for example, to allow OAuth on your preview
domains? Whitelisting <code>vercel.app</code> is out of the question since it would
allow <em>any</em> Vercel website (including an attacker‚Äôs website) to be a
valid redirect URI for our OAuth provider!</p>
<p>And we can‚Äôt typically whitelist a domain pattern like <code>*-my-org.vercel.app</code>,
not that this would be a good idea anyway because <strong>this pattern is
<em>not</em> private to your organization</strong>. Any random Vercel user can use
it in their own app!</p>
<p>Then, it would be useful to use your own domain instead of <code>vercel.app</code>.
Turns out Vercel supports this, and <a href="https://vercel.com/docs/concepts/deployments/generated-urls#preview-deployment-suffix">charges $100/month</a>
for it! Steep.</p>
<p>Steep, but if you‚Äôre looking for a turnkey solution, it‚Äôs definitely
worth it. Otherwise, keep reading.</p>
<h2 id="using-the-vercel-cli" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/vercel-custom-preview-domain.html#using-the-vercel-cli"><span>Using the Vercel CLI</span></a></h2>
<p>An interesting thing in the Vercel CLI is that it lets us manually
associate a custom domain to a given deployment using
<a href="https://vercel.com/docs/cli/alias"><code>vercel alias</code></a>. üòè</p>
<p>Let‚Äôs say <code>codejam.info</code> is part of my Vercel-managed domains:</p>
<pre><code class="hljs language-sh">vercel <span class="hljs-built_in">alias</span> <span class="hljs-built_in">set</span> my-app-lkj8trp27-my-org.vercel.app hello-world.preview.codejam.info
</code></pre>
<p>This will associate the deployment example from earlier to my custom
domain!</p>
<p>We can literally put anything we want under <code>codejam.info</code> there, and it
will happily generate a SSL certificate for that arbitrary subdomain,
and associate it to our deployment.</p>
<p>This will work as long you associated a wildcard subdomain on your DNS
to Vercel, like <code>*.preview.codejam.info</code> in our example.</p>
<p>This is a good start, but it doesn‚Äôt scale!</p>
<h2 id="using-the-vercel-api" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/vercel-custom-preview-domain.html#using-the-vercel-api"><span>Using the Vercel API</span></a></h2>
<p>Luckily, the Vercel API exposes an endpoint to do just the same thing:
<a href="https://vercel.com/docs/rest-api/endpoints#assign-an-alias"><code>POST /v2/deployments/{id}/aliases</code></a>.
In our example, we can call it with:</p>
<pre><code class="hljs language-json"><span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;alias&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;hello-world.preview.codejam.info&quot;</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<p>How do we find the deployment ID though? We need the full deployment ID,
something looking like <code>dpl_hgLKkCqMExSzNpTtA3Dy6sVfWuYj</code>.</p>
<p>Vercel gives us a handy <a href="https://vercel.com/docs/rest-api/endpoints#get-a-deployment-by-id-or-url"><code>GET /v13/deployments/{idOrUrl}</code></a>
endpoint for this, where we can pass our deployment URL and get the
deployment object back, including its full <code>id</code>.</p>
<p>By combining those two endpoints, we can dynamically associate our
custom domain to any Vercel preview deployment. üôè</p>
<h2 id="getting-an-api-token" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/vercel-custom-preview-domain.html#getting-an-api-token"><span>Getting an API token</span></a></h2>
<p>In order to call the API, we need to pass a bearer token in the
<code>Authorization</code> header. You can create a token from your
<a href="https://vercel.com/account/tokens">Vercel account settings</a>.</p>
<p>Then, you can put it in your app‚Äôs environment variables, e.g. as
<code>VERCEL_TOKEN</code>, so it‚Äôs available in your server-side code environment.</p>
<h2 id="associating-the-domain-on-the-fly" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/vercel-custom-preview-domain.html#associating-the-domain-on-the-fly"><span>Associating the domain on the fly</span></a></h2>
<p>From there, we can detect when we‚Äôre running under a <code>vercel.app</code>
preview domain, call the API to associate our own custom domain, and
finally redirect to it. This will add a bit of delay when loading our
preview deployments from the <code>vercel.app</code> domains, but no big deal.</p>
<p>Where you hook in order to do that is up to you. <code>_app.jsx</code> may be a
good start, or maybe some component that‚Äôs included in all of your
pages, maybe just the home page if you don‚Äôt expect any deep link on
your <code>vercel.app</code> preview domains, or maybe even somewhere in
<code>getServerSideProps</code>?</p>
<p>If you do this client-side, you‚Äôll want to add an API route or go
through a SSR page that will be doing the call to the Vercel API (you
don‚Äôt want to expose your Vercel API token client-side), but if you‚Äôre
hooking directly in <code>getServerSideProps</code>, you can skip that step.</p>
<p>On the client, you could do something like this:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">const</span> router = <span class="hljs-title function_">useRouter</span>()

<span class="hljs-keyword">if</span> (location.<span class="hljs-property">host</span>.<span class="hljs-title function_">endsWith</span>(<span class="hljs-string">&#x27;-my-org.vercel.app&#x27;</span>)) {
  <span class="hljs-comment">// Preview env</span>
  router.<span class="hljs-title function_">push</span>(<span class="hljs-string">&#x27;/preview-redirect&#x27;</span>)
}
</code></pre>
<p>Then, implement a <code>preview-redirect</code> page to associate your custom
domain to the current preview environment, then redirect to it.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">export</span> <span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">getServerSideProps</span> (context) {
  <span class="hljs-keyword">const</span> deployment = <span class="hljs-keyword">await</span> <span class="hljs-title function_">getDeployment</span>(context.<span class="hljs-property">req</span>.<span class="hljs-property">headers</span>.<span class="hljs-property">host</span>)

  <span class="hljs-keyword">const</span> domain = <span class="hljs-string">`pr-<span class="hljs-subst">${deployment.gitSource.prId}</span>.preview.codejam.info`</span>

  <span class="hljs-keyword">await</span> <span class="hljs-title function_">associateDomainToDeployment</span>(deployment.<span class="hljs-property">id</span>, domain)

  <span class="hljs-keyword">return</span> {
    <span class="hljs-attr">redirect</span>: {
      <span class="hljs-attr">destination</span>: <span class="hljs-string">`https://<span class="hljs-subst">${domain}</span>`</span>
    }
  }
}
</code></pre>
<p>Where <code>getDeployment</code> is a wrapper to <a href="https://vercel.com/docs/rest-api/endpoints#get-a-deployment-by-id-or-url"><code>GET /v13/deployments/{idOrUrl}</code></a>,
and <code>associateDomainToDeployment</code> wraps <a href="https://vercel.com/docs/rest-api/endpoints#assign-an-alias"><code>POST /v2/deployments/{id}/aliases</code></a>
(writing those is left as an exercise to the reader).</p>
<p>Here, I chose to prefix the domain with <code>pr-</code> and the PR number, but
you‚Äôre free to construct your preview domains however you want.</p>
<p>You‚Äôll notice this works the first time, but obviously if you open again
the <code>vercel.app</code> preview URL, it will fail because the domain was
already assigned! To cover that, you need to call <a href="https://vercel.com/docs/rest-api/endpoints#list-deployment-aliases"><code>/v2/deployments/{id}/aliases</code></a>
and redirecting to the existing domain if you already associated it
before.</p>
<p>We can add something like this in the beginning of our previous
function:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">const</span> aliases = <span class="hljs-keyword">await</span> <span class="hljs-title function_">getDeploymentAliases</span>(deployment.<span class="hljs-property">id</span>)

<span class="hljs-keyword">const</span> existingDomain = aliases.<span class="hljs-title function_">find</span>(<span class="hljs-function"><span class="hljs-params">alias</span> =&gt;</span>
  alias.<span class="hljs-property">alias</span>.<span class="hljs-title function_">endsWith</span>(<span class="hljs-string">&#x27;.preview.codejam.info&#x27;</span>)
)

<span class="hljs-keyword">if</span> (existingDomain) {
  <span class="hljs-keyword">return</span> {
    <span class="hljs-attr">redirect</span>: {
      <span class="hljs-attr">destination</span>: <span class="hljs-string">`https://<span class="hljs-subst">${existingDomain.alias}</span>`</span>
    }
  }
}
</code></pre>
<p>After this, you should have your free custom preview domains working,
congrats!</p>
<h2 id="about-vercel-certificates" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/vercel-custom-preview-domain.html#about-vercel-certificates"><span>About Vercel certificates</span></a></h2>
<p>However, you may realize this is bloating your domain‚Äôs SSL certificates
list on Vercel. Every single preview deployment will add a new entry in
your SSL certificates list, and because the Vercel UI for this doesn‚Äôt
really expect an infinitely growing list of certificates, it‚Äôll make it
a pain for you to manage your ‚Äúactual‚Äù certificates!</p>
<p>To prevent this, you need to manually create a wildcard certificate for
the domain you use for your preview deployments. In the example featured
in this post, that would be <code>*.preview.codejam.info</code>.</p>
<p>Vercel is smart enough to notice when we associate a new domain to a
deployment, that a wildcard certificate covering it already exists, and
so doesn‚Äôt create an <em>individual</em> certificate for <em>that</em> particular
preview. This will keep your certificates list clean and tidy!</p>
<p>You can‚Äôt create the wildcard certificate from the dashboard directly,
but you can do so with the CLI using <a href="https://vercel.com/docs/cli/certs#extended-usage"><code>vercel certs issue</code></a>.</p>
<pre><code class="hljs language-sh">vercel certs issue <span class="hljs-string">&#x27;*.preview.codejam.info&#x27;</span>
</code></pre>
<p>Note that this will only work if you use Vercel‚Äôs nameservers. This
means the following won‚Äôt work (e.g. in the <code>codejam.info</code> DNS zone):</p>
<pre><code class="hljs">*.preview CNAME cname.vercel-dns.com
</code></pre>
<p>But the following will work:</p>
<pre><code class="hljs">preview NS ns1.vercel-dns.com
preview NS ns2.vercel-dns.com
</code></pre>
<h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/vercel-custom-preview-domain.html#conclusion"><span>Conclusion</span></a></h2>
<p>If you made it here, congrats! You now have everything you need in order
to implement your own custom preview domains, without paying Vercel big
money for it.</p>
<p>Is going through all of this worth saving $100/month? That‚Äôs up to you.
But as far as I‚Äôm concerned, the joy of putting together this little
system was well worth the savings. üòú</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Join the discussion on <a href="https://twitter.com/valeriangalliat/status/1644037565907890180">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Recovering Kobo eReader highlights after an accidental factory reset!</title>
    <link href="https://www.codejam.info/2023/04/kobo-highlights-recover.html" />
    <id>https://www.codejam.info/2023/04/kobo-highlights-recover.html</id>
    <updated>2023-04-05T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>The other day my Kobo eReader had some issues where it was instantly
dying when not plugged in, despite showing a full battery! This happened
after I let it charge overnight on an external battery. üò¨</p>
<p>I restarted it a few times, hoping this would fix the issue, but without
luck. Until‚Ä¶ the last restart was a bit different: <strong>it asked me to
chose a language</strong>.</p>
<p>At this very moment, I knew I fucked up.</p>
<p>I somehow managed to accidentally factory reset my eReader!</p>
<h2 id="limiting-the-damage-make-a-disk-image" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/kobo-highlights-recover.html#limiting-the-damage-make-a-disk-image"><span>Limiting the damage: make a disk image</span></a></h2>
<p>Because I was in denial, I didn‚Äôt instantly accept that a factory reset
had happened. So I went on, picked my language and connected it again to
my Wi-Fi, so I could access the main screen.</p>
<p>Indeed, all my books were gone. Not a big deal because I have a copy on
my computer. More problematic though, my highlights and notes were
gone too!</p>
<p>I do back them up once in a while, but I‚Äôve been neglecting that, so my
last backup was over 4 months old! I‚Äôve read a bunch of books since
then, and highlighted quite some stuff I would have been happy to go
through again in the future. Bummer.</p>
<p>To prevent further damage, once I realized my data was gone, I stopped
doing anything with the device that could write to the storage.</p>
<p>As any good data recovery starts, I plugged it to my laptop and cloned
the entire storage to an image file:</p>
<pre><code class="hljs language-sh"><span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=/dev/sdb of=kobo-raw-disk bs=1M
</code></pre>
<div class="note">
<p><strong>Note:</strong> I used <code>dd</code> because the storage of the eReader was presumably
healthy, if not for the fact that a factory reset had happened.</p>
<p>If I had actual corruption issues with the disk, it would have been good
to use <a href="https://www.gnu.org/software/ddrescue/"><code>ddrescue</code></a>.</p>
</div>
<h2 id="testdisk-trying-to-recover-the-original-partition" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/kobo-highlights-recover.html#testdisk-trying-to-recover-the-original-partition"><span>TestDisk: trying to recover the original partition</span></a></h2>
<p>The first thing I tried was to use <a href="https://www.cgsecurity.org/wiki/TestDisk">TestDisk</a>
to recover the partition table from before the factory reset, but this
wasn‚Äôt successful.</p>
<p>I think it would have been a more appropriate tool to recover specific
partitions that were deleted without being written over, or if only the
partition table was corrupted or lost.</p>
<p>Here though, I think the factory reset process overwrote too much data
to make TestDisk successful. It didn‚Äôt hurt to try though!</p>
<h2 id="photorec-extract-recognizable-file-formats-from-raw-disk" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/kobo-highlights-recover.html#photorec-extract-recognizable-file-formats-from-raw-disk"><span>PhotoRec: extract recognizable file formats from raw disk</span></a></h2>
<p>Had I been successful with TestDisk, I would have recovered the original
partition and filesystem, with the entire directory structure and
filenames.</p>
<p>As a fallback though, I decided to use
<a href="https://www.cgsecurity.org/wiki/PhotoRec">PhotoRec</a>
(another tool by the same creators as TestDisk), to try and identify
well-known file formats from the raw disk image.</p>
<p>The inconvenient of that is that we lose all the filenames and their
arborescence, but I can live with that.</p>
<p>The output of PhotoRec was 7304 files, split in directories containing
500 files each, going from <code>recup_dir.1</code> to <code>recup_dir.15</code>.</p>
<p>Each file is named after the logical sector it was found at, which is
not very useful to me here, and has the extension of the filetype that
was identified.</p>
<p>Here‚Äôs all the extensions it was able to find, along with the number of
files for that extension:</p>
<pre><code class="hljs language-console"><span class="hljs-meta prompt_">$ </span><span class="language-bash">find photorec-out -<span class="hljs-built_in">type</span> f | sed <span class="hljs-string">&#x27;s/.*\.//&#x27;</span> | <span class="hljs-built_in">sort</span> | <span class="hljs-built_in">uniq</span> -c</span>
      5 c
      1 csv
      2 elf
     77 epub
      5 f
     11 gz
     18 h
     18 html
      1 ico
   1493 ini
    154 java
   3964 jpg
      4 pdf
      3 plist
    161 png
     27 py
      6 sqlite
      1 sxw
      1 tar
   1343 txt
      1 xml
      8 zip
</code></pre>
<h2 id="trying-to-recover-the-sqlite-databases" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/kobo-highlights-recover.html#trying-to-recover-the-sqlite-databases"><span>Trying to recover the SQLite databases</span></a></h2>
<p>I knew that Kobo stores the highlights in a SQLite database, located in
<code>.kobo/KoboReader.sqlite</code>. If this was intact, I had all my highlights
back!</p>
<p>I tried to open the 6 identified SQLite databases, but sadly, the few
that weren‚Äôt corrupted didn‚Äôt have the tables I was looking for, and the
only one that was about as large as what I would expect for my
<code>KoboReader.sqlite</code> (a bit bigger than the one of my last backup) was
corrupted.</p>
<p>I tried using the <a href="https://www.sqlite.org/recovery.html"><code>.recover</code></a>
SQLite command, but that didn‚Äôt work either:</p>
<pre><code class="hljs language-sh">sqlite3 corrupt.db .recover &gt; data.sql
</code></pre>
<p>I tried <a href="https://www.nucleustechnologies.com/blog/best-6-sqlite-database-recovery-tools/">a whole bunch</a>
of different proprietary tools to recover corrupted SQLite databases,
but none of them was able to do anything.</p>
<p>When I was looking at the raw contents of the SQLite database though,
e.g. using <code>less</code> directly on the binary file, or using <code>xxd</code> or
<code>strings</code>, I could see some highlights data, but definitely not as much
as I expected.</p>
<h2 id="looking-at-the-raw-disk-directly" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/kobo-highlights-recover.html#looking-at-the-raw-disk-directly"><span>Looking at the raw disk directly</span></a></h2>
<p>I tried pretty hard for that SQLite database, but I had to come to the
fact it wasn‚Äôt gonna be my savior here. However there was something I
liked about this idea of looking at the raw binary data directly.</p>
<p>I had a light of hope when I decided to <code>grep</code> into the corrupted SQLite
database, as well as the raw disk image, for fragments of sentences I
definitely remembered having highlighted. The binary files, in fact,
matched! There was after all a chance that at least some of my
highlights were there, but it wasn‚Äôt exactly clear where, how many, and
under what form.</p>
<p>Since I couldn‚Äôt do anything with the database, I decided to focus on
the raw disk image. Using <code>less</code> and <code>xxd</code> to visualize it wasn‚Äôt very
successful (it took too long to go through the huge amounts of
unreadable data to notice anything actually usable). However, <code>strings</code>,
that only outputs printable data, made it much easier for me to filter
through its contents.</p>
<p>When I looked up in the <code>strings</code> output for some sentence I remembered
highlighting, it was, in fact, part of a fairly large XML string! What?</p>
<h2 id="looking-at-the-recovered-xml-files" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/kobo-highlights-recover.html#looking-at-the-recovered-xml-files"><span>Looking at the recovered XML files</span></a></h2>
<p>It turned out that whole time, the Kobo eReader was storing annotations
not only in a database, but also in XML files!</p>
<p>For some reason PhotoRec identified them all as <code>txt</code> instead of <code>xml</code>,
but it was pretty easy to extract them. All the annotations XML started
with <code>&lt;annotationSet</code>.</p>
<pre><code class="hljs language-sh">grep -R --files-with-match <span class="hljs-string">&#x27;&lt;annotationSet&#x27;</span> photorec-out/**/*.txt
</code></pre>
<p>With <code>-R</code> for recursive, and <code>--files-with-match</code>, this command printed
the filenames of all the files that contained <code>&lt;annotationSet</code>.</p>
<p>I copied them to a separate directory for analysis.</p>
<p>I quickly identified a pattern: each XML file contained all the
annotations for a given book, but I had many different XML files for the
same books, with more or less annotations in them. It was like I had the
history of every single time each file was written to as I added new
highlights!</p>
<p>I wrote a quick script to validate this theory, and surely, the XML with
the most annotations for each book systematically contained all of the
annotations of the other, smaller XML files for that same book. This
allowed me to filter quite a lot amongst those files.</p>
<h2 id="integrity-check-comparing-with-my-backup-database" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/kobo-highlights-recover.html#integrity-check-comparing-with-my-backup-database"><span>Integrity check: comparing with my backup database</span></a></h2>
<p>Remember, I still had that copy of the database from a few months ago. I
decided to check the integrity of the XMLs I recovered against what was
in my backup, so I wrote a quick script to compare them.</p>
<p>I wasn‚Äôt happy with what I found though. For the books for which I did
have a backup, this showed that I recovered <em>most</em> of the highlights in
the XML files, but not <em>all</em>. This means that for the ones where I
didn‚Äôt have a backup, I couldn‚Äôt hope to have recovered <em>everything</em>.</p>
<p>This was better than nothing, but I was pretty uncomfortable with that
state of having recovered <em>some</em> data but not knowing what data I had
actually lost. üòÖ</p>
<p>I scratched my head a bit, and surely enough, I was able to recall a few
words for a sentence that I definitely remembered highlighting recently,
and that was not part of the XMLs that PhotoRec recovered.</p>
<p>What was exciting though, is that I could successfully <code>grep</code> for this
sentence in the binary disk image! Did PhotoRec miss some XML files
somehow?</p>
<h2 id="grepping-for-xml-files-on-the-raw-disk-directly" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/kobo-highlights-recover.html#grepping-for-xml-files-on-the-raw-disk-directly"><span>Grepping for XML files on the raw disk directly!</span></a></h2>
<p>Only one way to know. Since I knew exactly the patterns to look for at
the start and end of the annotations XML, I could find them in the raw
disk image, extract the byte offset, and then <code>dd</code> everything
in between each start and end offset!</p>
<p>Using <code>grep</code> with <code>--text</code> to force it to treat the disk binary data as text,
and <code>--byte-offset</code> to get the byte offset of the matches, I was able to
extract the position of the markers:</p>
<pre><code class="hljs language-sh">grep --byte-offset --text -o <span class="hljs-string">&#x27;&lt;annotationSet&#x27;</span> kobo-raw-disk &gt; xml-start-markers-offsets
grep --byte-offset --text -o <span class="hljs-string">&#x27;&lt;/annotationSet&gt;&#x27;</span> kobo-raw-disk &gt; xml-end-markers-offsets
</code></pre>
<p>Each file looked like this:</p>
<pre><code class="hljs">199089105:&lt;annotationSet
222029926:&lt;annotationSet
799936271:&lt;annotationSet
830499395:&lt;annotationSet
839015506:&lt;annotationSet
</code></pre>
<p>From there, I used the <code>paste</code> command to merge both files side by side
(using a <code>cut</code> subshell in order to keep only the offset before the
<code>:</code>):</p>
<pre><code class="hljs language-sh"><span class="hljs-built_in">paste</span> &lt;(<span class="hljs-built_in">cut</span> -d: -f1 xml-start-markers-offsets) &lt;(<span class="hljs-built_in">cut</span> -d: -f1 xml-end-markers-offsets)
</code></pre>
<p>Which gave me something like:</p>
<pre><code class="hljs">199089105	199091941
222029926	222031623
799936271	799937945
830499395	830499742
839015506	839016589
</code></pre>
<p>I could then use <code>dd</code> to extract the bytes from the raw disk image
in between those offsets:</p>
<pre><code class="hljs language-sh">start=199089105
end=199091941
<span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=kobo-raw-disk of=raw-xml-dump/<span class="hljs-variable">$start</span>.xml bs=1 skip=<span class="hljs-variable">$start</span> count=$((end - start + <span class="hljs-number">16</span>))
</code></pre>
<p>What‚Äôs the 16 in that command? It‚Äôs the length of the end marker
<code>&lt;/annotationSet&gt;</code>! Because <code>grep</code> gave us the offset of the <em>start</em> of
the search.</p>
<p>So I piped both of those commands together through a <code>while</code> loop to
extract all the XMLs:</p>
<pre><code class="hljs language-sh"><span class="hljs-built_in">paste</span> &lt;(<span class="hljs-built_in">cut</span> -d: -f1 xml-start-markers-offsets) &lt;(<span class="hljs-built_in">cut</span> -d: -f1 xml-end-markers-offsets) \
   | <span class="hljs-keyword">while</span> <span class="hljs-built_in">read</span> start end; <span class="hljs-keyword">do</span>
      <span class="hljs-built_in">dd</span> <span class="hljs-keyword">if</span>=kobo-raw-disk of=raw-xml-dump/<span class="hljs-variable">$start</span>.xml bs=1 skip=<span class="hljs-variable">$start</span> count=$((end - start + <span class="hljs-number">16</span>))
   <span class="hljs-keyword">done</span>
</code></pre>
<p>With that method, I was able to find 287 XMLs, where PhotoRec only
recovered 234!</p>
<p>I ran my integrity check script against this new output, and was
astonished: <em>it was a perfect match</em>!</p>
<p>Every single highlight I had i my backup were found in those XMLs, which
gave me confidence that the ones that were <em>not</em> in my backup were most
likely there too.</p>
<h2 id="confidence-checking" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/kobo-highlights-recover.html#confidence-checking"><span>Confidence checking</span></a></h2>
<p>In order to be even more sure about this, I checked the last recovered
highlight for all of the books I‚Äôve read since my last backup. Each
highlight contains a <code>progress</code> attribute, between 0 and 1, representing
how far in the book it‚Äôs situated.</p>
<p>For all of the books I finished, the last highlight was pretty close to
the end of the book, and since we saw earlier that the file with the
most highlights always contained the highlights of the previous versions
of that file, I was pretty confident I‚Äôve recovered all of my data at
that point! üéâ</p>
<h2 id="wrapping-up" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/kobo-highlights-recover.html#wrapping-up"><span>Wrapping up</span></a></h2>
<p>This was a rollercoaster of emotions! Between losing all my highlights,
recovering a database that turned out to be unusable, finding the XMLs
with PhotoRec but noticing they were incomplete, and finally using
<code>grep</code> and <code>dd</code> to extract the XML files myself directly from the raw
disk. Luckily, I was able to recover everything I was looking for thanks
to the last method!</p>
<p>But really, the morale of this story is that, <strong>if you care about some
data, you better make sure that you back it up</strong>, and that you do so
rigorously and frequently (or even better, automatically).</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Join the discussion on <a href="https://twitter.com/valeriangalliat/status/1643793553032617986">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Postgres casting to bit vs. varbit vs. &quot;bit&quot; (with quotes)</title>
    <link href="https://www.codejam.info/2023/04/postgres-bit-varbit.html" />
    <id>https://www.codejam.info/2023/04/postgres-bit-varbit.html</id>
    <updated>2023-04-05T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>Here‚Äôs a few things to know if you‚Äôre working with bit strings in
Postgres.</p>
<h2 id="bit-means-bit-1" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/postgres-bit-varbit.html#bit-means-bit-1"><span><code>bit</code> means <code>bit(1)</code></span></a></h2>
<p>As documented <a href="https://www.postgresql.org/docs/8.0/functions-bitstring.html">here</a>,
<code>bit</code> is an alias for <code>bit(1)</code>, so it will only keep the least
significant bit.</p>
<pre><code class="hljs">42::bit(10)  0000101010
42::bit(1)            0
42::bit               0

43::bit(10)  0000101011
43::bit(1)            1
43::bit               1
</code></pre>
<h2 id="define-bit-strings-with-b-101010" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/postgres-bit-varbit.html#define-bit-strings-with-b-101010"><span>Define bit strings with <code>B'101010'</code></span></a></h2>
<p>You can define bit strings with the <code>B</code> prefix:</p>
<pre><code class="hljs">B&#x27;101010&#x27;             101010
B&#x27;101010&#x27;::int        42
pg_typeof(B&#x27;101010&#x27;)  bit
</code></pre>
<h2 id="cast-a-string-to-bit" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/postgres-bit-varbit.html#cast-a-string-to-bit"><span>Cast a string to <code>bit</code></span></a></h2>
<p>If you have a string made of only 0s and 1s, you can cast it to a bit
string too! Useful for dynamically generating bit sequences.</p>
<pre><code class="hljs">&#x27;101010&#x27;::bit(10)  1010100000
</code></pre>
<p>But we instantly notice an interesting difference: when casting from an
integer as we did earlier, the truncation (or padding otherwise) was
right-aligned, while when casting from a string, it‚Äôs left-aligned.</p>
<pre><code class="hljs">12::bit(4)       1100
12::bit(2)         00
&#x27;1100&#x27;::bit(4)   1100
&#x27;1100&#x27;::bit(2)   11
B&#x27;1100&#x27;::bit(2)  11
</code></pre>
<h2 id="dynamic-length-bit-strings" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/postgres-bit-varbit.html#dynamic-length-bit-strings"><span>Dynamic length bit strings</span></a></h2>
<p>What if you‚Äôre generating a bit string from‚Ä¶ an actual string, but you
don‚Äôt know its length in advance? You can always use a fixed length
that‚Äôs larger than what you think you‚Äôll need, but that may not be very
efficient.</p>
<p>Instead, you can use the <code>bit varying</code> type, also known as <code>varbit</code>!</p>
<pre><code class="hljs">&#x27;101010&#x27;::bit varying             101010
&#x27;101010&#x27;::varbit                  101010
pg_typeof(&#x27;101010&#x27;::bit varying)  bit varying
pg_typeof(&#x27;101010&#x27;::varbit)       bit varying
</code></pre>
<p>Alternatively, there‚Äôs an <a href="https://dba.stackexchange.com/a/204838/240451">internal, undocumented <code>&quot;bit&quot;</code> type</a>
(to not be mistaken with <code>bit</code> without the quotes), which will
automatically cast to a static-sized <code>bit</code>, <em>but inferring the size form
the input</em>!</p>
<pre><code class="hljs">&#x27;101010&#x27;::bit                1
&#x27;101010&#x27;::varbit             101010
pg_typeof(&#x27;101010&#x27;::varbit)  bit varying
&#x27;101010&#x27;::&quot;bit&quot;              101010
pg_typeof(&#x27;101010&#x27;::&quot;bit&quot;)   bit
</code></pre>
<div class="note">
<p><strong>Note:</strong> there‚Äôs probably very little situations where you‚Äôd need
<code>&quot;bit&quot;</code> instead of <code>varbit</code>, but at least now you know it exists. I
wouldn‚Äôt recommend relying on a type that‚Äôs internal to Postgres and
undocumented though!</p>
</div>
<p>The <code>&quot;bit&quot;</code> magic is not transparent to <a href="https://www.postgresql.org/docs/current/app-psql.html"><code>\gdesc</code></a>
though:</p>
<pre><code class="hljs">&#x27;101010&#x27;::bit     1       bit(1)
&#x27;101010&#x27;::bit(6)  101010  bit(6)
&#x27;101010&#x27;::varbit  101010  bit varying
&#x27;101010&#x27;::&quot;bit&quot;   101010  &quot;bit&quot;
</code></pre>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Archiving Google Photos offline to free up space</title>
    <link href="https://www.codejam.info/2023/04/archiving-google-photos.html" />
    <id>https://www.codejam.info/2023/04/archiving-google-photos.html</id>
    <updated>2023-04-02T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>If you backup your phone photos to Google Photos automatically, and you
don‚Äôt pay for some kind of Google One subscription, you‚Äôll run sooner or
later into the 15 GB storage limit of your Google account.</p>
<p>15 GB is not a lot, especially when you consider than my Pixel 6a takes
pictures that are easily 3 to 5 MB each. üò¨</p>
<p>To be fair, if you want convenience and you value your time, Google
One‚Äôs $20/year for 100 GB is a pretty damn good deal. Same goes for the
higher options with more storage if you need.</p>
<p>But if you don‚Äôt like recurring bills like me, and you find it overkill
to keep that many old photos in the cloud, read on.</p>
<h2 id="my-protocol-for-archiving-photos-away-from-google" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#my-protocol-for-archiving-photos-away-from-google"><span>My protocol for archiving photos away from Google</span></a></h2>
<p>In order to save space, I‚Äôll periodically archive my old photos outside
of Google Photos.</p>
<p>This protocol is designed to archive photos <em>from the phone</em> that are
<em>backed up</em> to Google Photos, but preserving the phone‚Äôs original
arborescence. Google Photos doesn‚Äôt have the path information, only the
filename, so backing up <em>from Google Photos</em> directly would not work for
this use case.</p>
<p>The downside is that this doesn‚Äôt cover the case where you have photos
in Google Photos that are <em>not</em> on your phone.</p>
<p>Also it‚Äôs designed for a single phone backing up to a Google Photos
account that‚Äôs used <em>solely</em> for that device. Multiple devices sharing
the same Google Photos is not supported.</p>
<p>With that said, here‚Äôs how I do it.</p>
<h3 id="1-sync-phone-to-computer" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#1-sync-phone-to-computer"><span>1. Sync phone to computer</span></a></h3>
<p>First, I use <a href="https://syncthing.net/">Syncthing</a> to sync the contents of
my phone to a hard drive connected to my computer.</p>
<p>I configure Syncthing as ‚Äúsend only‚Äù on my phone, and ‚Äúreceive only‚Äù on
my computer, and I configure it to sync the root directory of my phone
(which can be tricky, <a href="https://www.codejam.info/2023/04/syncthing-root-directory.html">but possible</a>).</p>
<p><strong>After the sync is complete, I turn off Syncthing from my computer, to
make sure no incremental updates will happen during the archive
process.</strong></p>
<h3 id="2-copy-synced-folder-to-archive" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#2-copy-synced-folder-to-archive"><span>2. Copy synced folder to archive</span></a></h3>
<p>For this example, let‚Äôs assume I synced my phone to a
<code>/Volumes/Syncthing/Phone</code> directory, and I want to archive my old
photos in <code>/Volumes/Archive/Phone</code>.</p>
<p>I‚Äôll run the following command to copy the phone contents to my archive
directory:</p>
<pre><code class="hljs language-sh"><span class="hljs-built_in">cp</span> -a /Volumes/Syncthing/Phone/ /Volumes/Archive/Phone/
</code></pre>
<div class="note">
<p><strong>Note:</strong> the reason I copy the whole phone contents is because I want
to catch <em>all</em> photos and videos that are backed up to Google Photos.
Typically, apps like Messenger, Whats App, Signal, etc. all store photos
in different directories, so syncing only <code>DCIM/Camera</code> would not be
enough.</p>
</div>
<p>If the target directory already exists, this will append new files to it
(and overwrite them if a file already exists there)!</p>
<p>Also if the directory already exists, the trailing slashes are
important.</p>
<div class="note">
<p><strong>Note:</strong> if both directories are on the same filesystem, and you‚Äôre not
appending to an existing archive, you may use <code>mv</code> instead, but then
make sure to recreate the Syncthing directory and put back its
<code>.stfolder</code> (required for Syncthing to recognize it) and <code>.stignore</code> if
you have one!</p>
</div>
<h3 id="3-delete-everything-from-google-photos" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#3-delete-everything-from-google-photos"><span>3. Delete everything from Google Photos</span></a></h3>
<p>Not necessarily everything, but well, everything you want to delete to
free up space.</p>
<p>You can do it from your phone, or from Google Photos on your computer,
or on the web.</p>
<div class="note">
<p><strong>Note:</strong> be careful! If you have photos that are <em>only</em> on Google
Photos but not stored on your phone storage, the previous step didn‚Äôt
archive them. You need to make sure to download them from Google Photos
in the first place. Doing that in an automated way is not covered in
this post.</p>
</div>
<p>As an abundance of caution, you may want to double check that the number
of photos/videos you have on Google Photos matches exactly with the
number of photos/videos you have on your phone before doing that.</p>
<p>If there‚Äôs any mismatch, try to find where the difference it to make
sure you‚Äôre not accidentally losing any photo.</p>
<p>Alternatively, you can <a href="https://www.codejam.info/2023/04/archiving-google-photos.html#bonus-script-to-list-all-your-google-photos-using-the-api">use the Google Photos API</a>
to list all the filenames on Google Photos, and ensure you have a match
in your archive prior to deleting. Otherwise, you‚Äôll know the names of
the missing ones that you have to download.</p>
<div class="note">
<p><strong>Note:</strong> if you deleted the photos from the web or desktop app, make
sure to wait that the deletion is propagated to your phone before you
continue!</p>
</div>
<h3 id="4-sync-phone-to-computer-again" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#4-sync-phone-to-computer-again"><span>4. Sync phone to computer again</span></a></h3>
<p>Again with Syncthing in my case, I do a sync following the deletion.</p>
<div class="note">
<p><strong>Note:</strong> you may want to exclude <code>.trashed-*</code> files in your
<code>.stignore</code>, otherwise the photos you deleted will still be synced while
they‚Äôre in the trash.</p>
</div>
<p>Now in our example, <code>/Volumes/Syncthing/Phone</code> contains just the
photos we decided to keep around in Google Photos, while
<code>/Volumes/Archive/Phone</code> contains <em>all</em> the photos (also including the
ones we kept around).</p>
<p>On top of that, both directories contains <em>all other files</em> from the
phone, that are not managed by Google Photos.</p>
<div class="note">
<p><strong>Note:</strong> this process is not very efficient if you have a lot of files
that are not photos and videos, e.g. music and downloads. You may want
to ignore those directories in the earlier steps to avoid copying them
around unnecessarily!</p>
</div>
<h3 id="5-remove-the-overlap" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#5-remove-the-overlap"><span>5. Remove the overlap</span></a></h3>
<p>To avoid that duplication, we can remove all files from the archive that
are still in the Syncthing directory. That is, all the photos/videos we
kept, as well as all the files in the phone storage that are not managed
by Google Photos.</p>
<pre><code class="hljs language-sh">(<span class="hljs-built_in">cd</span> /Volumes/Syncthing/Phone &amp;&amp; find . -<span class="hljs-built_in">type</span> f) | <span class="hljs-keyword">while</span> <span class="hljs-built_in">read</span> f; <span class="hljs-keyword">do</span> <span class="hljs-built_in">rm</span> -v <span class="hljs-string">&quot;/Volumes/Archive/Phone/<span class="hljs-variable">$f</span>&quot;</span>; <span class="hljs-keyword">done</span>
</code></pre>
<p>Now, the archive directory only contains what we removed from Google
Photos (and from the phone), but there‚Äôs no duplicates!</p>
<h3 id="6-profit" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#6-profit"><span>6. Profit!</span></a></h3>
<p>You can now enjoy all the space you freed up by archiving your photos
and videos away from Google Photos!</p>
<p>Repeat every time you‚Äôre close to running out of storage. üòâ</p>
<h2 id="about-the-google-photos-app-home-page" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#about-the-google-photos-app-home-page"><span>About the Google Photos app ‚Äúhome page‚Äù</span></a></h2>
<p>I think there may be some display bugs when deleting <em>a lot</em> of photos
from Google Photos at once. For some reason the main photos list of my
Google Photos still shows a few of the photos I deleted! They‚Äôre in a
weird state where the UI offers me download them to my device (as if
they‚Äôre not on the device already), but also shows me a local path to
the file as if it was on device (but the photo is not actually there).</p>
<p>I‚Äôm thinking this issue will be gone when the photos in the trash are
permanently deleted, so this doesn‚Äôt concern me too much. What‚Äôs visible
in Google Photos on the web (and in their API) is consistent with the
state I want, and what‚Äôs on my phone‚Äôs raw storage is consistent too.</p>
<h2 id="what-about-motion-photos" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#what-about-motion-photos"><span>What about motion photos?</span></a></h2>
<p>Google‚Äôs motion photos are the equivalent of Apple‚Äôs live photos: a
photo that also contains a short video of the ‚Äúmoment‚Äù it was captured.</p>
<p>What happens to those during our archival process? Well, it‚Äôs
complicated.</p>
<p>In short, don‚Äôt worry, they‚Äôre backed up and the little video that goes
with the motion photo is not going to be lost, but you won‚Äôt be able to
watch the ‚Äúlive‚Äù part anymore, you‚Äôll only see the still picture.</p>
<p>The reason is that Google stores the MP4 video part at the end of the JPEG
file. This doesn‚Äôt prevent displaying the image, but there‚Äôs currently
no photo viewer other than Google Photos that knows to extract that MP4
section following the JPEG data, and display it properly.</p>
<p>So if you want to see the live part of a motion photo, you‚Äôll have to
re-import it to Google Photos.</p>
<p>Alternatively, you can extract the MP4 part of the motion photo to a
different file, which you can do by using a script like
<a href="https://mjanja.ch/2021/10/stripping-embedded-mp4s-out-of-android-12-motion-photos/">detailed in this post</a>.</p>
<div class="note">
<p><strong>Note:</strong> if you use the script from the above post on macOS, you‚Äôll
need GNU <code>grep</code> in order find the byte offset of the MP4 header.</p>
<p>This means you‚Äôll have to <code>brew install coreutils</code> and replace <code>grep</code> by
<code>ggrep</code> in the script for it to work.</p>
</div>
<h2 id="bonus-script-to-list-all-your-google-photos-using-the-api" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#bonus-script-to-list-all-your-google-photos-using-the-api"><span>Bonus: script to list all your Google Photos using the API</span></a></h2>
<p>In the previous section, we saw it can be useful to list all the photos
from Google Photos (not necessarily on any of your devices) prior to
running the archiving process, to make sure you can catch the ones that
are not backed up anywhere.</p>
<p>You can put this script in <code>photos.mjs</code> and run as <code>node photos.mjs</code>.
You‚Äôll need to put a Google OAuth access token with access to your
Google Photos for this to work.</p>
<p>If you want to generate one from the CLI, check out my
<a href="https://www.codejam.info/2021/02/google-oauth-from-cli-application.html#update-local-server-redirect">article on the subject</a>.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;node:fs/promises&#x27;</span>

<span class="hljs-keyword">const</span> accessToken = <span class="hljs-string">&#x27;YOUR_ACCESS_TOKEN&#x27;</span>

<span class="hljs-keyword">let</span> pageToken = <span class="hljs-string">&#x27;&#x27;</span>
<span class="hljs-keyword">let</span> pages = []

<span class="hljs-keyword">do</span> {
  <span class="hljs-keyword">const</span> url = <span class="hljs-string">&#x27;https://photoslibrary.googleapis.com/v1/mediaItems?pageSize=100&amp;pageToken=&#x27;</span> + <span class="hljs-built_in">encodeURIComponent</span>(pageToken)

  <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(url)

  <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(url, {
    <span class="hljs-attr">headers</span>: {
      <span class="hljs-string">&#x27;Authorization&#x27;</span>: <span class="hljs-string">`Bearer <span class="hljs-subst">${accessToken}</span>`</span>
    }
  })

  <span class="hljs-keyword">const</span> json = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>()

  pages.<span class="hljs-title function_">push</span>(json)

  pageToken = json.<span class="hljs-property">nextPageToken</span>
} <span class="hljs-keyword">while</span> (pageToken)

<span class="hljs-keyword">await</span> fs.<span class="hljs-title function_">writeFile</span>(<span class="hljs-string">&#x27;pages.json&#x27;</span>, <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(pages, <span class="hljs-literal">null</span>, <span class="hljs-number">2</span>))
</code></pre>
<p>This will fetch all pages from the Google Photos API and dump them in a
<code>pages.json</code> file. You can then iterate through it to do whatever
operations you need to, e.g. making sure you don‚Äôt leave any photo
around before deleting them from Google.</p>
<h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/archiving-google-photos.html#conclusion"><span>Conclusion</span></a></h2>
<p>Archiving photos away from Google Photos is not trivial, but possible.</p>
<p>If you care about not losing any of your photos, I recommend double
checking at every step that you‚Äôre not accidentally forgetting any file.</p>
<p>When done well, this allows to periodically free up some space from your
Google account without actually having to get rid of your photos and
videos! They‚Äôll still be available on your archive hard drive if you
want to. Your old photos are not as handy as if they were in the cloud,
but you know you can access them if needed.</p>
<p>Overall, you‚Äôre probably better off just paying Google to increase your
storage, but if you‚Äôre really motivated, I hope you can find inspiration
in the process I described in this post.</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Syncthing: sync phone root directory (all internal storage)</title>
    <link href="https://www.codejam.info/2023/04/syncthing-root-directory.html" />
    <id>https://www.codejam.info/2023/04/syncthing-root-directory.html</id>
    <updated>2023-04-02T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>If you use Syncthing on your phone, it may not let you select the
phone‚Äôs root directory as a shared folder source! At least that‚Äôs the
case on my phone running Android 13.</p>
<figure class="center">
  <img alt="Syncthing web GUI showing root directory path as text" srcset="../../img/2023/04/syncthing-forbidden.png 3x">
</figure>
<p>As you can see, ‚Äúuse this folder‚Äù is greyed out and there‚Äôs a warning
the folder can‚Äôt be used for privacy reasons.</p>
<p>But I own that phone and I don‚Äôt like being told what to do. In our
case, I want to be able to sync <strong>all</strong> of my phone‚Äôs storage to my
computer, as a backup system.</p>
<p>So how to circumvent that?</p>
<p>Turns out we can do that through Syncthing‚Äôs lesser known ‚Äúweb GUI‚Äù!</p>
<p>You can find it in the left menu (where you also exit Syncthing from).
It will open the web version of Syncthing. From there, instead of
selecting the directory to sync from your phone‚Äôs native folder picker
(which may prevent you to use the root directory), you can just <em>input a
path</em> as plain text.</p>
<figure class="center">
  <img alt="Syncthing web GUI showing root directory path as text" srcset="../../img/2023/04/syncthing-web-gui.png 3x">
</figure>
<p>Enter <code>/storage/emuilated/0</code> (or simply <code>~</code>), and there you go, you have
a Syncthing folder that syncs all of your phone‚Äôs internal storage! üôè</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Getting rid of ghost login items in macOS Ventura</title>
    <link href="https://www.codejam.info/2023/04/ghost-login-items-macos-ventura.html" />
    <id>https://www.codejam.info/2023/04/ghost-login-items-macos-ventura.html</id>
    <updated>2023-04-02T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>Let‚Äôs say you uninstalled an app on macOS Ventura, and you see some
leftovers from that app in <strong>System Settings &gt; General &gt; Login Items</strong>:</p>
<figure class="center">
  <img alt="macOS login items with leftover app that shouldn‚Äôt be there" srcset="../../img/2023/04/login-items-dirty.png 2x">
</figure>
<p>Here‚Äôs a few tips to solve it.</p>
<h2 id="reboot" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/ghost-login-items-macos-ventura.html#reboot"><span>Reboot</span></a></h2>
<p>More often than not, it seems that after removing login items and/or the
app behind then, it takes a reboot of macOS Ventura until they‚Äôre
‚Äúgarbage collected‚Äù from system settings. That‚Äôs the first thing you
should try.</p>
<h2 id="check-your-trash" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/ghost-login-items-macos-ventura.html#check-your-trash"><span>Check your trash!</span></a></h2>
<p>If you moved the app to the trash but didn‚Äôt empty the trash, its login
items are still referenced from the trash! They won‚Äôt go away until you
permanently delete the app (and reboot).</p>
<h2 id="check-leftover-launch-agents-and-daemons" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/ghost-login-items-macos-ventura.html#check-leftover-launch-agents-and-daemons"><span>Check leftover launch agents and daemons</span></a></h2>
<p>There‚Äôs a few places macOS looks for ‚Äúlogin items‚Äù on your filesystem:</p>
<ul>
<li><code>/Library/LaunchAgents</code></li>
<li><code>/Library/LaunchDaemons</code></li>
<li><code>~/Library/LaunchAgents</code></li>
<li><code>~/Library/LaunchDaemons</code></li>
</ul>
<p>Also, same thing under <code>/System/Library</code> but that‚Äôs for macOS own login
items and you have no control over them.</p>
<p>Check the 4 directories above for leftover <code>plist</code> files from the
applications you removed. You may need to do some cleanup. After that,
don‚Äôt forget to reboot!</p>
<h2 id="inspect-backgrounditems-v4-btm" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/ghost-login-items-macos-ventura.html#inspect-backgrounditems-v4-btm"><span>Inspect <code>BackgroundItems-v4.btm</code></span></a></h2>
<p>As shown in <a href="https://www.reddit.com/r/MacOSBeta/comments/w2we6q/cleaning_up_venturas_login_items/">this Reddit post</a>,
the list of login items in Ventura is managed in <code>/private/var/db/com.apple.backgroundtaskmanagement</code>. In my case, in a <code>BackgroundItems-v8.btm</code> file.</p>
<pre><code class="hljs language-console"><span class="hljs-meta prompt_">$ </span><span class="language-bash">file /private/var/db/com.apple.backgroundtaskmanagement/BackgroundItems-v8.btm</span>
Apple binary property list
</code></pre>
<p>As <code>file(1)</code> tells us, this is a binary property list file. We can
inspect it with <code>plutil</code>:</p>
<pre><code class="hljs language-sh">plutil -p /private/var/db/com.apple.backgroundtaskmanagement/BackgroundItems-v8.btm
</code></pre>
<p>This will print the whole structure behind that file. From inspecting
its output, you should be able to determine what‚Äôs behind the ‚Äúghost items‚Äù
that you identified in the system settings. More often than not, it‚Äôll
point to some file or app that you forgot to get rid of, and cleaning
that up will fix your problem (again, after a reboot).</p>
<div class="note">
<p><strong>Note:</strong> if when accessing <code>/private/var/db</code> you get a permission
denied error, even as <code>root</code>, make sure to grant ‚Äúfull disk access‚Äù
permission to your terminal app!</p>
</div>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Embedding high DPI screenshots at normal size in HTML</title>
    <link href="https://www.codejam.info/2023/04/high-dpi-screenshot-size-html.html" />
    <id>https://www.codejam.info/2023/04/high-dpi-screenshot-size-html.html</id>
    <updated>2023-04-02T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>Since I moved to a Retina display, the screenshots I take are twice as
big as before!</p>
<p>This is a problem because, when embedded in an HTML page, they look
ginormous.</p>
<p>Let‚Äôs take a dummy example of some UI element screenshot:</p>
<pre><code class="hljs language-html"><span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">alt</span>=<span class="hljs-string">&quot;Save as popup&quot;</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;save-as-example.png&quot;</span>&gt;</span>
</code></pre>
<figure class="center">
  <img alt="Save as popup" src="https://www.codejam.info/img/2023/04/save-as-example.png">
</figure>
<p>This is huge. Crazy huge. It disturbs the reading flow and you only see
that oversized, slightly blurry image. Just bad.</p>
<p>How to fix it then?</p>
<h2 id="downsize-the-image" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/high-dpi-screenshot-size-html.html#downsize-the-image"><span>Downsize the image</span></a></h2>
<p>We can resize the image by 50% before embedding it. For example using
ImageMagick:</p>
<pre><code class="hljs language-sh">convert save-as-example.png -resize 50% save-as-example-small.png
</code></pre>
<p>This will work, but we introduce a loss in quality from the downscaling
operation. It‚Äôs not gonna be as ‚Äúsharp‚Äù as if the UI element was
rendered at the lower resolution in the first place, without being
downsized at the pixel level later on.</p>
<p>On top of that, when viewed on a high DPI screen, the image will not be
as crisp as what you saw when you took the screenshot, because half the
pixels got lost.</p>
<h2 id="take-the-screenshot-on-a-low-dpi-screen" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/high-dpi-screenshot-size-html.html#take-the-screenshot-on-a-low-dpi-screen"><span>Take the screenshot on a low DPI screen</span></a></h2>
<p>On macOS, if you have an external screen that has a low DPI, you can
take the screenshot on that screen. Then the screenshot will be taken at
a normal-looking native resolution, because there‚Äôs no scaling factor.</p>
<p>The downside is that you need a low DPI screen handy, and to plug it.
Not always applicable.</p>
<p>And on top of that, like the previous method, it won‚Äôt look as good when
viewed on a high DPI screen.</p>
<h2 id="use-srcset-with-a-2x-factor" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/04/high-dpi-screenshot-size-html.html#use-srcset-with-a-2x-factor"><span>Use <code>srcset</code> with a <code>2x</code> factor!</span></a></h2>
<p>If you want to go the lazy and easy way, <code>srcset</code> is the way.</p>
<p>It allows to specify a <em>pixel density descriptor</em> for the image being
referenced. In our case, because the screenshot was rendered at double
the size for the Retina display, we can specify <code>2x</code>.</p>
<pre><code class="hljs language-html"><span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">alt</span>=<span class="hljs-string">&quot;Save as popup&quot;</span> <span class="hljs-attr">srcset</span>=<span class="hljs-string">&quot;save-as-example.png 2x&quot;</span>&gt;</span>
</code></pre>
<figure class="center">
  <img alt="Save as popup" srcset="../../img/2023/04/save-as-example.png 2x">
</figure>
<p>This looks perfect! On a high DPI screen, we get the exact original
quality of the screenshot, as crisp as can be. On a low DPI screen, the
browser knows how to adapt the size of the screenshot to make it look
just right.</p>
<p>The only downside is that for low DPI screens, we‚Äôll still send the
heavier image with twice as much pixels, even if we don‚Äôt need it. If
that‚Äôs a problem for you, then you can use one of the earlier solutions
to generate a <code>1x</code> version of the screenshot, and use it as part of the
<code>srcset</code>.</p>
<div class="note">
<p><strong>Note:</strong> I also use this technique from phone screenshots! Phones
typically have high DPI screens too, and they generate pretty large
screenshots. <code>2x</code> might not be enough to make it look reasonable on a
web page, so feel free to go <code>3x</code> or <code>4x</code> if you want to reduce the size
of the image without altering the original and compromising its quality!</p>
</div>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Join the discussion on <a href="https://twitter.com/valeriangalliat/status/1642615952305954818">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Google Cloud Functions with a static IP: a guide to high throughput NAT</title>
    <link href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html" />
    <id>https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html</id>
    <updated>2023-03-16T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>One day or another, you‚Äôre gonna encounter a firewall with an IP
whitelist. It only accepts connections from specific IP addresses that
were explicitly allowed.</p>
<p>If you have a cloud, serverless and/or autoscaling infrastructure, new
resources are provisioned and deprovisioned automatically to accommodate
your load, and public IPs are dynamically allocated when needed. <strong>You
can‚Äôt predict what public IP address are your requests going to be sent
from</strong>.</p>
<p>This is a problem I‚Äôve needed to solve in virtually every company I‚Äôve
worked with in my career.</p>
<p>The solutions may vary depending on the technologies you use, and your
load patterns. For example, a simple proxy server (TCP, HTTP or SOCKS,
maybe load balanced between a few instances) might go a long way before
you need to resort to more complex solutions.</p>
<p>In this blog post I‚Äôll focus on Google Cloud, and in particular Cloud
Functions, but you might find it useful if you have the same use case
with Cloud Run or Kubernetes, or just need to fine tune a Cloud NAT in
general.</p>
<div class="note">
<p><strong>Note:</strong> this also applies to Firebase Functions, since they‚Äôre
implemented as Cloud Functions.</p>
</div>
<h2 id="building-blocks" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#building-blocks"><span>Building blocks</span></a></h2>
<p>If the proxy server approach doesn‚Äôt meet your requirements, or you just
want a more ‚Äúcloud native‚Äù solution, you‚Äôll need two pieces of
infrastructure in order to route your Cloud Functions or Cloud Run
containers traffic through static IPs: a <strong>VPC connector</strong>,
and a <strong>Cloud NAT</strong>.</p>
<p>If you run on Kubernetes, you only need the Cloud NAT.</p>
<h2 id="vpc-connector" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#vpc-connector"><span>VPC connector</span></a></h2>
<p>The VPC connector, also known as <a href="https://cloud.google.com/vpc/docs/configure-serverless-vpc-access">Serverless VPC Access connector</a>,
is a piece of Google Cloud infrastructure that lets you route the
traffic of Cloud Functions and Cloud Run containers to your <abbr title="Virtual Private Cloud">VPC</abbr>.</p>
<p>This is useful if you want them to access private resources, or in our
case, if you want them to access the internet through static IPs.</p>
<p>The VPC connector is backed by 2 to 10 plain old Compute Engine VMs,
that you can chose from a limited subset of instance sizes (<code>f1-micro</code>,
<code>e2-micro</code> and <code>e2-standard-4</code>). Not far from the naive proxy approach,
but it has to its advantage that those VMs are managed by Google, and
that they have a first-class integration with Cloud Functions and Cloud
Run.</p>
<p>Those instances live in a <code>/28</code> subnet that you allocate for them on
your VPC. <code>/28</code> gives you <a href="https://cloud.google.com/vpc/docs/serverless-vpc-access#ip_address_ranges">14 usable addresses</a>
which is sufficient for the 10 instances upper limit.</p>
<p>A few more things to note:</p>
<ul>
<li>You configure a minimum and maximum number of instances, between 2 and 10.</li>
<li>The connector starts with the minimum number of instances, and will
add more up to the maximum number if your traffic requires it.</li>
<li>After scaling up, the connector doesn‚Äôt scale down and you‚Äôll have to
recreate it if you want to lower the instances count. üôà</li>
</ul>
<p>Useful links:</p>
<ul>
<li><a href="https://cloud.google.com/vpc/docs/serverless-vpc-access">Serverless VPC Access</a></li>
<li><a href="https://cloud.google.com/vpc/docs/configure-serverless-vpc-access">Configure Serverless VPC Access</a></li>
</ul>
<h2 id="cloud-nat" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#cloud-nat"><span>Cloud NAT</span></a></h2>
<p>Cloud NAT is the second piece of the puzzle. It allows you to perform
network address translation (NAT) on Compute Engine VMs that don‚Äôt have
an external IP address.</p>
<p>Even though Cloud NAT depends on a Cloud Router, <a href="https://serverfault.com/q/1078625">it only uses it</a>
for configuring <a href="https://cloudplatform.googleblog.com/2014/04/enter-andromeda-zone-google-cloud-platforms-latest-networking-stack.html">Google Cloud‚Äôs networking stack</a>
but it‚Äôs not involved at the data level: it doesn‚Äôt add an extra hop and
potential bottleneck in your network topology!</p>
<p>In other words, you can expect an identical network bandwidth and
performance with Cloud NAT as if your VMs directly had public IPs, as
long as it‚Äôs appropriately configured for your situation (more on that
later).</p>
<p>Cloud NAT can be configured to dynamically allocate IP addresses as
needed, or use a static pool of IP addresses, which is going to be
useful for us.</p>
<p>In our case, Cloud NAT works hand in hand with the VPC connector VMs,
not only to provide them with internet access, but to do so using static
IPs if we configure the NAT that way!</p>
<p>Useful links:</p>
<ul>
<li><a href="https://cloud.google.com/nat/docs/overview">Cloud NAT overview</a></li>
<li><a href="https://medium.com/bluekiri/high-availability-nat-gateway-at-google-cloud-platform-with-cloud-nat-8a792b1c4cc4">High availability NAT gateway at Google Cloud Platform with Cloud NAT</a></li>
</ul>
<h2 id="configuring-them-together" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#configuring-them-together"><span>Configuring them together</span></a></h2>
<p>I‚Äôll skim over this part as there‚Äôs already decent coverage online:</p>
<ul>
<li><a href="https://dev.to/alvardev/gcp-cloud-functions-with-a-static-ip-3fe9">GCP Cloud Functions with a static IP</a></li>
<li><a href="https://gist.github.com/brokeyourbike/ee7c5ede900da6f31ced9fe587e0c706">Cloud Functions static outbound IP address</a></li>
<li><a href="https://stackoverflow.com/q/38811882">Possible to get static IP address for Google Cloud Functions?</a></li>
<li><a href="https://cloud.google.com/nat/docs/set-up-manage-network-address-translation">Set up and manage network address translation with Cloud NAT</a></li>
</ul>
<p>To add my own contribution, I‚Äôll share a <a href="https://www.pulumi.com/">Pulumi</a>
example to provision everything you need to get a static IP on your
Cloud Functions. It‚Äôll get you in the same place as the tutorials above.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">const</span> gcp = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;@pulumi/gcp&#x27;</span>)

<span class="hljs-keyword">const</span> subnet = <span class="hljs-keyword">new</span> gcp.<span class="hljs-property">compute</span>.<span class="hljs-title class_">Subnetwork</span>(<span class="hljs-string">&#x27;subnet&#x27;</span>, {
  <span class="hljs-attr">network</span>: <span class="hljs-string">&#x27;default&#x27;</span>,
   <span class="hljs-comment">// Arbitrary range that doesn&#x27;t conflict with other subnets in your VPC</span>
  <span class="hljs-attr">ipCidrRange</span>: <span class="hljs-string">&#x27;10.8.0.0/28&#x27;</span>
})

<span class="hljs-keyword">const</span> router = <span class="hljs-keyword">new</span> gcp.<span class="hljs-property">compute</span>.<span class="hljs-title class_">Router</span>(<span class="hljs-string">&#x27;router&#x27;</span>, {
  <span class="hljs-attr">network</span>: <span class="hljs-string">&#x27;default&#x27;</span>
})

<span class="hljs-keyword">const</span> ip1 = <span class="hljs-keyword">new</span> gcp.<span class="hljs-property">compute</span>.<span class="hljs-title class_">Address</span>(<span class="hljs-string">&#x27;ip-1&#x27;</span>, {})
<span class="hljs-keyword">const</span> ip2 = <span class="hljs-keyword">new</span> gcp.<span class="hljs-property">compute</span>.<span class="hljs-title class_">Address</span>(<span class="hljs-string">&#x27;ip-2&#x27;</span>, {})

<span class="hljs-keyword">new</span> gcp.<span class="hljs-property">compute</span>.<span class="hljs-title class_">RouterNat</span>(<span class="hljs-string">&#x27;nat&#x27;</span>, {
  <span class="hljs-attr">router</span>: router.<span class="hljs-property">name</span>,
  <span class="hljs-attr">region</span>: router.<span class="hljs-property">region</span>,

  <span class="hljs-attr">natIpAllocateOption</span>: <span class="hljs-string">&#x27;MANUAL_ONLY&#x27;</span>,
  <span class="hljs-attr">natIps</span>: [ip1.<span class="hljs-property">selfLink</span>, ip2.<span class="hljs-property">selfLink</span>],

  <span class="hljs-attr">sourceSubnetworkIpRangesToNat</span>: <span class="hljs-string">&#x27;LIST_OF_SUBNETWORKS&#x27;</span>,
  <span class="hljs-attr">subnetworks</span>: [
    {
      <span class="hljs-attr">name</span>: subnet.<span class="hljs-property">id</span>,
      <span class="hljs-attr">sourceIpRangesToNats</span>: [<span class="hljs-string">&#x27;ALL_IP_RANGES&#x27;</span>]
    }
  ],

  <span class="hljs-comment">// If not specified, Pulumi enables endpoint-independent mapping by default,</span>
  <span class="hljs-comment">// even though it&#x27;s not enabled by default when using the Google Cloud</span>
  <span class="hljs-comment">// console.</span>
  <span class="hljs-comment">//</span>
  <span class="hljs-comment">// To be in the same state as if we used the console, we explicitly</span>
  <span class="hljs-comment">// have to disable it here.</span>
  <span class="hljs-attr">enableEndpointIndependentMapping</span>: <span class="hljs-literal">false</span>

  <span class="hljs-comment">// More things to go here for optimal performance, see below</span>
})

<span class="hljs-keyword">new</span> gcp.<span class="hljs-property">vpcaccess</span>.<span class="hljs-title class_">Connector</span>(<span class="hljs-string">&#x27;connector&#x27;</span>, {
  <span class="hljs-attr">name</span>: <span class="hljs-string">&#x27;connector&#x27;</span>,
  <span class="hljs-attr">subnet</span>: {
    <span class="hljs-attr">name</span>: subnet.<span class="hljs-property">name</span>
  }
})
</code></pre>
<p>What this code does:</p>
<ol>
<li>Provision a <code>/28</code> subnet that is going to be used for the VPC
connector VMs, and that we‚Äôll attach the NAT to.</li>
<li>Create the Cloud Router necessary for the NAT to do its
configurational magic.</li>
<li>Allocate 2 static public IP addresses.</li>
<li>Create the Cloud NAT in manual mode with the IPs we just created, and
attach it to the VPC connector subnet.</li>
<li>Create the VPC connector in the subnet we prepared for it.</li>
</ol>
<p>Finally you can configure your Cloud Functions, Firebase Functions or
Cloud Run containers to use that VPC connector for all its traffic. In
the case of a Firebase Function, it looks like this:</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">const</span> functions = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;firebase-functions&#x27;</span>)

functions.<span class="hljs-title function_">runWith</span>({
  <span class="hljs-attr">vpcConnector</span>: <span class="hljs-string">&#x27;connector&#x27;</span>,
  <span class="hljs-attr">vpcConnectorEgressSettings</span>: <span class="hljs-string">&#x27;ALL_TRAFFIC&#x27;</span>
})
  <span class="hljs-comment">// Your actual function, for example</span>
  .<span class="hljs-property">https</span>.<span class="hljs-title function_">onRequest</span>(<span class="hljs-keyword">async</span> (req, res) =&gt; {
    response.<span class="hljs-title function_">json</span>(<span class="hljs-keyword">await</span> (<span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(<span class="hljs-string">&#x27;https://api.ipify.org?format=json&#x27;</span>)).<span class="hljs-title function_">json</span>())
  })
</code></pre>
<p>And ta-da, you get static IPs! üéâ</p>
<p>Everything seems to be working smoothly when you perform basic
connectivity testing (i.e. making a few requests to <a href="https://api.ipify.org"><code>api.ipify.org</code></a>),
so you pat yourself on the back ‚Äúnice, it wasn‚Äôt that bad after all‚Äù and
you go grab a beer.</p>
<p>Did you think you were done? Wait. We‚Äôre just getting started.</p>
<h2 id="when-the-trouble-start" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#when-the-trouble-start"><span>When the trouble start</span></a></h2>
<p>So you happily go on and deploy your Cloud NAT and VPC Connector. To
your staging environment, obviously, right? Right?</p>
<figure class="center">
  <img alt="Meme about deploying to staging" src="https://www.codejam.info/img/2023/03/to-staging-right.jpg">
</figure>
<p>But if you have some kind of traffic, you quickly notice something
wrong.</p>
<p>Your Cloud Functions‚Äô <strong>execution time</strong> is much higher than
usual (up to minutes instead of milliseconds), and as a side effect of
that, your <strong>instance count</strong> is likely higher than normal. On the VPC
Connector side, everything looks good. But on the Cloud NAT, you notice
a non-zero <strong>dropped sent packets rate</strong>, with <code>OUT_OF_RESOURCES</code> as a
reason.</p>
<p>Maybe you also have a non-zero <strong>dropped received packets rate</strong> but
that is probably not the concern here (<a href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#what-about-dropped-received-packets-rate">more on that later</a>).</p>
<p>So what is going on? <a href="https://cloud.google.com/nat/docs/monitoring#gateway_filtering_dimensions">The docs</a>
tell us that Cloud NAT is running out of NAT IP addresses or ports.</p>
<div class="note">
<p><strong>Note:</strong> if you used Pulumi (<a href="https://github.com/hashicorp/terraform-provider-google/issues/10609">or Terraform</a>)
without explicitly disabling endpoint-independent mapping, you might
have accidentally turned on <a href="https://cloud.google.com/nat/docs/ports-and-addresses#ports-reuse-endpoints">endpoint-independent mapping (EIM)</a>,
which wouldn‚Äôt have been on by default if you used the GCP console.</p>
<p>If you see dropped packets with reason <code>ENDPOINT_INDEPENDENT_CONFLICT</code>,
and you didn‚Äôt intend to enable endpoint-independent mapping, this is
your problem here. You probably want to disable it, or even to enable
<em>dynamic port allocation</em>.</p>
<p>In case you <em>do</em> need endpoint-independent mapping, <a href="https://cloud.google.com/nat/docs/troubleshooting#endpoint-independent-conflict">this section of the docs</a>
can help you troubleshoot this.</p>
</div>
<h2 id="fixing-nat-out-of-resources" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#fixing-nat-out-of-resources"><span>Fixing NAT out of resources</span></a></h2>
<p>Let‚Äôs say you assigned 2 NAT IPs. Each IP gives you
<a href="https://cloud.google.com/nat/docs/ports-and-addresses#ports">64,512</a>
ports to work with (65,536 minus the 1024 privileged ports). So we‚Äôre
working with a total of 129,024 ports available for NAT at a time.</p>
<p>If this looks a bit tight for your current traffic and network patterns,
there you go, you need to add more IPs.</p>
<p>But if you estimate that is is reasonable (or even <em>way, way enough</em>)
for your expected traffic, adding more IPs will likely not solve the
problem.</p>
<p>In our case with the <em>default</em> Cloud NAT configuration, it should be in
<a href="https://cloud.google.com/nat/docs/ports-and-addresses#static-port">static port allocation mode</a>,
aka the state it‚Äôs in if you don‚Äôt check ‚Äúdynamic port allocation‚Äù in
the <strong>advanced configurations</strong> part (hidden by default).</p>
<figure class="center">
  <img alt="Default Cloud NAT port allocation settings" srcset="../../img/2023/03/cloud-nat-port-allocation.png 2x">
</figure>
<p>What this reveals is one key piece of information. <strong>Our Cloud NAT
defaults to static port allocation with 64 ports per VM.</strong></p>
<p>The copy in this UI is misleading, because while it reads 64 ‚Äúminimum
ports per VM instance‚Äù, it is actually both a minimum <em>and</em> a maximum
(well, it‚Äôs <em>static</em>), which is why the ‚Äúmaximum ports‚Äù input is
disabled.</p>
<p><strong>So let‚Äôs say you configured your VPC Connector to use 2 VMs, this means
you‚Äôre artificially limiting your NAT to use a most 128 ports used at a
time!!</strong> (Out of your 129,024 available ports if you have 2 IPs. üòÖ)</p>
<p>No wonder when all the traffic of your Cloud Functions go through those
two poor VPC Connector VMs, you end up seeing huge network latency and
dropped packets.</p>
<div class="note">
<p><strong>Note:</strong> the number you care about here is really the number of VMs you
configured on the VPC Connector. You may have a much higher <em>instances
count</em> on your Cloud Functions side, but since their traffic has to go
through the VPC Connector instances first, it‚Äôs really this one that
matters.</p>
</div>
<p>Once we understand that, the fix becomes obvious: <strong>increase the number
of ports per VM</strong>.</p>
<p>Since in our example we have 2 VPC Connector VMs and 2 IP addresses,
assuming we use the Cloud NAT only for those, we could in theory assign
up to 65,536 ports (all of an IP‚Äôs ports) per VM!</p>
<p>Obviously adjust this number based on the maximum number of VMs you can
have relative to how many IPs you allocated.</p>
<p>Realistically, if you want to keep some headroom for adding more VPC
Connector VMs without adding more IPs in the future, you need to pick a
lower number like 4,096, 8,192, 16,384 or 32,768, especially if that‚Äôs
enough for your current needs.</p>
<p>The takeaway from is that the default Cloud NAT configuration is really
not adapted to be used with a VPC Connector, and we‚Äôre required to tune
the settings for proper network performance. It‚Äôs probably a decent
default for other use cases, but <em>definitely</em> not this one.</p>
<h2 id="going-further-with-dynamic-port-allocation" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#going-further-with-dynamic-port-allocation"><span>Going further with dynamic port allocation</span></a></h2>
<p>Increasing the static number of ports per VM should have helped quite a
bit with the latency and dropped packets, but we can do better.</p>
<p>If we enable <a href="https://cloud.google.com/nat/docs/ports-and-addresses#dynamic-port">dynamic port allocation</a>,
this will allow Cloud NAT to allocate even more ports to a given VM if
needed. It can use any number of ports between the min/max range you
configure, up to a maximum of 65,536 ports (a full IP).</p>
<p>An important thing to keep in mind with dynamic port allocation is that
when a VM uses all its pre-allocated ports and reaches the point where
it needs to allocate more more ports dynamically, <strong>this is not
instantaneous</strong>.</p>
<p>The symptoms of that would be, again, dropped packets and increased
latency while the ports are being allocated.</p>
<p>In our scenario, this would be very obvious if we used dynamic port
allocation with its default minimum ports per VM of 32 (when our load
needs orders of magnitude more ports than that).</p>
<p><strong>So we can‚Äôt rely solely on dynamic port allocation to save our day. We
do still need to configure a sensible minimum ports per VM <em>that matches
our expected needs</em>.</strong></p>
<p>Concretely, if we have 2 to 3 VPC Connector VMs, 2 NAT IPs, and the NAT
is solely used by the VPC Connector, using a dynamic port allocation
with a minimum ports per VM of 16,384 would be a good match that lives
us some headroom for adding a few more VMs without needing extra IPs.</p>
<h2 id="going-even-further-with-tcp-settings" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#going-even-further-with-tcp-settings"><span>Going even further with TCP settings</span></a></h2>
<p>The <strong>advanced configurations</strong> of Cloud NAT also gives us control over
various protocol timeouts. Here are the defaults:</p>
<figure class="center">
  <img alt="Default Cloud NAT timeouts settings" srcset="../../img/2023/03/cloud-nat-timeouts.png 2x">
</figure>
<p>Here we‚Äôll focus on the TCP settings. Here‚Äôs a more detailed description
of those from <a href="https://cloud.google.com/nat/docs/overview#specs-timeouts">the spec</a>:</p>
<ul>
<li><strong>TCP established connection idle timeout:</strong> specifies the time that a
connection is idle before the Cloud NAT mappings are removed.</li>
<li><strong>TCP transitory connection idle timeout:</strong> specifies the time that
TCP connections can remain in the <a href="https://en.wikipedia.org/wiki/TCP_half-open">half-open state</a>
before the Cloud NAT mappings can be deleted.</li>
<li><strong>TCP time wait:</strong> specifies the time that a
fully closed TCP connection is retained in the Cloud NAT mappings
after the connection expires.</li>
</ul>
<p>I‚Äôm not too concerned about half-open TCP connections, and keeping them
around for 30 seconds sounds like a reasonable value in the first place,
so we‚Äôll leave the <strong>transitory idle timeout</strong> alone.</p>
<p>For <strong>established idle timeout</strong> and <strong>TCP time wait</strong> though,
respective values of <strong>TWENTY MINUTES</strong> and 2 minutes may be more
problematic.</p>
<p>Those defaults are probably sensible when you have <em>actual</em> VMs
directly connecting through the NAT, but with the dynamic and
‚Äúserverless‚Äù nature of Cloud Functions, keeping idle and especially
closed connections around for that long is no good.</p>
<p><strong>Matching those with the value of your Cloud Functions request timeout
would make more sense</strong>, which may be as low as 60 or 30 seconds (or
even lower). Once a function gets killed because it exceeded its
timeout, there‚Äôs no point in keeping the TCP connections it opened (and
maybe failed to close) for any longer, especially not 20 minutes!</p>
<p>In my case, lowering those timeouts to 30 seconds had a noticeable
difference in the NAT <strong>open connections</strong> and port <strong>usage metric</strong>
(they got cut by half!).</p>
<h2 id="applying-this-to-our-pulumi-example" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#applying-this-to-our-pulumi-example"><span>Applying this to our Pulumi example</span></a></h2>
<p>After learning all this, we can go back to our initial <a href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#configuring-them-together">Pulumi example</a>
and apply those optimizations by adding the following to our <code>RouterNat</code>
configuration. <strong>Keep in mind this is tuned for 2 (min) to 3 (max) VPC
Connector VMs that are the <em>sole</em> VMs to use a Cloud NAT with 2 static IPs.</strong>
Tweak appropriately.</p>
<pre><code class="hljs language-js"><span class="hljs-keyword">new</span> gcp.<span class="hljs-property">compute</span>.<span class="hljs-title class_">RouterNat</span>(<span class="hljs-string">&#x27;nat&#x27;</span>, {
  <span class="hljs-comment">// ...</span>

  <span class="hljs-attr">enableDynamicPortAllocation</span>: <span class="hljs-literal">true</span>,
  <span class="hljs-attr">minPortsPerVm</span>: <span class="hljs-number">16384</span>,
  <span class="hljs-attr">maxPortsPerVm</span>: <span class="hljs-number">65536</span>,
  <span class="hljs-attr">tcpEstablishedIdleTimeoutSec</span>: <span class="hljs-number">60</span>,
  <span class="hljs-attr">tcpTimeWaitTimeoutSec</span>: <span class="hljs-number">60</span>
})
</code></pre>
<h2 id="what-about-dropped-received-packets-rate" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#what-about-dropped-received-packets-rate"><span>What about dropped received packets rate?</span></a></h2>
<p>After all this, we‚Äôre in a pretty good spot for our Cloud Functions to
go through Cloud NAT static IPs with optimal performance.</p>
<p>Yet, you might notice that your <a href="https://stackoverflow.com/q/72620834/4324668">dropped received packets</a>
metric in Cloud NAT is non-zero, and that may concern you.</p>
<p>My advice for you will be to look at the proportion of dropped received
packets compared to your total received packets.</p>
<p>While the Cloud NAT metrics tab shows you the dropped received packets
rate, you don‚Äôt get the total received packets rate there. You‚Äôll have
to go to GCP metrics explorer and look at <code>received_packets_count</code>.</p>
<p>If your dropped received packets rate is especially low compared to the
total received packets you‚Äôre processing (e.g. 0.01%), and you notice no
negative effects on your app (latency, errors or whatnot), you‚Äôre
probably fine. After all, those packets are packets that Cloud NAT could
not translate, which may include invalid or unauthorized traffic.</p>
<p>However if it‚Äôs affecting a non-negligible part of your traffic, and
if you‚Äôre noticing a high latency or network error rate, you definitely
need to address it. This will widely vary based on your specific context
and network patterns, but as a blind shot, if you underestimated how
long your Cloud Functions can be alive and you <a href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#going-even-further-with-tcp-settings">lowered the TCP timeouts</a>
too much, there‚Äôs probably some tuning to be done around there.</p>
<h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/03/cloud-functions-static-ip-nat.html#conclusion"><span>Conclusion</span></a></h2>
<p>If you made it here, congrats! This article was pretty dense, and we
covered a number of fairly complex topics. I hope you learnt everything
you needed in order to have a successful Cloud NAT configuration for
your serverless (or Kubernetes) environment.</p>
<p>If I made mistakes in this post, or if you found something of value that
would be worth adding there, as usual, <a href="https://www.codejam.info/val.html#contact">let me know</a>
and I‚Äôll be happy to make updates!</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Join the discussion on <a href="https://twitter.com/valeriangalliat/status/1636458334340390941">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>OVH email redirect causes SPF check failure</title>
    <link href="https://www.codejam.info/2023/02/ovh-email-redirect-spf-failure.html" />
    <id>https://www.codejam.info/2023/02/ovh-email-redirect-spf-failure.html</id>
    <updated>2023-02-26T05:00:00.000Z</updated>
    <content type="html"><![CDATA[<div class="note">
<p><strong>Note:</strong> I put OVH in the title but it could happen with any provider
that offers email redirects.</p>
</div>
<p>Let‚Äôs say you have an email address <code>foo@foo.com</code>, and you set up a
redirect with your provider so that it forwards all emails to
<code>foo@gmail.com</code>, which would be a common scenario to use Gmail with a
custom domain without paying for Google Workspace (using this as an
example but the problem is not directly related to Gmail).</p>
<p>In my case with OVH, this is configured in <code>foo@foo.com</code>'s MX plan,
under <a href="https://docs.ovh.com/ca/en/emails/email-redirection-guide/">manage redirections</a>.</p>
<p>It works well most of the time, <strong>except when a sender tells me they
failed to send me an email</strong>.</p>
<p>It could be someone sending me an email from, for example,
<code>bar@gmx.com</code>, who tells me (obviously via another mean) that their
email was returned, with a message similar to this:</p>
<blockquote>
<p><strong>Subject:</strong> Undelivered Mail Returned to Sender<br>
<strong>From:</strong> <code>MAILER-DAEMON@mo557.mail-out.ovh.net</code></p>
<p>This is the mail system at host <code>mo557.mail-out.ovh.net</code>.</p>
<p>I‚Äôm sorry to have to inform you that your message could not be
delivered to one or more recipients. It‚Äôs attached below.</p>
<p>For further assistance, please send mail to postmaster.</p>
<p>If you do so, please include this problem report. You can delete your
own text from the attached returned message.</p>
<p>The mail system</p>
<p><a href="mailto:foo@gmail.com">foo@gmail.com</a>: host gmail-smtp-in.l.google.com[66.102.1.27] said:
550-5.7.26 The MAIL FROM domain [gmx.com] has an SPF record with a
hard fail 550-5.7.26 policy (-all) but it fails to pass SPF checks
with the ip: 550-5.7.26 [46.105.33.1]. To best protect our users from
spam and phishing, the 550-5.7.26 message has been blocked. Please
visit 550-5.7.26 https://support.google.com/mail/answer/81126#authentication
for more 550 5.7.26 information.
h21-20020a05600c351500b003eb3caa4d08si2037016wmq.38 - gsmtp (in reply
to end of DATA command)</p>
</blockquote>
<div class="note">
<p><strong>Note:</strong> I use <a href="https://www.gmx.com/">GMX</a> as an example here because
it‚Äôs a sender domain that I was consistently getting SPF issues with
because of my redirect setup.</p>
</div>
<p>This doesn‚Äôt seem to be a very widely encountered problem, yet I could
find a few occurrences of it in the wild like on <a href="https://www.reddit.com/r/AnonAddy/comments/ju9vgc/ovh_mail_redirection_fails/">this Reddit post</a>
as well as those Google
<a href="https://support.google.com/mail/thread/175932116">support</a>
<a href="https://support.google.com/mail/thread/195729241">threads</a>
(in French, and sadly locked so I couldn‚Äôt post the solution there).</p>
<h2 id="the-issue" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/02/ovh-email-redirect-spf-failure.html#the-issue"><span>The issue</span></a></h2>
<p>What‚Äôs going wrong here? It turns out that the sender domain (in this
example, <code>gmx.com</code>) had configured a strict <a href="https://en.wikipedia.org/wiki/Sender_Policy_Framework">SPF policy</a>
that only allowed their own servers to deliver emails from <code>@gmx.com</code>
addresses.</p>
<p>We can check this running the following command:</p>
<pre><code class="hljs language-console"><span class="hljs-meta prompt_">$ </span><span class="language-bash">host -t TXT gmx.com | grep spf</span>
gmx.com descriptive text &quot;v=spf1 ip4:213.165.64.0/23 ip4:74.208.5.64/26 ip4:74.208.122.0/26 ip4:212.227.126.128/25 ip4:212.227.15.0/24 ip4:212.227.17.0/27 ip4:74.208.4.192/26 ip4:82.165.159.0/24 ip4:217.72.207.0/27 -all&quot;
</code></pre>
<div class="note">
<p><strong>Note:</strong> to be clear, this is not a bad thing. It‚Äôs totally legitimate
from GMX to only allow their own servers to deliver emails from
<code>@gmx.com</code> addresses!</p>
<p>The reason it doesn‚Äôt happen with most sender domains is that it‚Äôs
common to configure SPF with a ‚Äúsoft fail‚Äù (<code>~all</code>) instead of a ‚Äúhard
fail‚Äù (<code>-all</code>) like GMX does. See the difference <a href="https://knowledge.ondmarc.redsift.com/en/articles/1148885-spf-hard-fail-vs-spf-soft-fail">here</a>.</p>
<p>A soft fail would result in the email being delivered but potentially
being flagged as spam, whereas a hard fail gets downright rejected.</p>
</div>
<p>Since Gmail do check the origin sender SPF policy and enforces it, it
rejected the <code>@gmx.com</code> email being forwarded by my OVH relay.</p>
<p>If you want to learn more about this issue, <a href="https://support.tigertech.net/spf">this post from Tiger Technologies</a>
is a very good read.</p>
<h2 id="the-fix" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/02/ovh-email-redirect-spf-failure.html#the-fix"><span>The fix</span></a></h2>
<p>Sadly there‚Äôs no trivial fix for this. The post linked just above
mentions a few solutions in the last section but notes that they‚Äôre not
widely available and might cause other issues. Their conclusion is:
<strong>for now, we‚Äôd recommend simply not forwarding important mail to other
ISPs</strong>.</p>
<p>What can we do from there? Well in my case, I had to reverse the
relationship between my OVH address and Gmail: instead of my OVH address
forwarding emails to Gmail, I removed the redirect and <strong>configured
Gmail to fetch emails from my OVH address via POP3</strong>.</p>
<div class="note">
<p><strong>Note:</strong> this alternative method is not specific to OVH and Gmail. It
will work as long as:</p>
<ol>
<li>Your target email system supports fetching emails from other
addresses via IMAP or POP3.</li>
<li>The email hosting provider you use for your intermediary address
has IMAP or POP3 capabilities, not only redirects.</li>
</ol>
</div>
<p>In order to do this with Gmail, their guide on <a href="https://support.google.com/mail/answer/21289">checking emails from other accounts</a>.</p>
<p>And if your intermediary (redirect) address is on OVH, you can find the
proper POP3 settings to use on their configuration guide, either for
<a href="https://docs.ovh.com/fr/emails/mail-mutualise-guide-configuration-dun-e-mail-mutualise-ovh-sur-linterface-de-gmail/">OVH France</a> or
<a href="https://docs.ovh.com/ca/en/emails/gmail-configuration/">OVH Canada</a>. In
short, it‚Äôs:</p>
<table>
<thead>
<tr>
<th>Region</th>
<th>Host</th>
<th>Port</th>
</tr>
</thead>
<tbody>
<tr>
<td>France</td>
<td><code>ssl0.ovh.net</code></td>
<td>995</td>
</tr>
<tr>
<td>Canada</td>
<td><code>pop.mail.ovh.ca</code></td>
<td>995</td>
</tr>
</tbody>
</table>
<h2 id="the-downside" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/02/ovh-email-redirect-spf-failure.html#the-downside"><span>The downside</span></a></h2>
<p>The main downside of this solution is that instead of OVH <em>pushing</em>
mails to Gmail, Gmail has to <em>pull</em> them from OVH periodically.
Concretely, this means <strong>increased latency</strong>. Instead of receiving
emails right away, you‚Äôll have to wait until Gmail decides to fetch
emails from your external accounts.</p>
<p>There‚Äôs no clear rule on how often Gmail checks external accounts. It
seems to be proportional to how often you receive new emails: if you
often receive new emails, it‚Äôll check quite often, but if you receive
just a few messages per day, <strong>it can wait 10, 20 or even 30 minutes
between refreshes</strong>.</p>
<h2 id="forcing-gmail-to-refresh-external-accounts-more-often" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/02/ovh-email-redirect-spf-failure.html#forcing-gmail-to-refresh-external-accounts-more-often"><span>Forcing Gmail to refresh external accounts more often</span></a></h2>
<p>Looking into this I found quite a few smart tricks to get Gmail to check
your external accounts more often.</p>
<p>The first one is <a href="https://rakowski.pro/how-to-force-gmail-to-check-your-pop3-account-as-often-as-possible/">described in this post</a>,
and consists in configuring a server of yours to send you an email (on the
address you check via POP3) every minute or so. This way, Gmail will
notice you‚Äôre getting a lot of messages and will check more often.</p>
<p>To avoid your inbox getting flooded by those messages, you can simply
add a filter that puts them directly to the trash!</p>
<p>What if you don‚Äôt have a server handy to run this script? As described
in this <a href="https://lifehacker.com/increase-the-frequency-gmail-checks-your-other-email-ac-5580553">Lifehacker post</a>,
you can run the code as Google Apps Script on Google Sheets. That‚Äôs
pretty rad if you ask me. üòÇ</p>
<p>But what if you don‚Äôt want to write code at all? <a href="https://webapps.stackexchange.com/questions/1811/can-i-control-how-often-gmail-polls-pop3-accounts-for-incoming-mail#comment2919_2090">This Stack Exchange comment</a>
got you covered: just create a dummy Google Calendar event repeated every X
minutes, and set up an email reminder for this event. ü§Ø</p>
<h2 id="the-hybrid-approach" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/02/ovh-email-redirect-spf-failure.html#the-hybrid-approach"><span>The hybrid approach</span></a></h2>
<p>With OVH, there‚Äôs another option that allows you have the speed of the
redirect when it works, but still receive the messages even when there‚Äôs
an SPF rejection. The problem of this approach is that it will still
result in the ‚ÄúUndelivered Mail Returned to Sender‚Äù message being sent
to the sender in case SPF fails (even if you do get the email), which is
not ideal.</p>
<p>This solution consists in the OVH email redirect settings to ‚Äúkeep a
copy of the email at OVHcloud‚Äù. This option is only available during the
initial redirect creation and can‚Äôt be modified later on, so if you
initially configured it as ‚Äúdo not store a copy of the email‚Äù, you‚Äôll
need to delete your redirect and recreate it.</p>
<div class="note">
<p><strong>Note:</strong> creating a redirect that keeps a copy of the email on OVH
materializes as two redirect entries: one from source to source, and one from
source to destination.</p>
<p>For example, it‚Äôll show:</p>
<ul>
<li><code>foo@foo.com</code> to <code>foo@gmail.com</code></li>
<li><code>foo@foo.com</code> to <code>foo@foo.com</code></li>
</ul>
<p>Just noting that here because it can be confusing.</p>
</div>
<p>On top of that, you also configure Gmail to check your OVH emails via
POP3 <a href="https://www.codejam.info/2023/02/ovh-email-redirect-spf-failure.html#the-fix">as explained earlier</a>.</p>
<p>The result is that when the redirect works, you‚Äôll get your emails
instantly. When case the redirect fails, the message will still end up in
your OVH inbox, which Gmail will fetch eventually, so you‚Äôll still get
it that way.</p>
<p>But as I said, because the redirect failed, the sender will still
receive back an error email and believe that you didn‚Äôt receive their
email. I don‚Äôt know of a way to avoid that with this solution.</p>
<p>As for the messages that were successfully redirected, be assured that
<em>they won‚Äôt be duplicated</em>! Even though they will be fetched again when
Gmail refreshes the POP3 inbox, Gmail is able to tell that it‚Äôs the same
message that it already saw and will not show it to you twice. Neat.</p>
<h2 id="wrapping-up" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2023/02/ovh-email-redirect-spf-failure.html#wrapping-up"><span>Wrapping up</span></a></h2>
<p>In this post we explored the technical details of a common error that
happens when an email redirect conflicts with the origin SPF rules.</p>
<p>We saw that we can mitigate it under certain conditions, by reversing
the redirect relationship: instead of one email redirecting to another,
you need to have the second email checks the inbox of the former.</p>
<p>With Gmail, this can introduce unwanted latency, but there‚Äôs a few hacks
to work around that.</p>
<p>Finally we saw an hybrid approach that gets ‚Äúthe best of both worlds‚Äù,
except for the error message still being sent back‚Ä¶</p>
<p>What was your favorite option? Let me know on
<a href="https://twitter.com/valeriangalliat/status/1629899238728499200">Twitter</a>!</p>
<p>And if you know of other solutions I didn‚Äôt mention, please send me an
email! Even if you do strict SPF checks, don‚Äôt worry, I‚Äôll receive it. üòâ</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Join the discussion on <a href="https://twitter.com/valeriangalliat/status/1629899238728499200">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>GitHub: disable squash &amp; merge on specific branches</title>
    <link href="https://hookdeck.com/blog/post/building-chrome-extension-disable-squash-and-merge-github-branches" />
    <id>https://hookdeck.com/blog/post/building-chrome-extension-disable-squash-and-merge-github-branches</id>
    <updated>2022-08-15T04:00:00.000Z</updated>
  </entry>
  <entry>
    <title>Adobe Bridge mass update filetype associations</title>
    <link href="https://www.codejam.info/2022/07/adobe-bridge-mass-update-filetype-associations.html" />
    <id>https://www.codejam.info/2022/07/adobe-bridge-mass-update-filetype-associations.html</id>
    <updated>2022-07-19T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>Bridge is a pretty powerful file explorer by Adobe, that
<a href="https://prodesigntools.com/free-adobe-bridge-cc.html">now ships for free since the 2022 version</a>.
Sweet.</p>
<p>But there‚Äôs always one thing that bugs me with it: it keeps opening
photos in Photoshop, and Photoshop is sloooooow to start. When I
double-click on a picture, I most often just want to see it in Preview.</p>
<h2 id="the-manual-solution" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/07/adobe-bridge-mass-update-filetype-associations.html#the-manual-solution"><span>The manual solution</span></a></h2>
<p>The solution, is to go in ‚ÄúPreferences‚Äù, ‚ÄúFile Type
Associations‚Äù and associate pictures with Preview form there.</p>
<p>The problem is that there‚Äôs <em>countless</em> different extensions for
pictures and we need to update them one by one! Bummer.</p>
<figure class="center">
  <img alt="Bridge filetype associations" src="https://www.codejam.info/img/2022/07/bridge-preferences.png">
</figure>
<p>It would be fine if it only had to happen once, but on any new Bridge
installation, or even after a major update, it resets and I have to
start over.</p>
<h2 id="the-automated-solution" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/07/adobe-bridge-mass-update-filetype-associations.html#the-automated-solution"><span>The automated solution</span></a></h2>
<p>Luckily there‚Äôs a quick way to take everything that‚Äôs currently
associated to Photoshop and replace it with Preview!</p>
<p>First, we need to manually associate at least one filetype in the
previous dialog. This will make sure that Bridge creates the file
<code>~/Library/Application Support/Adobe/Bridge 2022/Adobe Bridge Opener Preferences.xml</code>,
where it stores the filetype associations.</p>
<p>From there, we can open it and using a text editor, replace every
occurrence of <code>Photoshop</code> with <code>/System/Applications/Preview.app</code>.</p>
<p>If you want a one-liner to paste, this can even be done with <code>sed</code>:</p>
<pre><code class="hljs language-sh">sed -i <span class="hljs-string">&#x27;&#x27;</span> <span class="hljs-string">&#x27;s,&quot;Photoshop&quot;,&quot;/System/Applications/Preview.app&quot;,g&#x27;</span> ~/Library/Application\ Support/Adobe/Bridge\ 2022/Adobe\ Bridge\ Opener\ Preferences.xml
</code></pre>
<p>After that, just restart Bridge and don‚Äôt ever be scared of a file
randomly opening in Photoshop again!</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Empty body vs. no body in HTTP/2</title>
    <link href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html" />
    <id>https://www.codejam.info/2022/06/empty-body-no-body-http2.html</id>
    <updated>2022-06-01T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p><abbr title="Today I learnt">TIL</abbr> there‚Äôs a (subtle) difference
in HTTP/2 between sending an empty body, and sending no body at all.</p>
<p>In this post we‚Äôll look at <strong>how that can happen</strong>, how to <strong>test it
with cURL</strong>, and the <strong>subtleties of HTTP/2</strong> that make this distinction
possible.</p>
<p>But as usual, I‚Äôll start by telling you the story of how I ended up with
such a hairy bug again <em>(I‚Äôm really, really good at getting myself in
this kind of fucked up situations for some reason)</em>.</p>
<h2 id="cloudflare-workers-and-the-content-length-header" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html#cloudflare-workers-and-the-content-length-header"><span>Cloudflare Workers and the <code>Content-Length</code> header</span></a></h2>
<p>At <a href="https://hookdeck.com/">Hookdeck</a>, we work heavily with <a href="https://workers.cloudflare.com/">Cloudflare
Workers</a>. And we also work heavily with
HTTP payloads.</p>
<p>One thing we asserted in the past, while it doesn‚Äôt seem to be
officially documented, is that Cloudflare computes the <code>Content-Length</code>
header if necessary before hitting the worker.</p>
<p>For example when sending a HTTP/1.1 <code>Transfer-Encoding: chunked</code> payload
(typically not including <code>Content-Length</code>), Cloudflare <strong>buffers the
whole body</strong> and sets the <code>Content-Length</code> header before calling the
worker, despite that header not being set by the client!</p>
<p>We observe a similar behavior in HTTP/2 (whose <code>DATA</code> frames
<a href="https://stackoverflow.com/questions/62439557/are-chunk-extensions-supported-by-http-2-and-if-so-how">resemble chunked encoding quite a bit</a>),
when the client omits the <code>Content-Length</code> header.</p>
<div class="note">
<p><strong>Note:</strong> even if we send a payload with an invalid <code>Content-Length</code>
(e.g. claiming a size much smaller than what we actually send),
Cloudflare catches it and refuses the request!</p>
</div>
<p>This is especially useful: because of that observation, we can actually
trust the <code>Content-Length</code> header, and rely on it to decide what to do
next in the worker.</p>
<h2 id="the-mysterious-requests-without-content-length" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html#the-mysterious-requests-without-content-length"><span>The mysterious requests without <code>Content-Length</code></span></a></h2>
<p>How then, during an incident response, do I find myself dealing with
<code>POST</code> requests that manifestly don‚Äôt have a <code>Content-Length</code> header?</p>
<p>My blind guess was to look at empty payloads. It‚Äôs the only edge case I
could think of that could, maybe, in some cases, result in Cloudflare
not enforcing a <code>Content-Length</code> header.</p>
<p>At first, I try the following:</p>
<pre><code class="hljs language-sh">curl https://events.hookdeck.com/e/source-id-goes-here \
  -X POST \
  -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span> \
  --data <span class="hljs-string">&#x27;&#x27;</span> \
  --verbose
</code></pre>
<p>But I notice in the verbose logs that cURL nicely computed and sent
<code>Content-Length: 0</code>. Luckily we can turn that off by passing an empty
<code>Content-Length</code> header (which makes cURL omit the header altogether in
its request):</p>
<pre><code class="hljs language-sh">curl https://events.hookdeck.com/e/source-id-goes-here \
  -X POST \
  -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span> \
  -H <span class="hljs-string">&#x27;Content-Length:&#x27;</span> \
  --data <span class="hljs-string">&#x27;&#x27;</span> \
  --verbose
</code></pre>
<p>But somehow, Cloudflare is still able to catch this and forces a
<code>Content-Length: 0</code> to be passed to my worker.</p>
<p>I try something else, which in my understanding <em>should</em> be the same
thing (omitting the <code>--data</code> parameter altogether):</p>
<pre><code class="hljs language-sh">curl https://events.hookdeck.com/e/source-id-goes-here \
  -X POST \
  -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span> \
  --verbose
</code></pre>
<p>To my surprise, although the verbose logs from cURL look <em>identical</em>,
<strong>this results in the request hitting my worker without a
<code>Content-Length</code> header</strong>, bypassing Cloudflare‚Äôs ‚Äúenforcement‚Äù. Bingo!</p>
<p>This is a good step forward, but I‚Äôm even more confused. To my knowledge
those two commands <em>should</em> result in the exact same HTTP requests over
the wire. ü§î</p>
<div class="note">
<p><strong>Note:</strong> at that point I had a confirmation that having no
<code>Content-Length</code> header here was, in fact, possible (in the case of some
obscure empty payloads that are different from ‚Äúnormal‚Äù empty payloads
<em>somehow</em>).</p>
<p>I went on and made sure that the code could handle that, but I wasn‚Äôt
exactly <em>satisfied</em>. The <em>‚Äúsomehow‚Äù</em> part of my previous sentence was
itching me in a particular manner.</p>
</div>
<h2 id="digging-deeper-with-trace" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html#digging-deeper-with-trace"><span>Digging deeper with <code>--trace</code></span></a></h2>
<p>I tried adding <code>--trace</code>, and <code>--trace-ascii</code> to the previous cURL
commands, in order to dump the raw protocol data and compare it:</p>
<pre><code class="hljs language-diff:sh"> curl https://events.hookdeck.com/e/source-id-goes-here \
   -X POST \
   -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span> \
   -H <span class="hljs-string">&#x27;Content-Length:&#x27;</span> \
<span class="hljs-deletion">-  --data <span class="hljs-string">&#x27;&#x27;</span></span>
<span class="hljs-addition">+  --data <span class="hljs-string">&#x27;&#x27;</span> \</span>
<span class="hljs-addition">+  --trace empty-body.txt</span>
 
 curl https://events.hookdeck.com/e/source-id-goes-here \
   -X POST \
<span class="hljs-deletion">-  -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span></span>
<span class="hljs-addition">+  -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span> \</span>
<span class="hljs-addition">+  --trace no-body.txt</span>
</code></pre>
<p>Then diffing it with:</p>
<pre><code class="hljs language-sh">git diff --no-index empty-body.txt no-body.txt
</code></pre>
<p>(I like the output of <code>git diff</code> more than plain old
<a href="https://linux.die.net/man/1/diff"><code>diff(1)</code></a>.)</p>
<p>But this shows no relevant differences. Only the ‚ÄúSSL data‚Äù bits change,
but those are unintelligible. It otherwise appears that cURL sends
<em>exactly</em> the same thing.</p>
<p>How in hell could Cloudflare distinguish those two different yet
identical cURL invocations? <em>Hint: probably in the unintelligible
bits‚Ä¶</em></p>
<h2 id="what-about-http-1-1" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html#what-about-http-1-1"><span>What about HTTP/1.1</span></a></h2>
<p>So far, cURL defaulted to use HTTP/2, which is great. Maybe it‚Äôs a
HTTP/2-specific thing? (I know, I kinda spoiled it in the title of this
post.)</p>
<p>I add <code>--http1.1</code> to the earlier cURL commands to try: both requests
don‚Äôt have the <code>Content-Length</code> header after going through Cloudflare.
Interesting.</p>
<p>So there‚Äôs absolutely no difference between ‚Äúno data‚Äù and ‚Äúempty body‚Äù
in HTTP/1.1, which makes a lot of sense based on my understanding of the
HTTP protocol. There‚Äôs, finally, some sanity in this world.</p>
<p>So my quest is now to figure <strong>how the f*** is Cloudflare able to
distinguish between ‚Äúno body‚Äù and ‚Äúempty body‚Äù in HTTP/2 specifically</strong>.</p>
<div class="note">
<p><strong>Note:</strong> the attentive reader might have noticed that there‚Äôs virtually
no business value in answering that question.</p>
<p>I already knew a few ways to trigger an undefined <code>Content-Length</code> header,
and that was enough information for me to fix the bug and replay
whatever requests needed to.</p>
<p>At that point I‚Äôm only trying to quench my thirst of knowledge for sheer
pleasure.</p>
</div>
<h2 id="making-a-poc-in-c" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html#making-a-poc-in-c"><span>Making a PoC in C</span></a></h2>
<p>I decide to go a bit lower level and instead of using the cURL command,
I make a C program using <code>libcurl</code> to try and reproduce that behavior.</p>
<pre><code class="hljs language-c"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;curl/curl.h&gt;</span></span>

<span class="hljs-type">int</span> <span class="hljs-title function_">main</span> <span class="hljs-params">(<span class="hljs-type">void</span>)</span> {
    curl_global_init(CURL_GLOBAL_ALL);

    CURL *curl = curl_easy_init();

    curl_easy_setopt(curl, CURLOPT_URL, <span class="hljs-string">&quot;https://events.hookdeck.com/e/source-id-goes-here&quot;</span>);
    curl_easy_setopt(curl, CURLOPT_HTTP_VERSION, CURL_HTTP_VERSION_2TLS);
    curl_easy_setopt(curl, CURLOPT_VERBOSE, <span class="hljs-number">1</span>);

    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">curl_slist</span> *<span class="hljs-title">list</span> =</span> <span class="hljs-literal">NULL</span>;

    <span class="hljs-built_in">list</span> = curl_slist_append(<span class="hljs-built_in">list</span>, <span class="hljs-string">&quot;Content-Type: text/plain&quot;</span>);
    <span class="hljs-built_in">list</span> = curl_slist_append(<span class="hljs-built_in">list</span>, <span class="hljs-string">&quot;Content-Length:&quot;</span>);

    curl_easy_setopt(curl, CURLOPT_HTTPHEADER, <span class="hljs-built_in">list</span>);

    <span class="hljs-comment">// Empty body</span>
    curl_easy_setopt(curl, CURLOPT_POSTFIELDS, <span class="hljs-string">&quot;&quot;</span>);

    <span class="hljs-comment">// No body</span>
    <span class="hljs-comment">// curl_easy_setopt(curl, CURLOPT_CUSTOMREQUEST, &quot;POST&quot;);</span>

    curl_easy_perform(curl);

    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;
}
</code></pre>
<p>Here, the <code>CURLOPT_POSTFIELDS</code> method will result in the ‚Äúempty body‚Äù
path (where Cloudflare can set <code>Content-Length: 0</code> by itself), while
the ‚Äúno body‚Äù version will let the request go through all the way
without <code>Content-Length</code>.</p>
<p>It can be compiled and run with:</p>
<pre><code class="hljs language-sh">gcc test.c -o <span class="hljs-built_in">test</span> -lcurl
./test
</code></pre>
<p>But this repro doesn‚Äôt really lead me anywhere. This is not
low-level enough.</p>
<h2 id="digging-even-deeper-with-netcat" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html#digging-even-deeper-with-netcat"><span>Digging even deeper with netcat</span></a></h2>
<p>If I can‚Äôt find on the client side what distinguishes those requests,
let‚Äôs analyze the server side.</p>
<p>My first bet is to use <a href="https://linux.die.net/man/1/nc"><code>nc(1)</code></a>
(netcat) in listen mode and send my two <code>curl</code> requests to it. Then I‚Äôll
be able to see the raw data sent by cURL the underlying socket and
hopefully tell them apart:</p>
<pre><code class="hljs language-sh">nc -l -k -p 8888
</code></pre>
<p>(This makes netcat listen on port 8888: <code>-l</code> to listen, <code>-k</code> to keep
listening after the first connection, and <code>-p</code> to specify the port.)</p>
<p>Then I can hit it:</p>
<pre><code class="hljs language-diff:sh"><span class="hljs-deletion">-curl https://events.hookdeck.com/e/source-id-goes-here \</span>
<span class="hljs-addition">+curl http://localhost:8888/ \</span>
   -X POST \
   -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span> \
   -H <span class="hljs-string">&#x27;Content-Length:&#x27;</span> \
   --data <span class="hljs-string">&#x27;&#x27;</span>
 
<span class="hljs-deletion">-curl https://events.hookdeck.com/e/source-id-goes-here \</span>
<span class="hljs-addition">+curl http://localhost:8888/ \</span>
   -X POST \
   -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span>
</code></pre>
<p>Sadly this results in the same HTTP/1.1 request in both cases:</p>
<pre><code class="hljs language-http"><span class="hljs-keyword">POST</span> <span class="hljs-string">/</span> <span class="hljs-meta">HTTP/1.1</span>
<span class="hljs-attribute">Host</span><span class="hljs-punctuation">: </span>localhost:8888
<span class="hljs-attribute">User-Agent</span><span class="hljs-punctuation">: </span>&lt;3
<span class="hljs-attribute">Accept</span><span class="hljs-punctuation">: </span>*/*
<span class="hljs-attribute">Content-Type</span><span class="hljs-punctuation">: </span>text/plain

</code></pre>
<p>(Yes <a href="https://github.com/valeriangalliat/dotfiles/blob/40ca54c1d6fdfca33e8dcc4e56807f9bf060de8e/net/curlrc#L1">my user agent is a heart in ASCII</a>,
what r u gonna do?)</p>
<p>And adding the <code>--http2</code> flag makes cURL ask for an upgrade to HTTP/2,
but can‚Äôt just send its HTTP/2 traffic right through:</p>
<pre><code class="hljs language-http"><span class="hljs-keyword">POST</span> <span class="hljs-string">/</span> <span class="hljs-meta">HTTP/1.1</span>
<span class="hljs-attribute">Host</span><span class="hljs-punctuation">: </span>localhost:8888
<span class="hljs-attribute">User-Agent</span><span class="hljs-punctuation">: </span>&lt;3
<span class="hljs-attribute">Accept</span><span class="hljs-punctuation">: </span>*/*
<span class="hljs-attribute">Connection</span><span class="hljs-punctuation">: </span>Upgrade, HTTP2-Settings
<span class="hljs-attribute">Upgrade</span><span class="hljs-punctuation">: </span>h2c
<span class="hljs-attribute">HTTP2-Settings</span><span class="hljs-punctuation">: </span>AAMAAABkAAQCAAAAAAIAAAAA
<span class="hljs-attribute">Content-Type</span><span class="hljs-punctuation">: </span>text/plain

</code></pre>
<p>Looks like some <em>negotiation</em> needs to happen prior to using HTTP/2.
Bummer.</p>
<h2 id="making-a-http-2-server-with-nodes-js" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html#making-a-http-2-server-with-nodes-js"><span>Making a HTTP/2 server with Nodes.js</span></a></h2>
<p>If we can‚Äôt <em>netcat</em> our way out of this, let‚Äôs make a real HTTP/2
server with Node.js.</p>
<p>First we‚Äôll generate a TLS key and certificate for <code>localhost</code> because
it appears that the HTTP/2 negotiation happens over TLS. Although it
doesn‚Äôt seem that the HTTP/2 spec <em>requires</em> TLS per se, I couldn‚Äôt make
it work without.</p>
<pre><code class="hljs language-sh">openssl req -x509 -newkey rsa:2048 -nodes -subj <span class="hljs-string">&#x27;/CN=localhost&#x27;</span> -keyout key.pem -out cert.pem
</code></pre>
<div class="note">
<p><strong>Note:</strong> in this command, <code>-nodes</code> <a href="https://stackoverflow.com/a/5087138/4324668">means ‚Äúno DES‚Äù and not ‚Äúnodes‚Äù</a>
and is used to leave the private key unencrypted. Without it, OpenSSL
will prompt for a passphrase.</p>
<p>Also the <code>-subj</code> argument is required otherwise OpenSSL will prompt for
all the certificate fields.</p>
</div>
<pre><code class="hljs language-js"><span class="hljs-keyword">import</span> http2 <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;node:http2&#x27;</span>
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;node:fs/promises&#x27;</span>

<span class="hljs-keyword">const</span> server = http2.<span class="hljs-title function_">createSecureServer</span>({
  <span class="hljs-attr">key</span>: <span class="hljs-keyword">await</span> fs.<span class="hljs-title function_">readFile</span>(<span class="hljs-string">&#x27;key.pem&#x27;</span>),
  <span class="hljs-attr">cert</span>: <span class="hljs-keyword">await</span> fs.<span class="hljs-title function_">readFile</span>(<span class="hljs-string">&#x27;cert.pem&#x27;</span>)
})

server.<span class="hljs-title function_">on</span>(<span class="hljs-string">&#x27;stream&#x27;</span>, <span class="hljs-function">(<span class="hljs-params">stream, headers, flags, rawHeaders</span>) =&gt;</span> {
  <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(flags, rawHeaders)

  stream.<span class="hljs-title function_">respond</span>({
    <span class="hljs-string">&#x27;:status&#x27;</span>: <span class="hljs-number">200</span>,
    <span class="hljs-string">&#x27;content-type&#x27;</span>: <span class="hljs-string">&#x27;text/plain&#x27;</span>
  })

  stream.<span class="hljs-title function_">end</span>(<span class="hljs-string">&#x27;Hello&#x27;</span>)
})

server.<span class="hljs-title function_">listen</span>(<span class="hljs-number">8888</span>)
</code></pre>
<p>For each new HTTP/2 stream, this server will log the
<a href="https://nodejs.org/api/http2.html#event-stream">‚Äúassociated flags‚Äù</a> as
well as the raw headers, in the hope to find the key difference there.</p>
<p>As before, we hit it, with the addition of <code>--insecure</code> because we don‚Äôt
want cURL to reject our self-signed certificate:</p>
<pre><code class="hljs language-diff:sh"> curl http://localhost:8888/ \
   -X POST \
   -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span> \
   -H <span class="hljs-string">&#x27;Content-Length:&#x27;</span> \
<span class="hljs-deletion">-  --data <span class="hljs-string">&#x27;&#x27;</span></span>
<span class="hljs-addition">+  --data <span class="hljs-string">&#x27;&#x27;</span> \</span>
<span class="hljs-addition">+  --insecure</span>
 
 curl http://localhost:8888/ \
   -X POST \
<span class="hljs-deletion">-  -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span></span>
<span class="hljs-addition">+  -H <span class="hljs-string">&#x27;Content-Type: text/plain&#x27;</span> \</span>
<span class="hljs-addition">+  --insecure</span>
</code></pre>
<p>And while the raw headers are exactly the same, the flag is different:
in the first case (empty body) it‚Äôs set to <strong>4</strong>, while for the second
one (no body) it‚Äôs <strong>5</strong>. Bingo!</p>
<p>So what are those flags about anyway? The <a href="https://nodejs.org/api/http2.html#event-stream">Node.js documentation</a>
doesn‚Äôt say much‚Ä¶</p>
<blockquote>
<p><code>flags</code> <code>&lt;number&gt;</code> The associated numeric flags.</p>
</blockquote>
<h2 id="understanding-the-http-2-flags" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html#understanding-the-http-2-flags"><span>Understanding the HTTP/2 flags</span></a></h2>
<p>We get a hint of the available flags in <code>http2.constants</code>:</p>
<pre><code class="hljs language-js"><span class="hljs-title class_">Object</span>.<span class="hljs-title function_">keys</span>(http2.<span class="hljs-property">constants</span>)
  .<span class="hljs-title function_">filter</span>(<span class="hljs-function"><span class="hljs-params">name</span> =&gt;</span> name.<span class="hljs-title function_">includes</span>(<span class="hljs-string">&#x27;_FLAG_&#x27;</span>))
  .<span class="hljs-title function_">map</span>(<span class="hljs-function"><span class="hljs-params">name</span> =&gt;</span> <span class="hljs-string">`<span class="hljs-subst">${name}</span>: <span class="hljs-subst">${http2.constants[name]}</span>`</span>)
  .<span class="hljs-title function_">join</span>(<span class="hljs-string">&#x27;\n&#x27;</span>)
</code></pre>
<pre><code class="hljs">NGHTTP2_FLAG_NONE: 0
NGHTTP2_FLAG_END_STREAM: 1
NGHTTP2_FLAG_END_HEADERS: 4
NGHTTP2_FLAG_ACK: 1
NGHTTP2_FLAG_PADDED: 8
NGHTTP2_FLAG_PRIORITY: 32
</code></pre>
<p>We‚Äôre in the presence of bitwise flags. Let‚Äôs ‚Äúflatten‚Äù all of that in
binary, and pad them with zeroes up to 5 digits for display. This can be
done with:</p>
<pre><code class="hljs language-js">(number).<span class="hljs-title function_">toString</span>(<span class="hljs-number">2</span>).<span class="hljs-title function_">padStart</span>(<span class="hljs-number">5</span>, <span class="hljs-number">0</span>)
</code></pre>
<p>(Parentheses around <code>number</code> required when putting a literal number in
there.)</p>
<p>This gives us:</p>
<pre><code class="hljs">00100 (4) empty body
00101 (5) no body
</code></pre>
<p>And the <code>http2.constants</code> flags:</p>
<pre><code class="hljs">00000 (0) NGHTTP2_FLAG_NONE
00001 (1) NGHTTP2_FLAG_END_STREAM
00100 (4) NGHTTP2_FLAG_END_HEADERS
00001 (1) NGHTTP2_FLAG_ACK
01000 (8) NGHTTP2_FLAG_PADDED
10000 (32) NGHTTP2_FLAG_PRIORITY
</code></pre>
<p>Here we can clearly see that ‚Äúempty body‚Äù is just the <code>END_HEADERS</code>
flag, whereas ‚Äúno body‚Äù is a combination of <code>END_HEADERS</code> <em>and</em>
<code>END_STREAM</code>.</p>
<p>This is what makes Cloudflare behave in two different ways based on
those cURL requests!</p>
<p>If we go to <a href="https://datatracker.ietf.org/doc/html/rfc7540">the HTTP/2 RFC</a>
we get extra information in <a href="https://datatracker.ietf.org/doc/html/rfc7540#section-6.2">section 6.2</a>:</p>
<blockquote>
<p><code>END_STREAM</code> (0x1): When set, bit 0 indicates that the header block
is the last that the endpoint will send for the identified stream.</p>
<p><code>END_HEADERS</code> (0x4): When set, bit 2 indicates that this frame
contains an entire header block and is not followed by any
<code>CONTINUATION</code> frames.</p>
</blockquote>
<h2 id="in-short" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html#in-short"><span>In short</span></a></h2>
<p>While in HTTP/1.1 a request with no body (<code>curl -X POST</code>) is strictly
equivalent to a request with an empty body (<code>curl -X POST --data ''</code>),
there‚Äôs a subtle difference when using HTTP/2:</p>
<p>A ‚Äúno body‚Äù requests sets the <code>END_HEADERS &amp; END_STREAM</code> flags on the
HTTP/2 stream, whereas an ‚Äúempty body‚Äù will result in only <code>END_HEADERS</code>
(at least in the cURL implementation).</p>
<p>This can lead to those requests being treated slightly differently,
especially when they don‚Äôt include a <code>Content-Length</code> header. In the
case of Cloudflare Workers, here‚Äôs a table of <strong>whether or not
Cloudflare computed the <code>Content-Length</code> header for us</strong> despite not
being set by the client:</p>
<table>
<thead>
<tr>
<th>Request</th>
<th>HTTP/1.1</th>
<th>HTTP/2</th>
</tr>
</thead>
<tbody>
<tr>
<td>non-empty body (chunked)</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>non-empty body (not chunked)</td>
<td>Illegal</td>
<td>Body is always ‚Äúchunked‚Äù in HTTP/2</td>
</tr>
<tr>
<td>empty body</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>no body</td>
<td>No</td>
<td>No</td>
</tr>
</tbody>
</table>
<div class="note">
<p><strong>Note:</strong> in the case of the <code>POST</code> body lacking <code>Content-Length</code> and
<code>Transfer-Encoding: chunked</code>, this is effectively <a href="https://stackoverflow.com/questions/14758729/http-post-content-length-header-required">forbidden</a>
in HTTP/1.1.</p>
<p>Cloudflare still accepts those requests, but the <code>Content-Length</code> header
will definitely not be set, and the worker will see the body as being
empty (despite the client sending actual data).</p>
<p>Not really supposed to happen but good to know.</p>
</div>
<h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/06/empty-body-no-body-http2.html#conclusion"><span>Conclusion</span></a></h2>
<p>This was a fun issue to dig into. It wasn‚Äôt necessary to go that deep in
the rabbit hole, but it was definitely a fun challenge, plus it made me
learnt quite a bit about HTTP/2 which I wasn‚Äôt really up-to-date with.</p>
<p>I hope you enjoyed the read. Stay curious! ü§ô</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Join the discussion on <a href="https://twitter.com/valeriangalliat/status/1532007358225862657">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Best GoPro mount position for skiing</title>
    <link href="https://www.codejam.info/2022/05/best-gopro-position-skiing.html" />
    <id>https://www.codejam.info/2022/05/best-gopro-position-skiing.html</id>
    <updated>2022-05-25T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>After a long journey figuring how to <a href="https://www.codejam.info/2022/05/gopro-canadian-winter.html">prevent my GoPro HERO5 from dying in cold weather</a>
(so basically, during all of the ski season üôÑ), it was time for me to
find the best mount point for recording POV skiing videos!</p>
<h2 id="helmet-top" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/best-gopro-position-skiing.html#helmet-top"><span>Helmet top</span></a></h2>
<p>This is what seems to be the most common place to mount a GoPro for
winter sports. Still, there‚Äôs many ways to mount a GoPro on top of a
helmet, and it will vary on the kind of helmet you have, and how much
you‚Äôre willing to trade convenience for better video quality.</p>
<p>The shape of your helmet might not allow you to stick a GoPro mount
anywhere, because of vents (and the switch to open or close them), and
shape variations like insets and other non-smooth cuts. So you might not
have that much choice for a solid mount.</p>
<p>If you do though, here‚Äôs the tradeoffs:</p>
<ul>
<li>
<p>If you mount it <strong>at the frontmost part</strong>, you‚Äôre going to get a more
immersive point of view, but the GoPro will be in the way of your
goggles, so you can‚Äôt put them up on your helmet while the GoPro is
mounted there. Not super convenient.</p>
<p>Also I found that this was putting quite a bunch of weight towards the
front of my helmet and it was constantly pushing it down. The pressure
around my nose was a bit uncomfortable and most importantly, it kept
slipping down as I was skiing, especially during big shocks. You might
get better results depending on your helmet/goggles combination, but
it wasn‚Äôt very successful for me.</p>
</li>
<li>
<p>If you put it <strong>fully on top</strong>, it will be more balanced while skiing
(although I still have the same slipping issue as before, to a lesser
extent), but the point of view will be a bit high. You don‚Äôt see the
skis as much, and if you point it down too much, you will start seeing
a bit of the helmet at the bottom of the frame.</p>
</li>
<li>
<p>Somewhere <strong>in between</strong>, you should still have the option to put your
goggles up on your helmet without bothering the GoPro. The point of
view is not bad, although you might risk seeing a bit of the helmet at
the bottom of the frame. It will still put most of the weight on the
front of the helmet so that might be an issue for comfort depending on
your gear.</p>
</li>
</ul>
<p>With the GoPro on top of the helmet, it‚Äôs also more prone to taking the
wind, which can get tiring for your neck after a while.</p>
<p>One of the best advantages of a helmet-mounted GoPro is that it moves
with your face! Meaning that when you look one way or another, the
camera will follow you, making it easy to show action happening left and
right.</p>
<p>The problem is that it will <em>always</em> move with your head, including when
you check your surroundings and behind you before changing directions.
For me, this happens much more often than when I want to show on camera
something that‚Äôs not in front of me. And those quick pans are not
usually desirable.</p>
<h2 id="helmet-side" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/best-gopro-position-skiing.html#helmet-side"><span>Helmet side</span></a></h2>
<p>I loved that point of view <a href="https://youtu.be/o0Y1C88BhdY">on my motorcycle helmet</a>.
You can get the GoPro exactly at your eye level which gives one of the
best POV styles in my opinion, although the side of the helmet might be
in the frame, and you‚Äôll see the ‚Äúclear‚Äù side wider than the helmet
side.</p>
<p>From my motorcycle experience with a side mount, it was definitely not
as tiring for the neck than a top mount is. Seems that my neck is
stronger for compensating horizontal rotation than vertical inclination.
üòÜ</p>
<p>But the biggest problem in the case of a ski helmet is that a side mount
will be <strong>in the way of the strap of your goggles</strong>!</p>
<p>Depending on the shape of your helmet you might find a way to make this
work, but I couldn‚Äôt manage it on my own helmet. Too bad.</p>
<h2 id="chest-mount" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/best-gopro-position-skiing.html#chest-mount"><span>Chest mount</span></a></h2>
<p>When I found <a href="https://youtu.be/pXfNA-xqETA">this video</a> after hours
(days‚Ä¶) of research about <a href="https://www.codejam.info/2022/05/gopro-canadian-winter.html">how to keep the GoPro alive in cold temperature</a>,
they also mentioned their chest mount setup.</p>
<p>It‚Äôs something that was quite new to me, I‚Äôve seen very little chest
mount ski videos online in the past, and I really liked the look. So I
<a href="https://amzn.to/3sXrOcR">got one for myself</a>.</p>
<p>It‚Äôs a bit lower so you can see the skis really well, while still having
the whole landscape in the frame. You also see a bit the hands with the
poles on the sides and that‚Äôs quite a cool view. You can even see the
knees at rare occasions e.g. on really bumpy parts.</p>
<p>Overall, I find it <strong>even more immersive</strong> than a helmet mount!</p>
<p>But because it‚Äôs chest mounted, <strong>you can‚Äôt as easily turn to the
sides</strong> to show some action happening around, you‚Äôll always see pretty
much just what‚Äôs in front where your body is facing. You can try turning
your chest a bit to the sides but the effect is very limited compared to
turning your head.</p>
<p>It‚Äôs not something that I need a lot, so no big deal for me. Actually it
can even be seen as <strong>a feature</strong>: you can freely look around while
skiing without worrying about the camera moving in all directions and
making you nauseous when you watch it!</p>
<p>The chest mount gives more stable results than a helmet mount especially
on really shaky and bumpy slopes, and in the end I find the chest mount
footage smoother and more enjoyable to watch.</p>
<p>But the biggest thing for me was that the chest mount is <strong>really
comfortable</strong>. Having the extra weight of the GoPro on my chest makes
little to no difference. I can ski with a chest mount GoPro all day
without even noticing it‚Äôs there!</p>
<p>It‚Äôs also easier to take off and on from its mount because it‚Äôs right in
front of you and you can see it. When it‚Äôs on your head, finding the
hole for the screw can be quite tricky, although after a few runs you
kinda get a sense for it. üòè</p>
<h2 id="pros-and-cons-table" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/best-gopro-position-skiing.html#pros-and-cons-table"><span>Pros and cons table</span></a></h2>
<p>To summarize, I‚Äôll compare those 3 options with a number of criteria:</p>
<ul>
<li><strong>POV:</strong> how realistic is the point of view footage.</li>
<li><strong>Compatibility:</strong> can you use it with most gear or is it tricky?</li>
<li><strong>Comfort:</strong> how comfortable, or tiring is it.</li>
<li><strong>Maneuverability:</strong> is it easy to take off and on?</li>
<li><strong>Look around:</strong> do the camera follow you when you look around?</li>
<li><strong>Stability:</strong> how stable is the footage.</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>Helmet top</th>
<th>Helmet side</th>
<th>Chest</th>
</tr>
</thead>
<tbody>
<tr>
<td>POV</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
</tr>
<tr>
<td>Compatibility</td>
<td>‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
<td>‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
</tr>
<tr>
<td>Comfort</td>
<td>‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</td>
<td>‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
</tr>
<tr>
<td>Maneuverability</td>
<td>‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
<td>‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
</tr>
<tr>
<td>Look around</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ</td>
<td>‚òÖ‚òÜ‚òÜ‚òÜ‚òÜ</td>
</tr>
<tr>
<td>Stability</td>
<td>‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
<td>‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ</td>
<td>‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ</td>
</tr>
</tbody>
</table>
<p>The helmet top POV is the most conventional, but <strong>only when the mount
it put at the frontmost part of the helmet</strong>. If it‚Äôs put higher, it
quickly feels too high and loses some of its immersiveness, and you risk
seeing a bit of the helmet at the bottom of the frame.</p>
<p>The side mount is a more typical kind of POV that might not be to the
taste of everyone, but I personally really like it. Same goes for the
chest mount.</p>
<p>The comfort of a top mount might be better with your particular
helmet/goggles combination but for me it was a no-no. Also if you want
to put the mount at the frontmost part, which is where it looks the
best, you lose the ability to put your goggles up on your helmet, and
I find that quite annoying.</p>
<p>Still better than the side mount where it‚Äôs basically incompatible with
having a goggles strap, but might work for you if your helmet has a
built-in visor.</p>
<p>The chest mount won‚Äôt follow your face when you look around, so you
might not be able to capture a sudden event around you where you‚Äôre not
directly facing. But we‚Äôve seen this also gives a much more stable image
and lets you freely move your head around (e.g. for safety) without
making your viewer nauseous.</p>
<p>When I‚Äôm not recording, I was wondering how annoying it would be to wear
the chest strap, and if I needed to take it off and on all the time. It
turns out the chest strap is really not a big deal, I can wear it all
day long without even noticing it‚Äôs there.</p>
<h2 id="what-about-the-field-of-view" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/best-gopro-position-skiing.html#what-about-the-field-of-view"><span>What about the field of view?</span></a></h2>
<p>In all cases, SuperView. While in my <a href="https://www.codejam.info/2020/06/my-settings-for-gopro-hero-2018-and-hero5-black.html">my GoPro settings post</a>
I said I prefer wide to SuperView, skiing is the exception.</p>
<p>When shooting POV skiing in anything else than SuperView, you‚Äôre mostly
just gonna see the snow and little to nothing of the landscape in front
of and around you.</p>
<p>SuperView all the way, trust me.</p>
<h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/best-gopro-position-skiing.html#conclusion"><span>Conclusion</span></a></h2>
<p>At the end of the day, the <a href="https://amzn.to/3sXrOcR">chest mount</a> won
for me: it‚Äôs by far <strong>the most comfortable and practical solution</strong>, so
that the fact I‚Äôm recording a video doesn‚Äôt impact my skiing experience
whatsoever.</p>
<p>This is a huge deal for me. If recording a video takes effort and
negatively affects my skiing, I‚Äôm less likely to do it, and at the end
of the day that might make me miss some runs that would have been really
cool to have on video. With a chest mount, especially now my GoPro can
<a href="https://www.codejam.info/2022/05/gopro-canadian-winter.html">handle the cold</a>, I can record liberally
while still having a great time. After all, I‚Äôm here <strong>mostly to ski,
not just to record a video</strong>.</p>
<p>On top of that, I like the chest mount POV quite a lot, despite being a
bit unconventional. I find it very immersive, and we get a great view of
the skis, the slope and the landscape around, all at the same time.</p>
<p>As a bonus, it seems my chest is better at absorbing shocks and
stabilizing footage than my head is. And the footage is smoother overall
because my chest is not moving as much as my head while skiing.</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Using a GoPro in the Canadian winter ‚ùÑÔ∏è</title>
    <link href="https://www.codejam.info/2022/05/gopro-canadian-winter.html" />
    <id>https://www.codejam.info/2022/05/gopro-canadian-winter.html</id>
    <updated>2022-05-25T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>If you ever used a GoPro in a very cold weather, I‚Äôm talking -20 or even -30 ¬∞C,
you probably noticed that its battery dies extremely fast. It‚Äôs
sometimes a matter of <em>minutes</em> before it shuts off and refuses to start
again until it‚Äôs warmed up again.</p>
<p><a href="https://www.newschoolers.com/forum/thread/919646/GoPro-Hero8-Can-t-Handle-the-Cold">The</a>
<a href="https://www.reddit.com/r/gopro/comments/kd48w1/gopro_hero_9_turns_off_at_cold_temperatures/">internet</a>
<a href="https://www.quora.com/I-wanted-to-use-my-GoPro-Hero-4-today-while-snowboarding-but-the-battery-died-prematurely-due-to-the-cold-What-is-a-way-of-getting-around-this-issue">is</a>
<a href="https://www.hardcoresledder.com/threads/gopro-camera-stops-in-cold-weather.620282/page-2">full</a>
<a href="https://www.codejam.info/img/2022/05/gopro-cold-weather.png">of</a>
<a href="https://youtu.be/WK6oi7x_sNU">people</a>
<a href="https://youtu.be/eeI7bTvlXyg">having</a>
<a href="https://youtu.be/noGoux3Yya4">issues</a>
<a href="https://youtu.be/9SnPMeXVK3E">with</a>
<a href="https://youtu.be/mU-FOWcvZNg">their</a>
<a href="https://youtu.be/FlqLFW9DZVk">GoPro</a>
(any version especially after HERO4) dying ridiculously fast after being used
in cold weather, e.g. with winter sports.</p>
<blockquote>
<p>In any conditions below 20 ¬∞F / -10 ¬∞C my GoPro HERO8 renders itself
utterly useless with the battery dying after about 10 minutes of
filming.
[<a href="https://www.newschoolers.com/forum/thread/919646/GoPro-Hero8-Can-t-Handle-the-Cold">Source</a>]</p>
<p>I am skiing right now with a GoPro max. I can‚Äôt even take it outside before it dies. 6-7 degrees below Celsius.
[<a href="https://www.reddit.com/r/gopro/comments/kd48w1/gopro_hero_9_turns_off_at_cold_temperatures/">Source</a>]</p>
<p>Found the same problem on my HERO8 Black. Tested in -27 here in Norway and it died immediately after 2-3 clips.
[<a href="https://youtu.be/FlqLFW9DZVk">Source</a>]</p>
<p>Battery life has been a real pain for me as well using it during the
winter months. I keep my camera and batteries in my pants pockets
until I need them but even doing that might yield me five minutes of
recording once it‚Äôs exposed to sub-zero temps.
[<a href="https://youtu.be/FlqLFW9DZVk">Source</a>]</p>
</blockquote>
<p>And it seems like it‚Äôs <a href="https://forum.dji.com/forum.php?mod=viewthread&amp;tid=252188">not just a problem with GoPro</a>.</p>
<blockquote>
<p>When the temperature is below -10 ‚ÑÉ, DJI Action 2 cannot be turned on.</p>
</blockquote>
<p>There‚Äôs a number of tips, lots of them that I‚Äôm not really read for.</p>
<blockquote>
<p>When you finish your line, stop and take the GoPro off and put it in a
pocket or backpack, take the battery out and put it back in warm
pocket.
[<a href="https://www.newschoolers.com/forum/thread/919646/GoPro-Hero8-Can-t-Handle-the-Cold">Source</a>]</p>
<p>Definitely worth buying an extra set of batteries. I‚Äôve gone through
4+ on a good pow day.
[<a href="https://www.newschoolers.com/forum/thread/919646/GoPro-Hero8-Can-t-Handle-the-Cold">Source</a>]</p>
<p>If you want to capture winter sports such as skiing, snowboarding,
sled dog racing etc., don‚Äôt waste your hard earned money on a Go Pro.
[<a href="https://www.newschoolers.com/forum/thread/919646/GoPro-Hero8-Can-t-Handle-the-Cold">Source</a>]</p>
</blockquote>
<p>You also see people putting it in a <a href="https://youtu.be/9SnPMeXVK3E">foam cover</a>
or others <a href="https://youtu.be/WK6oi7x_sNU">storing it in a bag</a> with <a href="https://youtu.be/eeI7bTvlXyg">hand
or feet warmers</a> when not in use.</p>
<h2 id="the-problem-with-the-default-case" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/gopro-canadian-winter.html#the-problem-with-the-default-case"><span>The problem with the default case</span></a></h2>
<p>I was actually surprised the first time I mounted the GoPro on my
helmet and I saw it die that fast. In the past I‚Äôve used that same GoPro
hand-held, an I remember being able to shoot quite a lot, and still had
some battery left at the end of a ski day.</p>
<p>So what could be the culprit? Sure when it‚Äôs mounted on my helmet I tend
to leave it outside a bit more, whereas before I would store it in my
pocket when it‚Äôs not recording, but I‚Äôve made a habit of taking it off
my helmet and putting it in my pocket during the lifts too, and still
had the same issues.</p>
<p>I‚Äôve tried different pockets, jacket pocket, pants pocket, internal
pocket, but it wouldn‚Äôt make much difference. It was always dead after a
few hours of being outside, regardless if I was recording a lot or
keeping it in my pocket most of the time.</p>
<p>One day, I‚Äôve even kept the GoPro in my pants pocket (turned out to be
the warmest) for an hour or two before first using it, and it was
<strong>already dead</strong> when I tried to record! WTF?</p>
<p>So I decided to <strong>try something</strong> and just use it hand-held like I used
to in the previous years, to see if my battery degraded significantly
over time or something. <strong>Turned out it performed great</strong> and I could
shoot plenty of hand-held shots the whole day without it dying at any
point!</p>
<p>Then I realized: the <em>only</em> difference between that day where it died in
my pocket before I even use it, and that time where I could record
hand-held all day, was <strong>the plastic case</strong>.</p>
<p>When I plan to use it hand-held only, I don‚Äôt put it in the black
plastic case that allows me to mount it.</p>
<ul>
<li>GoPro without case stored in pocket all day: <strong>full battery</strong>.</li>
<li>GoPro with case stored in pocket all day: <strong>battery dead after a
couple hours</strong>.</li>
</ul>
<p>Luckily if you have a newer GoPro, you don‚Äôt need a case to mount it
anymore, so this shouldn‚Äôt be as much of a problem. But if using one of
the GoPro that can only be mounted with a case, it‚Äôs a deal breaker!</p>
<h2 id="introducing-the-super-suit" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/gopro-canadian-winter.html#introducing-the-super-suit"><span>Introducing the Super Suit</span></a></h2>
<p>The <a href="https://amzn.to/3wP6ZkX">Super Suit</a>, alias ‚Äúprotection + dive
housing‚Äù and now renamed to <a href="https://amzn.to/3LMDaaa">‚Äúprotective housing + waterproof case‚Äù</a>
for the newer models, is a transparent waterproof case for your GoPro.</p>
<figure class="center">
  <img alt="GoPro Super Suit" src="https://www.codejam.info/img/2022/05/gopro-super-suit.jpg">
</figure>
<p>It‚Äôs designed to be used for diving under water, but as
<a href="https://www.hardcoresledder.com/threads/gopro-camera-stops-in-cold-weather.620282/post-20359482">some users</a>
<a href="https://youtu.be/pXfNA-xqETA">pointed out</a>, it does a great job at
protecting from the cold too. It seems this is a very little known trick
for this problem!</p>
<p>I <a href="https://amzn.to/3wP6ZkX">ordered one</a> and was shocked of the
difference it made. <strong>I was no longer running out of battery during the
day.</strong></p>
<p>I could record as much as I wanted at any time of the day and the GoPro
was always there and had plenty of battery life!</p>
<p>I don‚Äôt record necessary <em>a lot</em> while I ski: between 30 minutes to 2
hours of content during a 6 hours ski day. Still, <strong>before the Super
Suit, the GoPro would die in the first 2 hours of skiing</strong>, regardless
of whether it was recording or stored in my pocket the whole time.</p>
<p>After the Super Suit, I can not only record <strong>over 2 hours of content</strong>,
but I can do so <strong>over the course of the whole day</strong>, as long as I keep
it in my pocket when it‚Äôs not recording!</p>
<h2 id="impact-on-audio" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/gopro-canadian-winter.html#impact-on-audio"><span>Impact on audio</span></a></h2>
<p>One of the known downsides of the waterproof housing is that it muffles
the audio. It <em>does</em> sound like it‚Äôs inside a hermetic case.</p>
<p>This is a problem especially for voice: my own voice is somewhat audible
although not super crisp, but we can barely hear people talking around
me. This leads to some scenes where it sounds like I‚Äôm talking alone. üòÇ</p>
<p>But to be honest my HERO5 built-in audio has never been great, so if I
want to record somewhat usable vocals, I need to bring a real microphone
anyways. It seems that a lavalier microphone placed <em>inside</em> the collar
of a ski jacket is fantastic at recording your voice while skiing,
although I‚Äôve never tried myself.</p>
<p>For the ambient sound though, I‚Äôve found the muffled sound of the Super
Suit <strong>perfect to cancel the wind while skiing</strong>, so that we just hear
the skis gliding on the snow. This is in fact <strong>much better</strong> than what
we would otherwise get with the default case or no case (mostly wind,
skis not so much), and is actually usable as is!</p>
<p>I‚Äôve also seen in <a href="https://youtu.be/pXfNA-xqETA">this video</a> a setup
where they drilled tiny holes in the Super Suit around the different
GoPro microphones locations, and covered them with a wind muff. It seems
to give pretty solid results, but I don‚Äôt have the tools to do that
myself, and I guess I like that I can actually use my Super Suit for
diving if I want to. üòÑ</p>
<h2 id="in-short" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/gopro-canadian-winter.html#in-short"><span>In short</span></a></h2>
<p>GoPro dies in cold weather?</p>
<ul>
<li>Get a Super Suit, <a href="https://amzn.to/3wP6ZkX">Super Suit</a>,
alias <a href="https://amzn.to/3LMDaaa">‚Äúprotective housing + waterproof case‚Äù</a>
for newer models.</li>
<li>Keep the GoPro in your warmest pocket while not recording, don‚Äôt let
it turned off mounted on your helmet all day!</li>
<li>Record voices with a lavalier microphone if you need quality dialogue.</li>
</ul>
<p>I hope those tips help you to record more cool content while you‚Äôre
outside in the cold winter! If you found this post useful, you might also
like my article about <a href="https://www.codejam.info/2022/05/best-gopro-position-skiing.html">the best GoPro mount position for skiing</a>,
as well as <a href="https://www.codejam.info/2020/06/my-settings-for-gopro-hero-2018-and-hero5-black.html">my GoPro settings</a>.</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Start a conversation on <a href="https://twitter.com/valeriangalliat">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
  <entry>
    <title>Migrating from X11 to Wayland and from i3 to Sway</title>
    <link href="https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html" />
    <id>https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html</id>
    <updated>2022-05-15T04:00:00.000Z</updated>
    <content type="html"><![CDATA[<p>Finally. After so long. I switched to Wayland. üéâ</p>
<p>I remember back when I started using Linux, more than 10 years ago now,
I was already reading about Wayland, and seeing early adopters on forums
using it and loving it despite running into all kinds of issues‚Ä¶ this
wasn‚Äôt for me. X11, while old and outdated, was well supported for
everything I wanted to do, and that was awesome.</p>
<p>But the other day, I was bored or something, and I asked myself: is
Wayland mainstream enough for me to use it yet?</p>
<p>The answer was‚Ä¶ nearly yes. Yes enough for me to switch. And that‚Äôs a
fucking good news.</p>
<p>In this post I‚Äôll share with you what was needed to <strong>get a usable Wayland
server running with Sway</strong>, all the Wayland alternatives to the X11
programs I was previously using, and finally how I completely purged
X11 from my system.</p>
<p>I‚Äôm a Arch Linux user, so the commands will be adapted to that system.</p>
<h2 id="installing-wayland-and-sway" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html#installing-wayland-and-sway"><span>Installing Wayland and Sway</span></a></h2>
<p>Being a long time <a href="https://i3wm.org/">i3</a> user,
<a href="https://swaywm.org/">Sway</a> was the obvious choice as a Wayland
compositor. The fact it‚Äôs compatible with my existing i3 config should
ease the transition quite a lot.</p>
<pre><code class="hljs language-sh">pacman -S wayland sway
</code></pre>
<p>Then from a TTY I could just run <code>sway</code>, and end up in an environment
pretty close to my habitual i3! Good start.</p>
<h2 id="figuring-all-the-wayland-alternatives" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html#figuring-all-the-wayland-alternatives"><span>Figuring all the Wayland alternatives</span></a></h2>
<p>There‚Äôs a number of X11 programs that I was using, that just don‚Äôt work
on Wayland. The good thing is that the Wayland ecosystem is mature
enough nowadays that there was a solid alternative for all of them!</p>
<ul>
<li><a href="https://tools.suckless.org/dmenu/">dmenu</a>, the great simple dynamic menu
is now <a href="https://github.com/Cloudef/bemenu">bemenu</a>.</li>
<li><a href="https://feh.finalrewind.org/">feh</a>, the fast and light image
viewer, is now <a href="https://sr.ht/~exec64/imv/">imv</a>.</li>
<li><a href="https://github.com/naelstrof/maim">maim</a> (the improved
<a href="https://github.com/resurrecting-open-source-projects/scrot">scrot</a>)
to take screenshots is replaced by <a href="https://sr.ht/~emersion/grim/">grim</a>,
with <a href="https://github.com/emersion/slurp">slurp</a> to select a region.</li>
<li><a href="https://github.com/astrand/xclip">xclip</a> and <a href="https://vergenet.net/~conrad/software/xsel/">XSel</a>
that allow to manipulate the selection and clipboard from the terminal
are replaced by <a href="https://github.com/bugaevc/wl-clipboard">wl-clipboard</a>
(providing <code>wl-copy</code> and <code>wl-paste</code>).</li>
<li><code>xbacklight</code> that helps controlling the screen backlight is now
<a href="https://github.com/haikarainen/light">Light</a>.</li>
<li><a href="https://www.semicomplete.com/projects/xdotool/">xdotool</a> that I
<a href="https://github.com/valeriangalliat/dmenumoji/blob/997e48c69315131b32f9e3368b88151f811d14eb/dmenumoji#L24">use to type an emoji</a>
in my <a href="https://github.com/valeriangalliat/dmenumoji">dmenumoji</a> emoji
picker is now <a href="https://github.com/atx/wtype">wtype</a> (and I made a
<a href="https://github.com/valeriangalliat/dotfiles/blob/14bcdb5d9e7c9d14f15cf3af33c0c862e18bdfb2/bin/bemenumoji"><code>bemenumoji</code></a>
script instead).</li>
<li><a href="http://jonls.dk/redshift/">Redshift</a> that gives an orange tint to the
screen in the evening, is now <a href="https://gitlab.com/chinstrap/gammastep">Gammastep</a>.</li>
</ul>
<p>There‚Äôs also a number of programs that are no longer needed:</p>
<ul>
<li><a href="https://bitbucket.org/raymonad/xss-lock">xss-lock</a> that I used to
lock the screen on suspend and hibernate is superseded by
<a href="https://github.com/swaywm/swayidle">swayidle</a>.</li>
<li><a href="https://gitlab.com/jD91mZM2/xidlehook">xidlehook</a> (the replacement
for <a href="https://linux.die.net/man/1/xautolock">xautolock</a>) allowing to
execute commands after a certain idle period (like dim screen, lock,
suspend), is superseded by <a href="https://github.com/swaywm/swayidle">swayidle</a>
too.</li>
<li><code>xset</code> that I used to set to lower the keyboard repeat delay is
replaced by the <code>repeat_delay</code> Sway option.</li>
<li><a href="https://github.com/yshui/picom">picom</a>, the compositor I used with
X11 is no longer needed because Sway itself is a compositor.</li>
</ul>
<p>So in the end, this leaves us with the following commands:</p>
<pre><code class="hljs language-sh">pacman -S bemenu-wayland imv grim slurp wl-clipboard light wtype gammastep
pacman -Rns dmenu feh maim xclip xsel xorg-xbacklight xdotool redshift xss-lock xidlehook xorg-xset picom
</code></pre>
<p>Because <a href="https://codeberg.org/dnkl/foot">foot</a> is the default terminal
emulator of Sway, I <a href="https://www.codejam.info/2022/04/xfce4-terminal-vs-foot.html">decided to try it</a>
instead of my usual <a href="https://docs.xfce.org/apps/terminal/start">xfce4-terminal</a>.
That wasn‚Äôt a complete success for me and I rolled back to
xfce4-terminal since it works just fine on Wayland anyways!</p>
<p>Finally, I had a few <code>.xmodmaprc</code> modifications that I use to
<a href="https://github.com/valeriangalliat/dotfiles/blob/1d2098a7da513dab195554997efaac22a0d77a02/x11/xmodmaprc">invert <kbd>Alt</kbd> and <kbd>Ctrl</kbd></a>
and also <a href="https://www.codejam.info/2019/06/software-fn-lock.html">emulate <kbd>Fn Lock</kbd></a>
because it‚Äôs not supported on my laptop.</p>
<p>xmodmap is a X11-only thing, and I had to <a href="https://www.codejam.info/2022/04/xmodmaprc-wayland.html">configure XKB directly</a>
to reproduce this behavior. XKB stands for ‚ÄúX keyboard extension‚Äù but it
is also <a href="https://wayland-book.com/seat/xkb.html">used by Wayland</a>.</p>
<h2 id="full-diff" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html#full-diff"><span>Full diff</span></a></h2>
<p>If you want to see the details, here‚Äôs
<a href="https://github.com/valeriangalliat/dotfiles/commit/537f9e14f332b6591a7d932aee056d4d412ec873#diff-d46a2e36b87ce6bb331477a420580121b2fe0c856f81fd5176053ffc4e0828af">the link to the full diff in my dotfiles</a>.</p>
<p>I anchored it to the conversion from <code>~/.config/i3/config</code> to
<code>~/.config/sway/config</code> but feel free to move around and see the other
changes I did.</p>
<p>I took this as an opportunity to change a few unrelated things in there
so not all the modifications were strictly necessary.</p>
<h2 id="cleaning-up" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html#cleaning-up"><span>Cleaning up</span></a></h2>
<p>Now we have a working Wayland and Sway installation, we can remove X11
altogether from the system! Or can we?</p>
<pre><code class="hljs language-sh">pacman -Rns xorg-server i3
</code></pre>
<p>Turns out this didn‚Äôt work for me. VLC, <a href="https://mpv.io/">mpv</a>, Chromium
and <a href="https://calibre-ebook.com/">calibre</a> all required some X11
dependency that would be removed by this command. Bummer.</p>
<p>So what I did instead:</p>
<pre><code class="hljs language-sh">pacman -Rns xorg-server i3 vlc mpv chromium calibre
pacman -S vlc mpv chromium calibre
</code></pre>
<h2 id="quirks" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html#quirks"><span>Quirks</span></a></h2>
<h3 id="qt-and-wayland" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html#qt-and-wayland"><span>Qt and Wayland</span></a></h3>
<p>VLC and calibre both use Qt, and as <a href="https://wiki.archlinux.org/title/wayland#Qt">documented on the ArchWiki</a>,
we need to install <code>qt5-wayland</code> for Qt to work.</p>
<h3 id="special-flags-for-chromium" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html#special-flags-for-chromium"><span>Special flags for Chromium</span></a></h3>
<p>Programs built on Chromium (including Chromium itself obviously) support
Wayland <em>nearly</em> out of the box, but they require some kind of flag to
enable the support. Not really sure why this is a thing, but basically I
need to start Chromium and Visual Studio Code like this:</p>
<pre><code class="hljs language-sh">chromium --ozone-platform-hint=auto
code --enable-features=UseOzonePlatform --ozone-platform=wayland
</code></pre>
<p>I use those programs once in a blue moon anyways, so I don‚Äôt really
care.</p>
<h3 id="idle-inhibitor" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html#idle-inhibitor"><span>Idle inhibitor</span></a></h3>
<p>I used to use <code>xidlehook --not-when-audio</code> to prevent dimming the
screen, disconnecting the screen, or locking the computer after an idle
period if there‚Äôs audio playing.</p>
<p>This is great for example when watching a movie‚Ä¶ you don‚Äôt necessarily
actively use the computer but you don‚Äôt want it to lock and suspend or
hibernate while it‚Äôs playing either!</p>
<p>Some programs like mpv support inhibiting idle while playing, which
is great, but others like VLC and Firefox don‚Äôt.</p>
<p>In general, the ‚Äúnot when audio‚Äù trick was a pretty good fallback that
didn‚Äôt require any custom implementation in existing programs.</p>
<p>Luckily, there‚Äôs <a href="https://github.com/ErikReider/SwayAudioIdleInhibit">SwayAudioIdleInhibit</a>
(<a href="https://aur.archlinux.org/packages/sway-audio-idle-inhibit-git">on the AUR</a>)
that does exactly that. Fantastic.</p>
<p>The only quirk I noticed with it is that in Firefox, some very specific
sites like <a href="https://artlist.io/">Artlist</a> (the only one I identified so
far) manage to register an active audio channel at all times even if
they‚Äôre not playing anything, and as long as the tab is open, idle will
be inhibited. This is not good as I tend to keep tabs around for days if
not weeks!</p>
<p>To be able to notice when this happens more easily, I
<a href="https://github.com/valeriangalliat/dotfiles/commit/2fd9359a6a0e76891b6b10fe1ef97f7aec35f926">modified my i3blocks volume block</a>
to display a different icon whether or not there‚Äôs any PulseAudio sink
in state <code>RUNNING</code>.</p>
<h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="https://www.codejam.info/2022/05/migrating-x11-wayland-i3-sway.html#conclusion"><span>Conclusion</span></a></h2>
<p>Migrating to Wayland was a pretty smooth transition at the end of the
day, and I‚Äôm glad I finally did it! Everything works great, it seems
like Wayland programs are usually more recent and have better UX than
their X11 equivalent that I was previously using.</p>
<p>For example I love the <em>slurp</em> screen selection, and I don‚Äôt have to
<a href="https://www.codejam.info/2021/08/dmenu-libxft-bgra-emoji-support.html">patch dmenu</a> anymore
in order to support emojis, since they natively work with bemenu, and
basically everything else?</p>
<p>Also I realized that Wayland allowed me to zoom in on any part of the
screen with my trackpad out of the box, and that‚Äôs pretty useful. One of
the features I was kinda missing from MacBooks but never spent the time
to figure if I could do it or not with X11.</p>
<p>If you‚Äôve been thinking about migrating to Wayland, it‚Äôs probably a good
time to do so!</p>
<section class="post-footer">
  <h3>Want to leave a comment?</h3>
  <p>
    Join the discussion on <a href="https://twitter.com/valeriangalliat/status/1525897795844153344">Twitter</a> or send me an <a href="mailto:val@codejam.info">email</a>! üíå<br>
    This post helped you? <a href="https://ko-fi.com/funkyval">Buy me a coffee</a>! üçª
  </p>
</section>
]]></content>
  </entry>
</feed>
